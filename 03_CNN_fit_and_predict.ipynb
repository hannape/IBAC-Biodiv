{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03 CNN fit and predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannape/IBAC-Biodiv/blob/master/03_CNN_fit_and_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sohX5JSmwCbK",
        "colab_type": "text"
      },
      "source": [
        "Fitujemy model i robimy predykcję na 20 nagraniach. \n",
        "\n",
        "Na podstawie I: Copy of CNN-weighting-IBAC.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGotjA5iff5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# previous version, with hidden code and comments - cnn_scorer_not_working, cnn_weighting\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from __future__ import print_function\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3Kkaxwbp1Vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import  make_scorer\n",
        "from sklearn.metrics import log_loss\n",
        "K.set_image_dim_ordering('th')\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import functools\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import LearningRateScheduler, CSVLogger\n",
        "import math\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "from functools import partial, update_wrapper\n",
        "\n",
        "\n",
        "from keras.models import load_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sidyS8Up3yJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(667)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(667)\n",
        "import random\n",
        "random.seed()\n",
        "\n",
        "'''\n",
        "#################### Rep 1 - spektro ####################\n",
        "# rep 1  ------- 63 x 148 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep1/X_train_rep1.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep1/y_train.npy', allow_pickle=True)\n",
        "'''\n",
        "# rep 1V2 ------- 63 x 148 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep1V2/X_train_rep1.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep1V2/y_train_rep1.npy', allow_pickle=True)\n",
        "'''\n",
        "# rep 1b ------- 63 x 63 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep1b/X_train_rep1b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep1b/y_train_rep1b.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 3 - mel-spektro ####################\n",
        "\n",
        "# rep 3 ------- 60 x 111 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep3/X_train_rep3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep3/y_train.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V2 ------- 60 x 111 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep3V2/X_train_rep3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep3V2/y_train_rep3.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3b ------- 60 x 63 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep3b/X_train_rep3b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep3b/y_train_rep3b.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V3 ------- 60 x 148 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep3V3/X_train_rep3V3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep3V3/y_train_rep3V3.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 5 - mel-spektro ####################\n",
        "\n",
        "# rep 5 ------- 64 x 61 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep5/X_train_rep5.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep5/y_train.npy', allow_pickle=True)\n",
        "\n",
        "# rep 5V2 ------- 64 x 61 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep5V2/X_train_rep5.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep5V2/y_train_rep5.npy', allow_pickle=True)\n",
        "\n",
        "# rep 5b ------- 64 x 149 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep5b/X_train_rep5b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep5b/y_train_rep5b.npy', allow_pickle=True)\n",
        "'''\n",
        "\n",
        "print('Training set size:')\n",
        "print(np.shape(X_train1))\n",
        "print(np.shape(y_train1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQJx_n6arYgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train1, y_train1, test_size=0.2, random_state=667) # default shuffle:true\n",
        "r = np.shape(X_val)[1]\n",
        "s = np.shape(X_val)[2]\n",
        "\n",
        "del X_train1, y_train1\n",
        "\n",
        "exemp = 4# exemplary frame\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(X_train[exemp], cmap=\"jet\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(X_val[exemp], cmap=\"jet\")\n",
        "\n",
        "print(y_train[exemp],y_val[exemp])\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, r, s).astype('float32')\n",
        "X_val = X_val.reshape(X_val.shape[0], 1, r, s).astype('float32')\n",
        "\n",
        "print('Training set size:')\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(y_train))\n",
        "\n",
        "print('Validation set size:')\n",
        "print(np.shape(X_val))\n",
        "print(np.shape(y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoNYVtGXr3YO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/keras-team/keras/issues/2115\n",
        "\n",
        "### definiowanie wag \n",
        "for_zeros = 0.1\n",
        "for_ones = 0.9\n",
        "###\n",
        "\n",
        "### SCORERS\n",
        "\n",
        "def binary_crossentropy_weigted(y_true, y_pred, class_weights):\n",
        "\ty_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "  \n",
        "\tloss = K.mean(class_weights*(-y_true * K.log(y_pred) - (1.0 - y_true) * K.log(1.0 - y_pred)),axis=-1)\n",
        "\treturn loss\n",
        "\n",
        "def weighted_binary_crossentropy( y_true, y_pred, weights_10) :\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "    logloss = -(y_true * K.log(y_pred) * weights_10[0] + (1 - y_true) * K.log(1 - y_pred) * weights_10[1])\n",
        "    return K.mean(logloss, axis=-1)\n",
        "\n",
        "custom_loss1 = partial(binary_crossentropy_weigted, class_weights=np.array([for_zeros,for_ones])) ## scoring for model.compile\n",
        "custom_loss1.__name__ ='binary_crossentropy_weigted'\n",
        "\n",
        "custom_loss2 = partial(weighted_binary_crossentropy, weights_10=np.array([for_ones,for_zeros])) ## scoring for model.compile\n",
        "custom_loss2.__name__ ='weighted_binary_crossentropy'\n",
        "\n",
        "## AUC METRIC\n",
        "def as_keras_metric(method):\n",
        "    import functools\n",
        "    from keras import backend as K\n",
        "    \n",
        "    @functools.wraps(method)\n",
        "    def wrapper(self, args, **kwargs):\n",
        "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
        "        value, update_op = method(self, args, **kwargs)\n",
        "        K.get_session().run(tf.local_variables_initializer())\n",
        "        with tf.control_dependencies([update_op]):\n",
        "            value = tf.identity(value)\n",
        "        return value\n",
        "    return wrapper\n",
        "  \n",
        "auc_roc = as_keras_metric(tf.metrics.auc)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTdwqcDjr9UY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######### Fitowanie modelu N razy\n",
        "\n",
        "N = 6                                                # N razy fitujemy\n",
        "repr_nr = '_rep3V3_'                                 # numer reprezentacji, będzie w nazwie pliku z modelem\n",
        "n_epochs = 50                                        # liczba epok\n",
        "model_out = 'drive/My Drive/models/rep3V3/50epoch/'  # gdzie zapiszemy model, foldery tworzymy w zależności od liczby epok\n",
        "\n",
        "for k in range(0,N): \n",
        "  \n",
        "  def make_model_modified(my_loss): # definiujemy za każdym razem, by initial weight się zmieniły? Czy się zmieniają przy wywołaniu compile?\n",
        "\n",
        "      dense_layer_sizes=128\n",
        "      filters=10\n",
        "      kernel_size=(3,3) \n",
        "      pool_size= (2,2) \n",
        "      drop_out = 0.5\n",
        "\n",
        "      ##  3 conv (3,3), na zmianę z (2,2) pooling\n",
        "      model = Sequential()\n",
        "      model.add(Conv2D(filters, kernel_size,input_shape=input_shape, activation='relu'))\n",
        "      model.add(MaxPooling2D(pool_size=pool_size))\n",
        "      model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "      model.add(MaxPooling2D(pool_size=pool_size))\n",
        "      model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "      model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "      ## Gęste warstwy: 128, 32, 1, dropout 0.5\n",
        "      model.add(Flatten())\n",
        "      model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "      model.add(Dropout(drop_out))\n",
        "      model.add(Dense(32, activation='relu'))\n",
        "      model.add(Dropout(drop_out))\n",
        "      model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "      model.compile(loss=my_loss, # 'binary_crossentropy',\n",
        "                    optimizer='adam',#keras.optimizers.Adam(lr),#'adam',\n",
        "                    metrics=['accuracy',auc_roc])\n",
        "\n",
        "      return model\n",
        "\n",
        "  ################################# Definicja modelu\n",
        "  input_shape = (1, r, s)\n",
        "  loss_custom = custom_loss2\n",
        "  loss_bc = 'binary_crossentropy'\n",
        "\n",
        "  my_model = make_model_modified(loss_bc)\n",
        "\n",
        "  #################################\n",
        "\n",
        "  now = datetime.datetime.now()\n",
        "  start_time = str(now.year)+str('-')+str(now.month)+str('-')+str(now.day)+str('_')+str(now.hour+2)+str(':')+str(now.minute)\n",
        "  \n",
        "  y_frame = pd.DataFrame(y_train)\n",
        "  print(np.shape(y_frame))\n",
        "\n",
        "  print(\"Model binary crossentropy, bez wag\")\n",
        "  print(\"Run: \" + str(k))\n",
        "  grid_result = my_model.fit(X_train, y_train, batch_size = 32, epochs = n_epochs, validation_data=(X_val, y_val), verbose = 2)\n",
        "\n",
        "  my_model.save((model_out + str(start_time) + '-' + str(n_epochs) + 'ep-' + str(repr_nr) + str(loss_bc) + '_run'+str(k+1)+'_my_model.h5')) \n",
        "\n",
        "  #########################################################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTXICtmHs5aS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## PREDYKCJA\n",
        "\n",
        "\n",
        "del X_train, X_val  ## Wyrzucamy by nie zajmowało miejsca\n",
        "\n",
        "'''\n",
        "#################### Rep 1 - spektro ####################\n",
        "# rep 1  ------- 63 x 148 ------ \n",
        "\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep1/X_test2_rep1.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep1/y_test2_rep1.npy', allow_pickle=True)\n",
        "\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep1/X_test_rep1.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep1/y_test_rep1.npy', allow_pickle=True)  ### ----- 322 positive! -----\n",
        "\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep1/X_test1_rep1_nowa676.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep1/y_test1_rep1_nowa676.npy', allow_pickle=True)  ### ----- 676 positive! -----\n",
        "\n",
        "# rep 1b ------- 63 x 63 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep1b/X_test2_rep1b.npy', allow_pickle=True) \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep1b/y_test2_rep1b.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep1b/X_test1_rep1b.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep1b/y_test1_rep1b.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 3 - mel-spektro ####################\n",
        "\n",
        "# rep 3 ------- 60 x 111 ------ \n",
        "X_test1_14 = numpy.load('drive/My Drive/rep3/X_test2_rep3.npy', allow_pickle=True)  \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep3/y_test2_popr.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep3/X_test_rep3.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep3/y_test.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3b ------- 60 x 63 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep3b/X_test2_rep3b.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep3b/y_test2_rep3b.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep3b/X_test1_rep3b.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep3b/y_test1_rep3b.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V3 ------- 60 x 148 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep3V3/X_test2_rep3V3.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep3V3/y_test2_rep3V3.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep3V3/X_test1_rep3V3.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep3V3/y_test1_rep3V3.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 5 - mel-spektro ####################\n",
        "\n",
        "# rep 5 ------- 64 x 61 ------ \n",
        "X_test1_14 = numpy.load('drive/My Drive/rep5/X_test2_rep5.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep5/y_test2_rep5.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep5/X_test_rep5.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep5/y_test_rep5.npy', allow_pickle=True) ### ----- 322 positive! -----\n",
        "\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep5/X_test1_rep5_nowa676.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep5/y_test1_rep5_nowa676.npy', allow_pickle=True)  ### ----- 676 positive! -----\n",
        "\n",
        "# rep 5b ------- 64 x 149 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep5b/X_test2_rep5b.npy', allow_pickle=True)  \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep5b/y_test2_rep5b.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep5b/X_test1_rep5b.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep5b/y_test1_rep5b.npy', allow_pickle=True)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "# model_out = 'drive/My Drive/models/rep3V2/'    # gdzie są zapisane modele\n",
        "model_out_save = 'drive/My Drive/models/rep3V2/'  # gdzie zapiszemy predykcje\n",
        "\n",
        "###\n",
        "r = np.shape(X_test1_6)[1]\n",
        "s = np.shape(X_test1_6)[2]\n",
        "\n",
        "X_test1_14 = X_test1_14.reshape(X_test1_14.shape[0], 1, r, s).astype('float32')\n",
        "y_test1_14 = np.squeeze(y_test1_14)\n",
        "X_test1_6 = X_test1_6.reshape(X_test1_6.shape[0], 1, r, s).astype('float32')\n",
        "y_test1_6 = np.squeeze(y_test1_6)\n",
        "\n",
        "modele = []\n",
        "modele += [each for each in sorted(os.listdir(model_out)) if each.endswith('.h5')] \n",
        "print(modele)\n",
        "\n",
        "for k in range(0,6):  \n",
        "  model = load_model(model_out + modele[k], custom_objects={'auc': auc_roc})\n",
        "  print(modele[k])\n",
        "  print(k)\n",
        "  \n",
        "  ### Predykcja na 20 nagraniach\n",
        "  \n",
        "  probs1 = model.predict_proba(X_test1_14)\n",
        "  probs2 = model.predict_proba(X_test1_6)\n",
        "  probs = np.concatenate([probs1, probs2])\n",
        "  y_test1 = np.concatenate([y_test1_14, y_test1_6])\n",
        "  \n",
        "  ## Predykcja na 14 z 2018 \n",
        "  \n",
        "  #probs = model.predict_proba(X_test1_14)    \n",
        "  #y_test1=y_test1_14\n",
        "  \n",
        "  ###\n",
        "  \n",
        "  print(np.shape(probs))\n",
        "  print(np.shape(y_test1))\n",
        "  auc = roc_auc_score(y_test1, probs)\n",
        "  print(\"Run: \" + str(k))\n",
        "  print('AUC: %.6f' % auc)\n",
        "  fpr, tpr, thresholds = roc_curve(y_test1, probs)\n",
        "\n",
        "  print(np.shape(fpr))\n",
        "  run = k +1\n",
        "  np.savez((model_out_save + str(repr_nr)+ str(k+1)), fpr=fpr, tpr=tpr, thresholds=thresholds, run= run )\n",
        "\n",
        "  a = np.load(model_out_save + str(repr_nr)+ str(k+1) + \".npz\")\n",
        "  print('Dostepne dane: ' + str(a.files))\n",
        "  print(a[\"run\"])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}