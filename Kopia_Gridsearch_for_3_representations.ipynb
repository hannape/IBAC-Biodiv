{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopia Gridsearch for 3 representations. ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannape/IBAC-Biodiv/blob/master/Kopia_Gridsearch_for_3_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mq81jmxhMYV",
        "colab_type": "text"
      },
      "source": [
        "Gridsearch dla każdej reprezentacji osobno. Bo na IBACu wszystkie 3 reprezentacje były puszczone na architekturę z mel-spectrogramu (chyba?)\n",
        "\n",
        "Na podstawie hania.dldisc: cnn_gridsearch.ipynb , 03 CNN fit and predict (I2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65s8txmrhL7Y",
        "colab_type": "code",
        "outputId": "937921ef-dca2-4a8f-d61d-6f92b9211ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from __future__ import print_function"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CddWh85JjLPZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8be46846-d2ad-4c3b-8105-402a1e2fc008"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "import numpy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import numpy\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "#K.set_image_dim_ordering('th')\n",
        "K.tensorflow_backend.set_image_dim_ordering('th')\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from keras.datasets import mnist\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from functools import partial, update_wrapper\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAjy6F71jTVn",
        "colab_type": "code",
        "outputId": "bed5b24e-89ff-49a0-ead1-2debfd579ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(667)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(667)\n",
        "import random\n",
        "random.seed()\n",
        "\n",
        "'''\n",
        "#################### Rep 1 - spektro ####################\n",
        "# rep 1  ------- 63 x 148 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep1/X_train_rep1.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep1/y_train.npy', allow_pickle=True)\n",
        "'''\n",
        "# rep 1V2 ------- 63 x 148 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep1V2/X_train_rep1.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep1V2/y_train_rep1.npy', allow_pickle=True)\n",
        "'''\n",
        "# rep 1b ------- 63 x 63 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep1b/X_train_rep1b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep1b/y_train_rep1b.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 3 - mel-spektro ####################\n",
        "\n",
        "# rep 3 ------- 60 x 111 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3/X_train_rep3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3/y_train.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V2 ------- 60 x 111 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V2/X_train_rep3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V2/y_train_rep3.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3b ------- 60 x 63 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3b/X_train_rep3b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3b/y_train_rep3b.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V3 ------- 60 x 148 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V3/X_train_rep3V3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V3/y_train_rep3V3.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 5 - mel-spektro ####################\n",
        "\n",
        "# rep 5 ------- 64 x 61 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep5/X_train_rep5.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep5/y_train.npy', allow_pickle=True)\n",
        "\n",
        "# rep 5V2 ------- 64 x 61 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep5V2/X_train_rep5.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep5V2/y_train_rep5.npy', allow_pickle=True)\n",
        "\n",
        "# rep 5b ------- 64 x 149 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep5b/X_train_rep5b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep5b/y_train_rep5b.npy', allow_pickle=True)\n",
        "'''\n",
        "\n",
        "print('Training set size:')\n",
        "print(np.shape(X_train1))\n",
        "print(np.shape(y_train1))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size:\n",
            "(15904, 63, 148)\n",
            "(15904,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST0WDnQSxeLD",
        "colab_type": "code",
        "outputId": "e796eb12-0bc0-4e3f-cb08-81ad8242bfe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "r = np.shape(X_train1)[1]\n",
        "s = np.shape(X_train1)[2]\n",
        "y_train = y_train1\n",
        "X_train = X_train1.reshape(X_train1.shape[0], 1, r, s).astype('float32')\n",
        "print('Training set size:')\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(y_train))\n",
        "input_shape = (1, r, s)\n",
        "print(input_shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size:\n",
            "(15904, 1, 63, 148)\n",
            "(15904,)\n",
            "(1, 63, 148)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j27RDs6YmWqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/keras-team/keras/issues/2115\n",
        "\n",
        "### definiowanie wag \n",
        "for_zeros = 0.1\n",
        "for_ones = 0.9\n",
        "###\n",
        "\n",
        "### SCORERS\n",
        "\n",
        "#def binary_crossentropy_weigted(y_true, y_pred, class_weights):\n",
        "#\ty_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "#  \n",
        "#\tloss = K.mean(class_weights*(-y_true * K.log(y_pred) - (1.0 - y_true) * K.log(1.0 - y_pred)),axis=-1)\n",
        "#\treturn loss\n",
        "\n",
        "def weighted_binary_crossentropy( y_true, y_pred, weights_10) :\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "    logloss = -(y_true * K.log(y_pred) * weights_10[0] + (1 - y_true) * K.log(1 - y_pred) * weights_10[1])\n",
        "    return K.mean(logloss, axis=-1)\n",
        "\n",
        "#custom_loss1 = partial(binary_crossentropy_weigted, class_weights=np.array([for_zeros,for_ones])) ## scoring for model.compile\n",
        "#custom_loss1.__name__ ='binary_crossentropy_weigted'\n",
        "\n",
        "custom_loss2 = partial(weighted_binary_crossentropy, weights_10=np.array([for_ones,for_zeros])) ## scoring for model.compile\n",
        "custom_loss2.__name__ ='weighted_binary_crossentropy'\n",
        "\n",
        "## AUC METRIC\n",
        "def as_keras_metric(method):\n",
        "    import functools\n",
        "    from keras import backend as K\n",
        "    \n",
        "    @functools.wraps(method)\n",
        "    def wrapper(self, args, **kwargs):\n",
        "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
        "        value, update_op = method(self, args, **kwargs)\n",
        "        K.get_session().run(tf.local_variables_initializer())\n",
        "        with tf.control_dependencies([update_op]):\n",
        "            value = tf.identity(value)\n",
        "        return value\n",
        "    return wrapper\n",
        "  \n",
        "auc_roc = as_keras_metric(tf.metrics.auc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Oq0VwYupNmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_model(dense_layer_sizes, filters, kernel_size, pool_size, hidden_layers, loss_function):\n",
        "   \n",
        "    #hidden_layers = 1\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,input_shape=input_shape, activation='relu', data_format='channels_first'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    #model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    #model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    #model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    #model.add(MaxPooling2D(pool_size=pool_size))\n",
        "   \n",
        "    for i in range(0,hidden_layers):\n",
        "      # Add one hidden layer\n",
        "      print('Warstwa '+ str(i+1))\n",
        "      model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "      model.add(MaxPooling2D(pool_size=pool_size))\n",
        "  \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss=loss_function,\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy',auc_roc])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NUxERPzqK4I",
        "colab_type": "code",
        "outputId": "4f374079-8b82-4a47-9687-b9def8914d6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "hidden_layers= 2\n",
        "for i in range(hidden_layers):\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5svLHOfqjgDp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "  \n",
        "def make_model_modified(dense_layer_sizes, filters, kernel_size, pool_size):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,input_shape=input_shape, activation='relu', data_format='channels_first')) # \"channels_first\" corresponds to inputs with shape (batch, channels, height, width)\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "   \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "  \n",
        "def probny_model(dense_layer_sizes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(10, (3,3),input_shape=input_shape, activation='relu', data_format='channels_first')) # \"channels_first\" corresponds to inputs with shape (batch, channels, height, width)\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "   \n",
        "    model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model  \n",
        "  \n",
        "def make_model(dense_layer_sizes, filters, kernel_size, pool_size):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,\n",
        "                     padding='valid',\n",
        "                     input_shape=input_shape, data_format='channels_first'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters, kernel_size))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    for layer_size in dense_layer_sizes:\n",
        "        model.add(Dense(layer_size))\n",
        "        model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))#(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', #'categorical_crossentropy',\n",
        "                  optimizer='adadelta',\n",
        "                  metrics=['accuracy'])\n",
        "    return model  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYsSf-Oucqxl",
        "colab_type": "code",
        "outputId": "34b832d8-ec9a-4ec0-e52b-8df4672994c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "y=y_train.astype('int')\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(y_train))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15904, 1, 63, 148)\n",
            "(15904,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRdAp3bGb7RJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "my_classifier3 = KerasClassifier(make_model)\n",
        "validator3 = GridSearchCV(my_classifier3,\n",
        "                         param_grid={'dense_layer_sizes': [128]}, \n",
        "                         scoring='neg_log_loss')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76411ZmN85od",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "dense_size_candidates = [[32], [64]] #, [32, 32], [64, 64]]\n",
        "my_classifier = KerasClassifier(make_model, batch_size=32)\n",
        "\n",
        "validator = GridSearchCV(my_classifier,\n",
        "                         param_grid={'dense_layer_sizes': [[32], [64]],\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [1],\n",
        "                                     'filters': [8],\n",
        "                                     'kernel_size': [3],\n",
        "                                     'pool_size': [2]},\n",
        "                         scoring='neg_log_loss',\n",
        "                         n_jobs=1)\n",
        "#y2=y_train.astype('int')\n",
        "validator.fit(X_train, y) #_binary)\n",
        "\n",
        "print('The parameters of the best model are: ')\n",
        "print(validator.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MadBqOqYbAo4",
        "colab_type": "code",
        "outputId": "8ba0c2a2-2d53-44fd-d611-21fe31e9ec7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "my_classifier66 = KerasClassifier(search_model)\n",
        "validator66 = GridSearchCV(my_classifier66,\n",
        "                         param_grid={'dense_layer_sizes': [128, 256],\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [15, 30, 50], #[10, 30, 50],\n",
        "                                     'filters': [20], #[10, 20],\n",
        "                                     'kernel_size': [(3,3)], #[(3,3)],\n",
        "                                     'pool_size': [(2,2)],#[(2,2)],\n",
        "                                     'hidden_layers': [3],\n",
        "                                     'loss_function': ['binary_crossentropy'],\n",
        "                                     'batch_size': [32], #[32, 64]\n",
        "                                     }, \n",
        "                         #scoring='neg_log_loss',\n",
        "                         cv = StratifiedKFold(n_splits = 5, random_state=667, shuffle = True))\n",
        "#y=y_train.astype('int')\n",
        "'''\n",
        "validator66 = GridSearchCV(my_classifier66,\n",
        "                         param_grid={'dense_layer_sizes': [128, 256],\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [10,30], #[10, 30, 50],\n",
        "                                     'filters': [10,20], #[10, 20],\n",
        "                                     'kernel_size': [(3,3)], #[(3,3)],\n",
        "                                     'pool_size': [(2,2)],#[(2,2)],\n",
        "                                     'hidden_layers': [2,3],\n",
        "                                     'loss_function': ['binary_crossentropy', custom_loss2],\n",
        "                                     'batch_size': [32,64], #[32, 64]\n",
        "                                     }, \n",
        "                         scoring='neg_log_loss', cv = StratifiedKFold(n_splits =4, random_state=667, shuffle = True))\n",
        "'''\n",
        "grid_result = validator66.fit(X_train, y)\n",
        "print('The parameters of the best model are: ')\n",
        "print(validator66.best_params_)\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:809: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/15\n",
            "12723/12723 [==============================] - 10s 761us/step - loss: 0.7112 - acc: 0.5506 - auc: 0.5036\n",
            "Epoch 2/15\n",
            "12723/12723 [==============================] - 6s 460us/step - loss: 0.6870 - acc: 0.5546 - auc: 0.5014\n",
            "Epoch 3/15\n",
            "12723/12723 [==============================] - 6s 461us/step - loss: 0.6870 - acc: 0.5546 - auc: 0.5007\n",
            "Epoch 4/15\n",
            "12723/12723 [==============================] - 6s 459us/step - loss: 0.6986 - acc: 0.5544 - auc: 0.5027\n",
            "Epoch 5/15\n",
            "12723/12723 [==============================] - 6s 458us/step - loss: 0.6876 - acc: 0.5542 - auc: 0.5026\n",
            "Epoch 6/15\n",
            "12723/12723 [==============================] - 6s 459us/step - loss: 0.6875 - acc: 0.5546 - auc: 0.5014\n",
            "Epoch 7/15\n",
            "12723/12723 [==============================] - 6s 457us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5006\n",
            "Epoch 8/15\n",
            "12723/12723 [==============================] - 6s 464us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5009\n",
            "Epoch 9/15\n",
            "12723/12723 [==============================] - 6s 460us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5006\n",
            "Epoch 10/15\n",
            "12723/12723 [==============================] - 6s 461us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5007\n",
            "Epoch 11/15\n",
            "12723/12723 [==============================] - 6s 461us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5005\n",
            "Epoch 12/15\n",
            "12723/12723 [==============================] - 6s 465us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5000\n",
            "Epoch 13/15\n",
            "12723/12723 [==============================] - 6s 462us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.4997\n",
            "Epoch 14/15\n",
            "12723/12723 [==============================] - 6s 466us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4995\n",
            "Epoch 15/15\n",
            "12723/12723 [==============================] - 6s 463us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4996\n",
            "3181/3181 [==============================] - 1s 226us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/15\n",
            "12723/12723 [==============================] - 7s 524us/step - loss: 0.7040 - acc: 0.5484 - auc: 0.5107\n",
            "Epoch 2/15\n",
            "12723/12723 [==============================] - 6s 465us/step - loss: 0.6879 - acc: 0.5551 - auc: 0.5084\n",
            "Epoch 3/15\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.6126 - acc: 0.6510 - auc: 0.5257\n",
            "Epoch 4/15\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.4091 - acc: 0.8267 - auc: 0.6293\n",
            "Epoch 5/15\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.3453 - acc: 0.8656 - auc: 0.7187\n",
            "Epoch 6/15\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.3194 - acc: 0.8761 - auc: 0.7702\n",
            "Epoch 7/15\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.2985 - acc: 0.8879 - auc: 0.8048\n",
            "Epoch 8/15\n",
            "12723/12723 [==============================] - 6s 467us/step - loss: 0.2878 - acc: 0.8941 - auc: 0.8290\n",
            "Epoch 9/15\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.2753 - acc: 0.8992 - auc: 0.8465\n",
            "Epoch 10/15\n",
            "12723/12723 [==============================] - 6s 467us/step - loss: 0.2660 - acc: 0.9016 - auc: 0.8603\n",
            "Epoch 11/15\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.2538 - acc: 0.9076 - auc: 0.8710\n",
            "Epoch 12/15\n",
            "12723/12723 [==============================] - 6s 467us/step - loss: 0.2472 - acc: 0.9091 - auc: 0.8799\n",
            "Epoch 13/15\n",
            "12723/12723 [==============================] - 6s 465us/step - loss: 0.2400 - acc: 0.9131 - auc: 0.8876\n",
            "Epoch 14/15\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.2391 - acc: 0.9131 - auc: 0.8937\n",
            "Epoch 15/15\n",
            "12723/12723 [==============================] - 6s 465us/step - loss: 0.2301 - acc: 0.9171 - auc: 0.8991\n",
            "3181/3181 [==============================] - 1s 229us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/15\n",
            "12723/12723 [==============================] - 7s 526us/step - loss: 0.6982 - acc: 0.5451 - auc: 0.5040\n",
            "Epoch 2/15\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.6822 - acc: 0.5636 - auc: 0.5056\n",
            "Epoch 3/15\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.4992 - acc: 0.7683 - auc: 0.5887\n",
            "Epoch 4/15\n",
            "12723/12723 [==============================] - 6s 466us/step - loss: 0.3744 - acc: 0.8467 - auc: 0.6976\n",
            "Epoch 5/15\n",
            "12723/12723 [==============================] - 6s 464us/step - loss: 0.3291 - acc: 0.8728 - auc: 0.7651\n",
            "Epoch 6/15\n",
            "12723/12723 [==============================] - 6s 464us/step - loss: 0.3055 - acc: 0.8846 - auc: 0.8058\n",
            "Epoch 7/15\n",
            "12723/12723 [==============================] - 6s 466us/step - loss: 0.2929 - acc: 0.8893 - auc: 0.8325\n",
            "Epoch 8/15\n",
            "12723/12723 [==============================] - 6s 463us/step - loss: 0.2772 - acc: 0.8981 - auc: 0.8516\n",
            "Epoch 9/15\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.2591 - acc: 0.9038 - auc: 0.8661\n",
            "Epoch 10/15\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.2562 - acc: 0.9060 - auc: 0.8774\n",
            "Epoch 11/15\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2453 - acc: 0.9118 - auc: 0.8864\n",
            "Epoch 12/15\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.2414 - acc: 0.9120 - auc: 0.8936\n",
            "Epoch 13/15\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.2325 - acc: 0.9161 - auc: 0.8998\n",
            "Epoch 14/15\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.2262 - acc: 0.9206 - auc: 0.9051\n",
            "Epoch 15/15\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.2243 - acc: 0.9202 - auc: 0.9096\n",
            "3181/3181 [==============================] - 1s 258us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/15\n",
            "12723/12723 [==============================] - 7s 532us/step - loss: 0.7078 - acc: 0.5464 - auc: 0.5099\n",
            "Epoch 2/15\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.6568 - acc: 0.6120 - auc: 0.5518\n",
            "Epoch 3/15\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.4453 - acc: 0.8038 - auc: 0.6401\n",
            "Epoch 4/15\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.3550 - acc: 0.8595 - auc: 0.7392\n",
            "Epoch 5/15\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.3221 - acc: 0.8761 - auc: 0.7944\n",
            "Epoch 6/15\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.3006 - acc: 0.8884 - auc: 0.8276\n",
            "Epoch 7/15\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2966 - acc: 0.8887 - auc: 0.8489\n",
            "Epoch 8/15\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2902 - acc: 0.8930 - auc: 0.8632\n",
            "Epoch 9/15\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.2778 - acc: 0.8965 - auc: 0.8745\n",
            "Epoch 10/15\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.2682 - acc: 0.8996 - auc: 0.8828\n",
            "Epoch 11/15\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.2671 - acc: 0.9007 - auc: 0.8900\n",
            "Epoch 12/15\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.2574 - acc: 0.9084 - auc: 0.8958\n",
            "Epoch 13/15\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.2513 - acc: 0.9063 - auc: 0.9008\n",
            "Epoch 14/15\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.2433 - acc: 0.9120 - auc: 0.9051\n",
            "Epoch 15/15\n",
            "12723/12723 [==============================] - 6s 466us/step - loss: 0.2452 - acc: 0.9094 - auc: 0.9088\n",
            "3181/3181 [==============================] - 1s 264us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/15\n",
            "12724/12724 [==============================] - 7s 556us/step - loss: 0.7309 - acc: 0.5439 - auc: 0.4981\n",
            "Epoch 2/15\n",
            "12724/12724 [==============================] - 6s 473us/step - loss: 0.6913 - acc: 0.5524 - auc: 0.5055\n",
            "Epoch 3/15\n",
            "12724/12724 [==============================] - 6s 473us/step - loss: 0.6870 - acc: 0.5547 - auc: 0.5044\n",
            "Epoch 4/15\n",
            "12724/12724 [==============================] - 6s 468us/step - loss: 0.6868 - acc: 0.5545 - auc: 0.5028\n",
            "Epoch 5/15\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.6870 - acc: 0.5547 - auc: 0.5022\n",
            "Epoch 6/15\n",
            "12724/12724 [==============================] - 6s 468us/step - loss: 0.6867 - acc: 0.5546 - auc: 0.5027\n",
            "Epoch 7/15\n",
            "12724/12724 [==============================] - 6s 471us/step - loss: 0.6871 - acc: 0.5545 - auc: 0.5025\n",
            "Epoch 8/15\n",
            "12724/12724 [==============================] - 6s 475us/step - loss: 0.6864 - acc: 0.5548 - auc: 0.5025\n",
            "Epoch 9/15\n",
            "12724/12724 [==============================] - 6s 466us/step - loss: 0.6869 - acc: 0.5546 - auc: 0.5022\n",
            "Epoch 10/15\n",
            "12724/12724 [==============================] - 6s 477us/step - loss: 0.6864 - acc: 0.5546 - auc: 0.5017\n",
            "Epoch 11/15\n",
            "12724/12724 [==============================] - 6s 472us/step - loss: 0.6881 - acc: 0.5546 - auc: 0.5013\n",
            "Epoch 12/15\n",
            "12724/12724 [==============================] - 6s 472us/step - loss: 0.6873 - acc: 0.5545 - auc: 0.5009\n",
            "Epoch 13/15\n",
            "12724/12724 [==============================] - 6s 471us/step - loss: 0.6866 - acc: 0.5544 - auc: 0.5005\n",
            "Epoch 14/15\n",
            "12724/12724 [==============================] - 6s 469us/step - loss: 0.6867 - acc: 0.5545 - auc: 0.5005\n",
            "Epoch 15/15\n",
            "12724/12724 [==============================] - 6s 466us/step - loss: 0.6866 - acc: 0.5545 - auc: 0.4999\n",
            "3180/3180 [==============================] - 1s 284us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 7s 554us/step - loss: 0.6379 - acc: 0.6247 - auc: 0.5382\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.4122 - acc: 0.8240 - auc: 0.7364\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 465us/step - loss: 0.3567 - acc: 0.8590 - auc: 0.8145\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.3244 - acc: 0.8768 - auc: 0.8462\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.3213 - acc: 0.8781 - auc: 0.8653\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 466us/step - loss: 0.2934 - acc: 0.8894 - auc: 0.8766\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2717 - acc: 0.9027 - auc: 0.8871\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2719 - acc: 0.9012 - auc: 0.8948\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.2649 - acc: 0.9025 - auc: 0.9008\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.2519 - acc: 0.9062 - auc: 0.9059\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.2494 - acc: 0.9104 - auc: 0.9101\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.2439 - acc: 0.9115 - auc: 0.9138\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 464us/step - loss: 0.2303 - acc: 0.9154 - auc: 0.9174\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.2300 - acc: 0.9187 - auc: 0.9205\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2249 - acc: 0.9186 - auc: 0.9232\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 467us/step - loss: 0.2236 - acc: 0.9191 - auc: 0.9256\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.2240 - acc: 0.9181 - auc: 0.9277\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2146 - acc: 0.9249 - auc: 0.9297\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.2101 - acc: 0.9249 - auc: 0.9316\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.2015 - acc: 0.9296 - auc: 0.9334\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.1979 - acc: 0.9326 - auc: 0.9353\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.1960 - acc: 0.9301 - auc: 0.9368\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 466us/step - loss: 0.1877 - acc: 0.9327 - auc: 0.9384\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 462us/step - loss: 0.1810 - acc: 0.9365 - auc: 0.9400\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 466us/step - loss: 0.1791 - acc: 0.9382 - auc: 0.9415\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.1737 - acc: 0.9408 - auc: 0.9429\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 467us/step - loss: 0.1879 - acc: 0.9336 - auc: 0.9441\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.1630 - acc: 0.9417 - auc: 0.9453\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 464us/step - loss: 0.1628 - acc: 0.9425 - auc: 0.9467\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 467us/step - loss: 0.1527 - acc: 0.9462 - auc: 0.9480\n",
            "3181/3181 [==============================] - 1s 296us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 7s 572us/step - loss: 0.6968 - acc: 0.5411 - auc: 0.5062\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.6887 - acc: 0.5542 - auc: 0.4977\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6875 - acc: 0.5546 - auc: 0.4959\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.6873 - acc: 0.5532 - auc: 0.4980\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.6829 - acc: 0.5654 - auc: 0.5009\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6868 - acc: 0.5553 - auc: 0.5039\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.6859 - acc: 0.5557 - auc: 0.5032\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.5849 - acc: 0.6873 - auc: 0.5139\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.3916 - acc: 0.8403 - auc: 0.5704\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.3243 - acc: 0.8778 - auc: 0.6270\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.3138 - acc: 0.8834 - auc: 0.6727\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 467us/step - loss: 0.2961 - acc: 0.8893 - auc: 0.7085\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.2738 - acc: 0.8999 - auc: 0.7378\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.2686 - acc: 0.8992 - auc: 0.7618\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.2629 - acc: 0.9043 - auc: 0.7814\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.2521 - acc: 0.9086 - auc: 0.7980\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.2467 - acc: 0.9091 - auc: 0.8121\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.2401 - acc: 0.9125 - auc: 0.8242\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2369 - acc: 0.9124 - auc: 0.8349\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2288 - acc: 0.9167 - auc: 0.8441\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.2274 - acc: 0.9187 - auc: 0.8523\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2251 - acc: 0.9168 - auc: 0.8596\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 467us/step - loss: 0.2169 - acc: 0.9227 - auc: 0.8661\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.2092 - acc: 0.9264 - auc: 0.8722\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.2123 - acc: 0.9232 - auc: 0.8776\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 466us/step - loss: 0.1983 - acc: 0.9286 - auc: 0.8825\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.2005 - acc: 0.9278 - auc: 0.8872\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 465us/step - loss: 0.1987 - acc: 0.9310 - auc: 0.8913\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.1928 - acc: 0.9318 - auc: 0.8951\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 469us/step - loss: 0.1869 - acc: 0.9327 - auc: 0.8988\n",
            "3181/3181 [==============================] - 1s 313us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 7s 585us/step - loss: 0.6863 - acc: 0.5941 - auc: 0.5249\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.6217 - acc: 0.6491 - auc: 0.6060\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.4909 - acc: 0.7758 - auc: 0.6755\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.4001 - acc: 0.8342 - auc: 0.7409\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.3483 - acc: 0.8637 - auc: 0.7863\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.3257 - acc: 0.8757 - auc: 0.8171\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.3024 - acc: 0.8860 - auc: 0.8376\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.3019 - acc: 0.8858 - auc: 0.8533\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.2827 - acc: 0.8937 - auc: 0.8646\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.2844 - acc: 0.8946 - auc: 0.8734\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2690 - acc: 0.8988 - auc: 0.8809\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.2641 - acc: 0.9034 - auc: 0.8871\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2573 - acc: 0.9062 - auc: 0.8925\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.2630 - acc: 0.9038 - auc: 0.8973\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.2576 - acc: 0.9062 - auc: 0.9008\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2451 - acc: 0.9098 - auc: 0.9041\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.2479 - acc: 0.9106 - auc: 0.9072\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.2408 - acc: 0.9124 - auc: 0.9100\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.2356 - acc: 0.9147 - auc: 0.9127\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.2257 - acc: 0.9181 - auc: 0.9150\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.2261 - acc: 0.9168 - auc: 0.9174\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2248 - acc: 0.9195 - auc: 0.9196\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2215 - acc: 0.9219 - auc: 0.9215\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.2151 - acc: 0.9234 - auc: 0.9232\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.2217 - acc: 0.9220 - auc: 0.9248\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.1988 - acc: 0.9286 - auc: 0.9266\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.1987 - acc: 0.9289 - auc: 0.9283\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.1968 - acc: 0.9322 - auc: 0.9300\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.1971 - acc: 0.9301 - auc: 0.9314\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.1855 - acc: 0.9340 - auc: 0.9330\n",
            "3181/3181 [==============================] - 1s 326us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 7s 586us/step - loss: 0.7185 - acc: 0.5371 - auc: 0.5134\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.5627 - acc: 0.7144 - auc: 0.5745\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.3926 - acc: 0.8395 - auc: 0.7116\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.3532 - acc: 0.8616 - auc: 0.7843\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.3163 - acc: 0.8815 - auc: 0.8232\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.2996 - acc: 0.8884 - auc: 0.8477\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.2806 - acc: 0.8957 - auc: 0.8648\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.2787 - acc: 0.8971 - auc: 0.8771\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.2711 - acc: 0.9010 - auc: 0.8859\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.2647 - acc: 0.9020 - auc: 0.8932\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.2484 - acc: 0.9090 - auc: 0.8990\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.2459 - acc: 0.9128 - auc: 0.9044\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.2417 - acc: 0.9110 - auc: 0.9085\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.2294 - acc: 0.9179 - auc: 0.9126\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.2266 - acc: 0.9178 - auc: 0.9162\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.2207 - acc: 0.9218 - auc: 0.9195\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.2165 - acc: 0.9238 - auc: 0.9223\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.2122 - acc: 0.9264 - auc: 0.9249\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.2074 - acc: 0.9274 - auc: 0.9273\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.1935 - acc: 0.9311 - auc: 0.9297\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.1927 - acc: 0.9319 - auc: 0.9319\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.1888 - acc: 0.9347 - auc: 0.9340\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.1809 - acc: 0.9363 - auc: 0.9359\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.1815 - acc: 0.9386 - auc: 0.9377\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.1755 - acc: 0.9392 - auc: 0.9394\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.1763 - acc: 0.9375 - auc: 0.9411\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.1591 - acc: 0.9466 - auc: 0.9426\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.1689 - acc: 0.9426 - auc: 0.9441\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.1607 - acc: 0.9446 - auc: 0.9455\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.1598 - acc: 0.9430 - auc: 0.9469\n",
            "3181/3181 [==============================] - 1s 372us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12724/12724 [==============================] - 8s 623us/step - loss: 0.6765 - acc: 0.5971 - auc: 0.5217\n",
            "Epoch 2/30\n",
            "12724/12724 [==============================] - 6s 477us/step - loss: 0.4457 - acc: 0.8123 - auc: 0.6956\n",
            "Epoch 3/30\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.3345 - acc: 0.8699 - auc: 0.7979\n",
            "Epoch 4/30\n",
            "12724/12724 [==============================] - 6s 477us/step - loss: 0.3083 - acc: 0.8849 - auc: 0.8438\n",
            "Epoch 5/30\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.2850 - acc: 0.8962 - auc: 0.8683\n",
            "Epoch 6/30\n",
            "12724/12724 [==============================] - 6s 481us/step - loss: 0.2666 - acc: 0.9040 - auc: 0.8839\n",
            "Epoch 7/30\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.2591 - acc: 0.9053 - auc: 0.8949\n",
            "Epoch 8/30\n",
            "12724/12724 [==============================] - 6s 476us/step - loss: 0.2454 - acc: 0.9109 - auc: 0.9036\n",
            "Epoch 9/30\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.2377 - acc: 0.9145 - auc: 0.9100\n",
            "Epoch 10/30\n",
            "12724/12724 [==============================] - 6s 479us/step - loss: 0.2310 - acc: 0.9189 - auc: 0.9153\n",
            "Epoch 11/30\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.2301 - acc: 0.9207 - auc: 0.9194\n",
            "Epoch 12/30\n",
            "12724/12724 [==============================] - 6s 477us/step - loss: 0.2346 - acc: 0.9169 - auc: 0.9227\n",
            "Epoch 13/30\n",
            "12724/12724 [==============================] - 6s 482us/step - loss: 0.2163 - acc: 0.9225 - auc: 0.9257\n",
            "Epoch 14/30\n",
            "12724/12724 [==============================] - 6s 481us/step - loss: 0.2113 - acc: 0.9266 - auc: 0.9287\n",
            "Epoch 15/30\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.2105 - acc: 0.9266 - auc: 0.9310\n",
            "Epoch 16/30\n",
            "12724/12724 [==============================] - 6s 484us/step - loss: 0.2080 - acc: 0.9268 - auc: 0.9333\n",
            "Epoch 17/30\n",
            "12724/12724 [==============================] - 6s 482us/step - loss: 0.1948 - acc: 0.9330 - auc: 0.9354\n",
            "Epoch 18/30\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1952 - acc: 0.9317 - auc: 0.9374\n",
            "Epoch 19/30\n",
            "12724/12724 [==============================] - 6s 486us/step - loss: 0.1954 - acc: 0.9326 - auc: 0.9391\n",
            "Epoch 20/30\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1868 - acc: 0.9374 - auc: 0.9408\n",
            "Epoch 21/30\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1870 - acc: 0.9335 - auc: 0.9422\n",
            "Epoch 22/30\n",
            "12724/12724 [==============================] - 6s 483us/step - loss: 0.1792 - acc: 0.9399 - auc: 0.9437\n",
            "Epoch 23/30\n",
            "12724/12724 [==============================] - 6s 494us/step - loss: 0.1786 - acc: 0.9400 - auc: 0.9451\n",
            "Epoch 24/30\n",
            "12724/12724 [==============================] - 6s 474us/step - loss: 0.1747 - acc: 0.9416 - auc: 0.9463\n",
            "Epoch 25/30\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1650 - acc: 0.9427 - auc: 0.9476\n",
            "Epoch 26/30\n",
            "12724/12724 [==============================] - 6s 481us/step - loss: 0.1731 - acc: 0.9406 - auc: 0.9488\n",
            "Epoch 27/30\n",
            "12724/12724 [==============================] - 6s 482us/step - loss: 0.1649 - acc: 0.9444 - auc: 0.9498\n",
            "Epoch 28/30\n",
            "12724/12724 [==============================] - 6s 485us/step - loss: 0.1556 - acc: 0.9461 - auc: 0.9510\n",
            "Epoch 29/30\n",
            "12724/12724 [==============================] - 6s 486us/step - loss: 0.1489 - acc: 0.9503 - auc: 0.9521\n",
            "Epoch 30/30\n",
            "12724/12724 [==============================] - 6s 482us/step - loss: 0.1484 - acc: 0.9497 - auc: 0.9532\n",
            "3180/3180 [==============================] - 1s 371us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/50\n",
            "12723/12723 [==============================] - 8s 616us/step - loss: 0.7494 - acc: 0.5528 - auc: 0.5116\n",
            "Epoch 2/50\n",
            "12723/12723 [==============================] - 6s 477us/step - loss: 0.6865 - acc: 0.5546 - auc: 0.5052\n",
            "Epoch 3/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.6872 - acc: 0.5545 - auc: 0.5015\n",
            "Epoch 4/50\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6891 - acc: 0.5545 - auc: 0.5007\n",
            "Epoch 5/50\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.5000\n",
            "Epoch 6/50\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6878 - acc: 0.5546 - auc: 0.4984\n",
            "Epoch 7/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 8/50\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.6875 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 9/50\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4985\n",
            "Epoch 10/50\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4986\n",
            "Epoch 11/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.6875 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 12/50\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.4986\n",
            "Epoch 13/50\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4984\n",
            "Epoch 14/50\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.4985\n",
            "Epoch 15/50\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4984\n",
            "Epoch 16/50\n",
            "12723/12723 [==============================] - 6s 471us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 17/50\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4990\n",
            "Epoch 18/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 19/50\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4984\n",
            "Epoch 20/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 21/50\n",
            "12723/12723 [==============================] - 6s 477us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 22/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 23/50\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 24/50\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 25/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4983\n",
            "Epoch 26/50\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4983\n",
            "Epoch 27/50\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4984\n",
            "Epoch 28/50\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4986\n",
            "Epoch 29/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 30/50\n",
            "12723/12723 [==============================] - 6s 477us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 31/50\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4986\n",
            "Epoch 32/50\n",
            "12723/12723 [==============================] - 6s 470us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4986\n",
            "Epoch 33/50\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4986\n",
            "Epoch 34/50\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 35/50\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 36/50\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 37/50\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 38/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4985\n",
            "Epoch 39/50\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4984\n",
            "Epoch 40/50\n",
            "12723/12723 [==============================] - 6s 468us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4984\n",
            "Epoch 41/50\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4983\n",
            "Epoch 42/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4983\n",
            "Epoch 43/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4983\n",
            "Epoch 44/50\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 45/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 46/50\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 47/50\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 48/50\n",
            "12723/12723 [==============================] - 6s 472us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 49/50\n",
            "12723/12723 [==============================] - 6s 477us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 50/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4981\n",
            "3181/3181 [==============================] - 1s 383us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/50\n",
            "12723/12723 [==============================] - 8s 632us/step - loss: 0.6976 - acc: 0.5476 - auc: 0.5036\n",
            "Epoch 2/50\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.6200 - acc: 0.6408 - auc: 0.5358\n",
            "Epoch 3/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.3860 - acc: 0.8452 - auc: 0.6774\n",
            "Epoch 4/50\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.3243 - acc: 0.8752 - auc: 0.7714\n",
            "Epoch 5/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2967 - acc: 0.8878 - auc: 0.8190\n",
            "Epoch 6/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2803 - acc: 0.8986 - auc: 0.8478\n",
            "Epoch 7/50\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.2674 - acc: 0.8998 - auc: 0.8665\n",
            "Epoch 8/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.2537 - acc: 0.9090 - auc: 0.8801\n",
            "Epoch 9/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.2467 - acc: 0.9122 - auc: 0.8904\n",
            "Epoch 10/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2416 - acc: 0.9124 - auc: 0.8983\n",
            "Epoch 11/50\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.2375 - acc: 0.9158 - auc: 0.9047\n",
            "Epoch 12/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2247 - acc: 0.9212 - auc: 0.9100\n",
            "Epoch 13/50\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.2211 - acc: 0.9221 - auc: 0.9146\n",
            "Epoch 14/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.2178 - acc: 0.9225 - auc: 0.9186\n",
            "Epoch 15/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.2117 - acc: 0.9242 - auc: 0.9219\n",
            "Epoch 16/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.2077 - acc: 0.9256 - auc: 0.9251\n",
            "Epoch 17/50\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.2020 - acc: 0.9295 - auc: 0.9279\n",
            "Epoch 18/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.1946 - acc: 0.9332 - auc: 0.9305\n",
            "Epoch 19/50\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.1912 - acc: 0.9350 - auc: 0.9327\n",
            "Epoch 20/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1875 - acc: 0.9357 - auc: 0.9348\n",
            "Epoch 21/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.1812 - acc: 0.9363 - auc: 0.9368\n",
            "Epoch 22/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.1797 - acc: 0.9378 - auc: 0.9386\n",
            "Epoch 23/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.1728 - acc: 0.9403 - auc: 0.9405\n",
            "Epoch 24/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.1678 - acc: 0.9425 - auc: 0.9422\n",
            "Epoch 25/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.1680 - acc: 0.9429 - auc: 0.9437\n",
            "Epoch 26/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.1641 - acc: 0.9432 - auc: 0.9452\n",
            "Epoch 27/50\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.1596 - acc: 0.9473 - auc: 0.9466\n",
            "Epoch 28/50\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.1638 - acc: 0.9426 - auc: 0.9479\n",
            "Epoch 29/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.1550 - acc: 0.9485 - auc: 0.9491\n",
            "Epoch 30/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.1468 - acc: 0.9499 - auc: 0.9504\n",
            "Epoch 31/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.1421 - acc: 0.9507 - auc: 0.9516\n",
            "Epoch 32/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.1509 - acc: 0.9485 - auc: 0.9528\n",
            "Epoch 33/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.1338 - acc: 0.9536 - auc: 0.9538\n",
            "Epoch 34/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.1322 - acc: 0.9561 - auc: 0.9550\n",
            "Epoch 35/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.1267 - acc: 0.9573 - auc: 0.9561\n",
            "Epoch 36/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.1302 - acc: 0.9566 - auc: 0.9571\n",
            "Epoch 37/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.1199 - acc: 0.9611 - auc: 0.9581\n",
            "Epoch 38/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.1113 - acc: 0.9624 - auc: 0.9591\n",
            "Epoch 39/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.1201 - acc: 0.9593 - auc: 0.9601\n",
            "Epoch 40/50\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.1160 - acc: 0.9623 - auc: 0.9610\n",
            "Epoch 41/50\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.1039 - acc: 0.9653 - auc: 0.9619\n",
            "Epoch 42/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.1031 - acc: 0.9675 - auc: 0.9629\n",
            "Epoch 43/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.0991 - acc: 0.9679 - auc: 0.9637\n",
            "Epoch 44/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.0988 - acc: 0.9665 - auc: 0.9646\n",
            "Epoch 45/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.1050 - acc: 0.9638 - auc: 0.9654\n",
            "Epoch 46/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.0879 - acc: 0.9716 - auc: 0.9662\n",
            "Epoch 47/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.0830 - acc: 0.9726 - auc: 0.9670\n",
            "Epoch 48/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.0877 - acc: 0.9712 - auc: 0.9678\n",
            "Epoch 49/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.0813 - acc: 0.9736 - auc: 0.9685\n",
            "Epoch 50/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.0857 - acc: 0.9700 - auc: 0.9693\n",
            "3181/3181 [==============================] - 1s 392us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/50\n",
            "12723/12723 [==============================] - 8s 666us/step - loss: 0.7058 - acc: 0.5527 - auc: 0.5059\n",
            "Epoch 2/50\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.6879 - acc: 0.5546 - auc: 0.5031\n",
            "Epoch 3/50\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.6870 - acc: 0.5546 - auc: 0.5015\n",
            "Epoch 4/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6864 - acc: 0.5547 - auc: 0.5003\n",
            "Epoch 5/50\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.6862 - acc: 0.5547 - auc: 0.4998\n",
            "Epoch 6/50\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.6869 - acc: 0.5547 - auc: 0.4999\n",
            "Epoch 7/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6880 - acc: 0.5546 - auc: 0.4998\n",
            "Epoch 8/50\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.6870 - acc: 0.5546 - auc: 0.5005\n",
            "Epoch 9/50\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.6866 - acc: 0.5546 - auc: 0.5003\n",
            "Epoch 10/50\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.6863 - acc: 0.5546 - auc: 0.5001\n",
            "Epoch 11/50\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.6860 - acc: 0.5547 - auc: 0.5000\n",
            "Epoch 12/50\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.6864 - acc: 0.5548 - auc: 0.5005\n",
            "Epoch 13/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.6856 - acc: 0.5547 - auc: 0.5008\n",
            "Epoch 14/50\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.6860 - acc: 0.5546 - auc: 0.5011\n",
            "Epoch 15/50\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.6862 - acc: 0.5547 - auc: 0.5013\n",
            "Epoch 16/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6916 - acc: 0.5546 - auc: 0.5012\n",
            "Epoch 17/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5010\n",
            "Epoch 18/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5007\n",
            "Epoch 19/50\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.5006\n",
            "Epoch 20/50\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.5003\n",
            "Epoch 21/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5002\n",
            "Epoch 22/50\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4999\n",
            "Epoch 23/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4996\n",
            "Epoch 24/50\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.4996\n",
            "Epoch 25/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4994\n",
            "Epoch 26/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4993\n",
            "Epoch 27/50\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4991\n",
            "Epoch 28/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4991\n",
            "Epoch 29/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4991\n",
            "Epoch 30/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4989\n",
            "Epoch 31/50\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4989\n",
            "Epoch 32/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4989\n",
            "Epoch 33/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4989\n",
            "Epoch 34/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4988\n",
            "Epoch 35/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 36/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 37/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 38/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 39/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 40/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4985\n",
            "Epoch 41/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4985\n",
            "Epoch 42/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4984\n",
            "Epoch 43/50\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4984\n",
            "Epoch 44/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4983\n",
            "Epoch 45/50\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4983\n",
            "Epoch 46/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 47/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 48/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 49/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 50/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4981\n",
            "3181/3181 [==============================] - 1s 420us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/50\n",
            "12723/12723 [==============================] - 8s 663us/step - loss: 0.7211 - acc: 0.5594 - auc: 0.5017\n",
            "Epoch 2/50\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.5757 - acc: 0.6996 - auc: 0.5958\n",
            "Epoch 3/50\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.4267 - acc: 0.8183 - auc: 0.7027\n",
            "Epoch 4/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.3558 - acc: 0.8571 - auc: 0.7737\n",
            "Epoch 5/50\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.3282 - acc: 0.8722 - auc: 0.8141\n",
            "Epoch 6/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.3016 - acc: 0.8859 - auc: 0.8397\n",
            "Epoch 7/50\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.3025 - acc: 0.8838 - auc: 0.8573\n",
            "Epoch 8/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.2885 - acc: 0.8911 - auc: 0.8695\n",
            "Epoch 9/50\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.2848 - acc: 0.8941 - auc: 0.8789\n",
            "Epoch 10/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2795 - acc: 0.8968 - auc: 0.8864\n",
            "Epoch 11/50\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.2680 - acc: 0.9006 - auc: 0.8924\n",
            "Epoch 12/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2746 - acc: 0.8949 - auc: 0.8974\n",
            "Epoch 13/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2695 - acc: 0.8991 - auc: 0.9011\n",
            "Epoch 14/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2586 - acc: 0.9053 - auc: 0.9048\n",
            "Epoch 15/50\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.2621 - acc: 0.9023 - auc: 0.9079\n",
            "Epoch 16/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.2654 - acc: 0.9012 - auc: 0.9103\n",
            "Epoch 17/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.2505 - acc: 0.9067 - auc: 0.9126\n",
            "Epoch 18/50\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.2475 - acc: 0.9087 - auc: 0.9150\n",
            "Epoch 19/50\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2515 - acc: 0.9036 - auc: 0.9170\n",
            "Epoch 20/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2393 - acc: 0.9115 - auc: 0.9190\n",
            "Epoch 21/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2349 - acc: 0.9154 - auc: 0.9208\n",
            "Epoch 22/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.2306 - acc: 0.9161 - auc: 0.9225\n",
            "Epoch 23/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.2345 - acc: 0.9143 - auc: 0.9243\n",
            "Epoch 24/50\n",
            "12723/12723 [==============================] - 6s 477us/step - loss: 0.2243 - acc: 0.9188 - auc: 0.9258\n",
            "Epoch 25/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2207 - acc: 0.9207 - auc: 0.9274\n",
            "Epoch 26/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.2143 - acc: 0.9222 - auc: 0.9289\n",
            "Epoch 27/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.2126 - acc: 0.9229 - auc: 0.9303\n",
            "Epoch 28/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.2289 - acc: 0.9179 - auc: 0.9314\n",
            "Epoch 29/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2059 - acc: 0.9248 - auc: 0.9326\n",
            "Epoch 30/50\n",
            "12723/12723 [==============================] - 6s 474us/step - loss: 0.2135 - acc: 0.9228 - auc: 0.9338\n",
            "Epoch 31/50\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.2103 - acc: 0.9234 - auc: 0.9348\n",
            "Epoch 32/50\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.2035 - acc: 0.9256 - auc: 0.9359\n",
            "Epoch 33/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.2106 - acc: 0.9238 - auc: 0.9369\n",
            "Epoch 34/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.2016 - acc: 0.9253 - auc: 0.9378\n",
            "Epoch 35/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.1949 - acc: 0.9282 - auc: 0.9388\n",
            "Epoch 36/50\n",
            "12723/12723 [==============================] - 6s 477us/step - loss: 0.1873 - acc: 0.9324 - auc: 0.9399\n",
            "Epoch 37/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.1761 - acc: 0.9353 - auc: 0.9409\n",
            "Epoch 38/50\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.1850 - acc: 0.9339 - auc: 0.9419\n",
            "Epoch 39/50\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1848 - acc: 0.9326 - auc: 0.9429\n",
            "Epoch 40/50\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.1685 - acc: 0.9388 - auc: 0.9438\n",
            "Epoch 41/50\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.1748 - acc: 0.9361 - auc: 0.9448\n",
            "Epoch 42/50\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.1616 - acc: 0.9418 - auc: 0.9458\n",
            "Epoch 43/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.1630 - acc: 0.9416 - auc: 0.9467\n",
            "Epoch 44/50\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.1602 - acc: 0.9413 - auc: 0.9476\n",
            "Epoch 45/50\n",
            "12723/12723 [==============================] - 6s 475us/step - loss: 0.1622 - acc: 0.9407 - auc: 0.9485\n",
            "Epoch 46/50\n",
            "12723/12723 [==============================] - 6s 477us/step - loss: 0.1539 - acc: 0.9425 - auc: 0.9493\n",
            "Epoch 47/50\n",
            "12723/12723 [==============================] - 6s 473us/step - loss: 0.1503 - acc: 0.9432 - auc: 0.9503\n",
            "Epoch 48/50\n",
            "12723/12723 [==============================] - 6s 477us/step - loss: 0.1520 - acc: 0.9444 - auc: 0.9511\n",
            "Epoch 49/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.1480 - acc: 0.9458 - auc: 0.9520\n",
            "Epoch 50/50\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.1606 - acc: 0.9433 - auc: 0.9527\n",
            "3181/3181 [==============================] - 1s 443us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/50\n",
            "12724/12724 [==============================] - 9s 674us/step - loss: 0.6634 - acc: 0.6196 - auc: 0.5248\n",
            "Epoch 2/50\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.4156 - acc: 0.8254 - auc: 0.7183\n",
            "Epoch 3/50\n",
            "12724/12724 [==============================] - 6s 481us/step - loss: 0.3610 - acc: 0.8556 - auc: 0.8029\n",
            "Epoch 4/50\n",
            "12724/12724 [==============================] - 6s 481us/step - loss: 0.3305 - acc: 0.8710 - auc: 0.8409\n",
            "Epoch 5/50\n",
            "12724/12724 [==============================] - 6s 484us/step - loss: 0.3224 - acc: 0.8755 - auc: 0.8612\n",
            "Epoch 6/50\n",
            "12724/12724 [==============================] - 6s 477us/step - loss: 0.3043 - acc: 0.8824 - auc: 0.8741\n",
            "Epoch 7/50\n",
            "12724/12724 [==============================] - 6s 476us/step - loss: 0.2919 - acc: 0.8913 - auc: 0.8834\n",
            "Epoch 8/50\n",
            "12724/12724 [==============================] - 6s 479us/step - loss: 0.2789 - acc: 0.8958 - auc: 0.8910\n",
            "Epoch 9/50\n",
            "12724/12724 [==============================] - 6s 479us/step - loss: 0.2741 - acc: 0.8965 - auc: 0.8971\n",
            "Epoch 10/50\n",
            "12724/12724 [==============================] - 6s 485us/step - loss: 0.2705 - acc: 0.8983 - auc: 0.9020\n",
            "Epoch 11/50\n",
            "12724/12724 [==============================] - 6s 481us/step - loss: 0.2667 - acc: 0.9000 - auc: 0.9060\n",
            "Epoch 12/50\n",
            "12724/12724 [==============================] - 6s 475us/step - loss: 0.2583 - acc: 0.9053 - auc: 0.9094\n",
            "Epoch 13/50\n",
            "12724/12724 [==============================] - 6s 481us/step - loss: 0.2526 - acc: 0.9095 - auc: 0.9123\n",
            "Epoch 14/50\n",
            "12724/12724 [==============================] - 6s 475us/step - loss: 0.2478 - acc: 0.9099 - auc: 0.9151\n",
            "Epoch 15/50\n",
            "12724/12724 [==============================] - 6s 479us/step - loss: 0.2379 - acc: 0.9126 - auc: 0.9174\n",
            "Epoch 16/50\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.2436 - acc: 0.9097 - auc: 0.9201\n",
            "Epoch 17/50\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.2384 - acc: 0.9152 - auc: 0.9220\n",
            "Epoch 18/50\n",
            "12724/12724 [==============================] - 6s 488us/step - loss: 0.2332 - acc: 0.9149 - auc: 0.9239\n",
            "Epoch 19/50\n",
            "12724/12724 [==============================] - 6s 485us/step - loss: 0.2170 - acc: 0.9208 - auc: 0.9258\n",
            "Epoch 20/50\n",
            "12724/12724 [==============================] - 6s 472us/step - loss: 0.2203 - acc: 0.9203 - auc: 0.9278\n",
            "Epoch 21/50\n",
            "12724/12724 [==============================] - 6s 481us/step - loss: 0.2103 - acc: 0.9240 - auc: 0.9296\n",
            "Epoch 22/50\n",
            "12724/12724 [==============================] - 6s 472us/step - loss: 0.2151 - acc: 0.9217 - auc: 0.9313\n",
            "Epoch 23/50\n",
            "12724/12724 [==============================] - 6s 477us/step - loss: 0.2061 - acc: 0.9269 - auc: 0.9328\n",
            "Epoch 24/50\n",
            "12724/12724 [==============================] - 6s 483us/step - loss: 0.2046 - acc: 0.9265 - auc: 0.9343\n",
            "Epoch 25/50\n",
            "12724/12724 [==============================] - 6s 475us/step - loss: 0.1942 - acc: 0.9311 - auc: 0.9358\n",
            "Epoch 26/50\n",
            "12724/12724 [==============================] - 6s 477us/step - loss: 0.1969 - acc: 0.9289 - auc: 0.9371\n",
            "Epoch 27/50\n",
            "12724/12724 [==============================] - 6s 475us/step - loss: 0.1918 - acc: 0.9316 - auc: 0.9384\n",
            "Epoch 28/50\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1806 - acc: 0.9358 - auc: 0.9398\n",
            "Epoch 29/50\n",
            "12724/12724 [==============================] - 6s 475us/step - loss: 0.1734 - acc: 0.9374 - auc: 0.9412\n",
            "Epoch 30/50\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.1775 - acc: 0.9375 - auc: 0.9425\n",
            "Epoch 31/50\n",
            "12724/12724 [==============================] - 6s 479us/step - loss: 0.1680 - acc: 0.9419 - auc: 0.9438\n",
            "Epoch 32/50\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1598 - acc: 0.9428 - auc: 0.9450\n",
            "Epoch 33/50\n",
            "12724/12724 [==============================] - 6s 479us/step - loss: 0.1801 - acc: 0.9376 - auc: 0.9462\n",
            "Epoch 34/50\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.1594 - acc: 0.9429 - auc: 0.9472\n",
            "Epoch 35/50\n",
            "12724/12724 [==============================] - 6s 476us/step - loss: 0.1558 - acc: 0.9438 - auc: 0.9483\n",
            "Epoch 36/50\n",
            "12724/12724 [==============================] - 6s 474us/step - loss: 0.1493 - acc: 0.9470 - auc: 0.9495\n",
            "Epoch 37/50\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.1412 - acc: 0.9496 - auc: 0.9506\n",
            "Epoch 38/50\n",
            "12724/12724 [==============================] - 6s 487us/step - loss: 0.1356 - acc: 0.9532 - auc: 0.9517\n",
            "Epoch 39/50\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1418 - acc: 0.9506 - auc: 0.9528\n",
            "Epoch 40/50\n",
            "12724/12724 [==============================] - 6s 476us/step - loss: 0.1417 - acc: 0.9501 - auc: 0.9537\n",
            "Epoch 41/50\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1356 - acc: 0.9540 - auc: 0.9547\n",
            "Epoch 42/50\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1278 - acc: 0.9544 - auc: 0.9556\n",
            "Epoch 43/50\n",
            "12724/12724 [==============================] - 6s 475us/step - loss: 0.1317 - acc: 0.9551 - auc: 0.9565\n",
            "Epoch 44/50\n",
            "12724/12724 [==============================] - 6s 481us/step - loss: 0.1211 - acc: 0.9568 - auc: 0.9575\n",
            "Epoch 45/50\n",
            "12724/12724 [==============================] - 6s 475us/step - loss: 0.1052 - acc: 0.9631 - auc: 0.9584\n",
            "Epoch 46/50\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1208 - acc: 0.9561 - auc: 0.9593\n",
            "Epoch 47/50\n",
            "12724/12724 [==============================] - 6s 472us/step - loss: 0.1106 - acc: 0.9633 - auc: 0.9601\n",
            "Epoch 48/50\n",
            "12724/12724 [==============================] - 6s 481us/step - loss: 0.1139 - acc: 0.9609 - auc: 0.9610\n",
            "Epoch 49/50\n",
            "12724/12724 [==============================] - 6s 478us/step - loss: 0.1050 - acc: 0.9648 - auc: 0.9618\n",
            "Epoch 50/50\n",
            "12724/12724 [==============================] - 6s 480us/step - loss: 0.0981 - acc: 0.9651 - auc: 0.9626\n",
            "3180/3180 [==============================] - 1s 463us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/15\n",
            "12723/12723 [==============================] - 9s 680us/step - loss: 0.7228 - acc: 0.5419 - auc: 0.5007\n",
            "Epoch 2/15\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.6895 - acc: 0.5513 - auc: 0.5077\n",
            "Epoch 3/15\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.6852 - acc: 0.5569 - auc: 0.5091\n",
            "Epoch 4/15\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.6122 - acc: 0.6601 - auc: 0.5324\n",
            "Epoch 5/15\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.4444 - acc: 0.8031 - auc: 0.6093\n",
            "Epoch 6/15\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.3727 - acc: 0.8473 - auc: 0.6820\n",
            "Epoch 7/15\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.3308 - acc: 0.8709 - auc: 0.7334\n",
            "Epoch 8/15\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.3058 - acc: 0.8861 - auc: 0.7708\n",
            "Epoch 9/15\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.2889 - acc: 0.8904 - auc: 0.7988\n",
            "Epoch 10/15\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2804 - acc: 0.8929 - auc: 0.8195\n",
            "Epoch 11/15\n",
            "12723/12723 [==============================] - 6s 478us/step - loss: 0.2832 - acc: 0.8966 - auc: 0.8355\n",
            "Epoch 12/15\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2686 - acc: 0.8985 - auc: 0.8483\n",
            "Epoch 13/15\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.2589 - acc: 0.9047 - auc: 0.8586\n",
            "Epoch 14/15\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.2518 - acc: 0.9072 - auc: 0.8677\n",
            "Epoch 15/15\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.2514 - acc: 0.9091 - auc: 0.8752\n",
            "3181/3181 [==============================] - 2s 479us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/15\n",
            "12723/12723 [==============================] - 9s 737us/step - loss: 0.7109 - acc: 0.5419 - auc: 0.5018\n",
            "Epoch 2/15\n",
            "12723/12723 [==============================] - 7s 537us/step - loss: 0.6864 - acc: 0.5580 - auc: 0.5025\n",
            "Epoch 3/15\n",
            "12723/12723 [==============================] - 6s 510us/step - loss: 0.6861 - acc: 0.5549 - auc: 0.5098\n",
            "Epoch 4/15\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.6868 - acc: 0.5547 - auc: 0.5102\n",
            "Epoch 5/15\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.6867 - acc: 0.5547 - auc: 0.5083\n",
            "Epoch 6/15\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.6864 - acc: 0.5552 - auc: 0.5077\n",
            "Epoch 7/15\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.6876 - acc: 0.5546 - auc: 0.5070\n",
            "Epoch 8/15\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.6875 - acc: 0.5546 - auc: 0.5058\n",
            "Epoch 9/15\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.6875 - acc: 0.5546 - auc: 0.5048\n",
            "Epoch 10/15\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5043\n",
            "Epoch 11/15\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5038\n",
            "Epoch 12/15\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5031\n",
            "Epoch 13/15\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5025\n",
            "Epoch 14/15\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.5020\n",
            "Epoch 15/15\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.8983 - acc: 0.5532 - auc: 0.5019\n",
            "3181/3181 [==============================] - 2s 503us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/15\n",
            "12723/12723 [==============================] - 9s 728us/step - loss: 0.7044 - acc: 0.5479 - auc: 0.4989\n",
            "Epoch 2/15\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.6875 - acc: 0.5538 - auc: 0.4975\n",
            "Epoch 3/15\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.6914 - acc: 0.5533 - auc: 0.4985\n",
            "Epoch 4/15\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6872 - acc: 0.5559 - auc: 0.5006\n",
            "Epoch 5/15\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.6862 - acc: 0.5549 - auc: 0.5046\n",
            "Epoch 6/15\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.6711 - acc: 0.5852 - auc: 0.5081\n",
            "Epoch 7/15\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.4941 - acc: 0.7661 - auc: 0.5477\n",
            "Epoch 8/15\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.3936 - acc: 0.8357 - auc: 0.6109\n",
            "Epoch 9/15\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.3579 - acc: 0.8529 - auc: 0.6625\n",
            "Epoch 10/15\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.3463 - acc: 0.8610 - auc: 0.7016\n",
            "Epoch 11/15\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.3178 - acc: 0.8747 - auc: 0.7322\n",
            "Epoch 12/15\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.3191 - acc: 0.8773 - auc: 0.7576\n",
            "Epoch 13/15\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.3048 - acc: 0.8832 - auc: 0.7776\n",
            "Epoch 14/15\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.2924 - acc: 0.8888 - auc: 0.7947\n",
            "Epoch 15/15\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.2936 - acc: 0.8880 - auc: 0.8086\n",
            "3181/3181 [==============================] - 2s 526us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/15\n",
            "12723/12723 [==============================] - 9s 719us/step - loss: 0.7102 - acc: 0.5456 - auc: 0.4967\n",
            "Epoch 2/15\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.6624 - acc: 0.5925 - auc: 0.5152\n",
            "Epoch 3/15\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.4756 - acc: 0.7843 - auc: 0.6187\n",
            "Epoch 4/15\n",
            "12723/12723 [==============================] - 6s 480us/step - loss: 0.3766 - acc: 0.8485 - auc: 0.7193\n",
            "Epoch 5/15\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.3368 - acc: 0.8684 - auc: 0.7757\n",
            "Epoch 6/15\n",
            "12723/12723 [==============================] - 6s 476us/step - loss: 0.3060 - acc: 0.8830 - auc: 0.8123\n",
            "Epoch 7/15\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.2977 - acc: 0.8878 - auc: 0.8370\n",
            "Epoch 8/15\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.2838 - acc: 0.8938 - auc: 0.8538\n",
            "Epoch 9/15\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.2796 - acc: 0.8948 - auc: 0.8665\n",
            "Epoch 10/15\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.2676 - acc: 0.9036 - auc: 0.8763\n",
            "Epoch 11/15\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.2601 - acc: 0.9047 - auc: 0.8845\n",
            "Epoch 12/15\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.2559 - acc: 0.9054 - auc: 0.8911\n",
            "Epoch 13/15\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.2458 - acc: 0.9105 - auc: 0.8970\n",
            "Epoch 14/15\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.2422 - acc: 0.9106 - auc: 0.9017\n",
            "Epoch 15/15\n",
            "12723/12723 [==============================] - 6s 479us/step - loss: 0.2338 - acc: 0.9164 - auc: 0.9062\n",
            "3181/3181 [==============================] - 2s 547us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/15\n",
            "12724/12724 [==============================] - 10s 763us/step - loss: 0.7300 - acc: 0.5476 - auc: 0.4790\n",
            "Epoch 2/15\n",
            "12724/12724 [==============================] - 7s 518us/step - loss: 0.6864 - acc: 0.5545 - auc: 0.4962\n",
            "Epoch 3/15\n",
            "12724/12724 [==============================] - 7s 512us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 4/15\n",
            "12724/12724 [==============================] - 6s 492us/step - loss: 0.6874 - acc: 0.5545 - auc: 0.4985\n",
            "Epoch 5/15\n",
            "12724/12724 [==============================] - 6s 483us/step - loss: 0.6876 - acc: 0.5545 - auc: 0.4989\n",
            "Epoch 6/15\n",
            "12724/12724 [==============================] - 6s 489us/step - loss: 0.6875 - acc: 0.5545 - auc: 0.4982\n",
            "Epoch 7/15\n",
            "12724/12724 [==============================] - 6s 487us/step - loss: 0.6873 - acc: 0.5545 - auc: 0.4982\n",
            "Epoch 8/15\n",
            "12724/12724 [==============================] - 6s 485us/step - loss: 0.6878 - acc: 0.5545 - auc: 0.4989\n",
            "Epoch 9/15\n",
            "12724/12724 [==============================] - 6s 484us/step - loss: 0.6875 - acc: 0.5545 - auc: 0.4988\n",
            "Epoch 10/15\n",
            "12724/12724 [==============================] - 6s 492us/step - loss: 0.6876 - acc: 0.5545 - auc: 0.4984\n",
            "Epoch 11/15\n",
            "12724/12724 [==============================] - 6s 485us/step - loss: 0.6873 - acc: 0.5545 - auc: 0.4979\n",
            "Epoch 12/15\n",
            "12724/12724 [==============================] - 6s 488us/step - loss: 0.6872 - acc: 0.5545 - auc: 0.4983\n",
            "Epoch 13/15\n",
            "12724/12724 [==============================] - 6s 490us/step - loss: 0.6873 - acc: 0.5545 - auc: 0.4984\n",
            "Epoch 14/15\n",
            "12724/12724 [==============================] - 6s 486us/step - loss: 0.6873 - acc: 0.5545 - auc: 0.4984\n",
            "Epoch 15/15\n",
            "12724/12724 [==============================] - 6s 487us/step - loss: 0.6873 - acc: 0.5545 - auc: 0.4984\n",
            "3180/3180 [==============================] - 2s 571us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 10s 766us/step - loss: 0.6987 - acc: 0.5489 - auc: 0.5195\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 505us/step - loss: 0.6945 - acc: 0.5547 - auc: 0.5163\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 509us/step - loss: 0.6872 - acc: 0.5587 - auc: 0.5142\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.6865 - acc: 0.5547 - auc: 0.5117\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.6842 - acc: 0.5592 - auc: 0.5106\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.5559 - acc: 0.7177 - auc: 0.5358\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.4253 - acc: 0.8166 - auc: 0.6006\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.3548 - acc: 0.8594 - auc: 0.6590\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.3226 - acc: 0.8746 - auc: 0.7052\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.3003 - acc: 0.8850 - auc: 0.7411\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.2959 - acc: 0.8889 - auc: 0.7688\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.2815 - acc: 0.8952 - auc: 0.7906\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.2684 - acc: 0.8983 - auc: 0.8084\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.2653 - acc: 0.9018 - auc: 0.8232\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2512 - acc: 0.9069 - auc: 0.8357\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2509 - acc: 0.9080 - auc: 0.8464\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.2442 - acc: 0.9112 - auc: 0.8554\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2523 - acc: 0.9086 - auc: 0.8634\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.2459 - acc: 0.9103 - auc: 0.8699\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.2323 - acc: 0.9161 - auc: 0.8757\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.2304 - acc: 0.9183 - auc: 0.8812\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2289 - acc: 0.9184 - auc: 0.8859\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2226 - acc: 0.9211 - auc: 0.8903\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2104 - acc: 0.9249 - auc: 0.8945\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2039 - acc: 0.9266 - auc: 0.8985\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.2112 - acc: 0.9223 - auc: 0.9018\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 481us/step - loss: 0.1992 - acc: 0.9305 - auc: 0.9051\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.1940 - acc: 0.9304 - auc: 0.9082\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1920 - acc: 0.9328 - auc: 0.9111\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.1919 - acc: 0.9311 - auc: 0.9137\n",
            "3181/3181 [==============================] - 2s 588us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 10s 769us/step - loss: 0.6600 - acc: 0.6048 - auc: 0.5151\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.3987 - acc: 0.8409 - auc: 0.7186\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.3391 - acc: 0.8658 - auc: 0.8141\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.3066 - acc: 0.8856 - auc: 0.8513\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2840 - acc: 0.8934 - auc: 0.8731\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.2772 - acc: 0.8991 - auc: 0.8859\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.2649 - acc: 0.9019 - auc: 0.8955\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2569 - acc: 0.9080 - auc: 0.9027\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.2580 - acc: 0.9072 - auc: 0.9081\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.2493 - acc: 0.9109 - auc: 0.9124\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 502us/step - loss: 0.2400 - acc: 0.9154 - auc: 0.9163\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2296 - acc: 0.9197 - auc: 0.9197\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2265 - acc: 0.9191 - auc: 0.9229\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.2169 - acc: 0.9245 - auc: 0.9257\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.2150 - acc: 0.9251 - auc: 0.9282\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2025 - acc: 0.9281 - auc: 0.9306\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 502us/step - loss: 0.2014 - acc: 0.9305 - auc: 0.9329\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1943 - acc: 0.9317 - auc: 0.9352\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.1836 - acc: 0.9364 - auc: 0.9372\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.1823 - acc: 0.9374 - auc: 0.9392\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1762 - acc: 0.9386 - auc: 0.9411\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1661 - acc: 0.9414 - auc: 0.9429\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.1650 - acc: 0.9439 - auc: 0.9446\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.1781 - acc: 0.9395 - auc: 0.9460\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.1568 - acc: 0.9457 - auc: 0.9474\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.1528 - acc: 0.9473 - auc: 0.9489\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1662 - acc: 0.9414 - auc: 0.9501\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1515 - acc: 0.9461 - auc: 0.9513\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.1598 - acc: 0.9455 - auc: 0.9525\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.1442 - acc: 0.9494 - auc: 0.9536\n",
            "3181/3181 [==============================] - 2s 602us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 10s 782us/step - loss: 0.6917 - acc: 0.5573 - auc: 0.4855\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.4777 - acc: 0.7880 - auc: 0.6327\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.3640 - acc: 0.8566 - auc: 0.7623\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.3196 - acc: 0.8779 - auc: 0.8196\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.3018 - acc: 0.8865 - auc: 0.8498\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2889 - acc: 0.8926 - auc: 0.8686\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.2818 - acc: 0.8982 - auc: 0.8809\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.2629 - acc: 0.9034 - auc: 0.8903\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2580 - acc: 0.9072 - auc: 0.8970\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2488 - acc: 0.9117 - auc: 0.9031\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.2465 - acc: 0.9102 - auc: 0.9078\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2451 - acc: 0.9136 - auc: 0.9119\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.2363 - acc: 0.9158 - auc: 0.9152\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.2236 - acc: 0.9231 - auc: 0.9185\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.2209 - acc: 0.9226 - auc: 0.9214\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.2171 - acc: 0.9249 - auc: 0.9240\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2141 - acc: 0.9237 - auc: 0.9264\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.2073 - acc: 0.9260 - auc: 0.9287\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2011 - acc: 0.9297 - auc: 0.9309\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.2000 - acc: 0.9304 - auc: 0.9329\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.1834 - acc: 0.9363 - auc: 0.9350\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.1815 - acc: 0.9374 - auc: 0.9369\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.1724 - acc: 0.9393 - auc: 0.9388\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 482us/step - loss: 0.1733 - acc: 0.9374 - auc: 0.9406\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.1706 - acc: 0.9414 - auc: 0.9423\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1595 - acc: 0.9424 - auc: 0.9439\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1531 - acc: 0.9462 - auc: 0.9455\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1543 - acc: 0.9444 - auc: 0.9470\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1513 - acc: 0.9440 - auc: 0.9485\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1414 - acc: 0.9501 - auc: 0.9500\n",
            "3181/3181 [==============================] - 2s 644us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 10s 799us/step - loss: 0.7140 - acc: 0.5599 - auc: 0.5031\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.4590 - acc: 0.7982 - auc: 0.6391\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.3404 - acc: 0.8662 - auc: 0.7744\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.3199 - acc: 0.8744 - auc: 0.8269\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 7s 520us/step - loss: 0.3108 - acc: 0.8782 - auc: 0.8533\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.2931 - acc: 0.8867 - auc: 0.8700\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2826 - acc: 0.8925 - auc: 0.8816\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.2713 - acc: 0.8979 - auc: 0.8904\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2741 - acc: 0.8989 - auc: 0.8969\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.2677 - acc: 0.8992 - auc: 0.9021\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2548 - acc: 0.9071 - auc: 0.9065\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2476 - acc: 0.9087 - auc: 0.9106\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.2482 - acc: 0.9084 - auc: 0.9138\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.2435 - acc: 0.9109 - auc: 0.9168\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2336 - acc: 0.9139 - auc: 0.9196\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.2323 - acc: 0.9144 - auc: 0.9222\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.2217 - acc: 0.9187 - auc: 0.9248\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.2191 - acc: 0.9205 - auc: 0.9269\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2060 - acc: 0.9260 - auc: 0.9293\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2047 - acc: 0.9262 - auc: 0.9313\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.2062 - acc: 0.9238 - auc: 0.9332\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.1997 - acc: 0.9268 - auc: 0.9349\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.1957 - acc: 0.9285 - auc: 0.9365\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.1947 - acc: 0.9300 - auc: 0.9381\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.1706 - acc: 0.9395 - auc: 0.9398\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1733 - acc: 0.9391 - auc: 0.9415\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.1600 - acc: 0.9431 - auc: 0.9432\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.1615 - acc: 0.9434 - auc: 0.9448\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.1546 - acc: 0.9462 - auc: 0.9464\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.1533 - acc: 0.9461 - auc: 0.9479\n",
            "3181/3181 [==============================] - 2s 653us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12724/12724 [==============================] - 10s 816us/step - loss: 0.5890 - acc: 0.6941 - auc: 0.5954\n",
            "Epoch 2/30\n",
            "12724/12724 [==============================] - 6s 506us/step - loss: 0.3782 - acc: 0.8461 - auc: 0.7918\n",
            "Epoch 3/30\n",
            "12724/12724 [==============================] - 6s 510us/step - loss: 0.3256 - acc: 0.8785 - auc: 0.8475\n",
            "Epoch 4/30\n",
            "12724/12724 [==============================] - 6s 508us/step - loss: 0.3005 - acc: 0.8888 - auc: 0.8738\n",
            "Epoch 5/30\n",
            "12724/12724 [==============================] - 6s 507us/step - loss: 0.2888 - acc: 0.8938 - auc: 0.8876\n",
            "Epoch 6/30\n",
            "12724/12724 [==============================] - 6s 508us/step - loss: 0.2804 - acc: 0.8956 - auc: 0.8973\n",
            "Epoch 7/30\n",
            "12724/12724 [==============================] - 7s 516us/step - loss: 0.2651 - acc: 0.9045 - auc: 0.9046\n",
            "Epoch 8/30\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.2600 - acc: 0.9052 - auc: 0.9101\n",
            "Epoch 9/30\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.2514 - acc: 0.9117 - auc: 0.9142\n",
            "Epoch 10/30\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.2461 - acc: 0.9105 - auc: 0.9180\n",
            "Epoch 11/30\n",
            "12724/12724 [==============================] - 6s 501us/step - loss: 0.2342 - acc: 0.9164 - auc: 0.9216\n",
            "Epoch 12/30\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.2320 - acc: 0.9170 - auc: 0.9245\n",
            "Epoch 13/30\n",
            "12724/12724 [==============================] - 6s 502us/step - loss: 0.2253 - acc: 0.9177 - auc: 0.9274\n",
            "Epoch 14/30\n",
            "12724/12724 [==============================] - 6s 499us/step - loss: 0.2161 - acc: 0.9237 - auc: 0.9300\n",
            "Epoch 15/30\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.2195 - acc: 0.9256 - auc: 0.9322\n",
            "Epoch 16/30\n",
            "12724/12724 [==============================] - 6s 501us/step - loss: 0.2102 - acc: 0.9266 - auc: 0.9342\n",
            "Epoch 17/30\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.2107 - acc: 0.9264 - auc: 0.9360\n",
            "Epoch 18/30\n",
            "12724/12724 [==============================] - 6s 499us/step - loss: 0.1951 - acc: 0.9304 - auc: 0.9379\n",
            "Epoch 19/30\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.1863 - acc: 0.9346 - auc: 0.9398\n",
            "Epoch 20/30\n",
            "12724/12724 [==============================] - 6s 495us/step - loss: 0.1871 - acc: 0.9345 - auc: 0.9416\n",
            "Epoch 21/30\n",
            "12724/12724 [==============================] - 6s 501us/step - loss: 0.1870 - acc: 0.9330 - auc: 0.9433\n",
            "Epoch 22/30\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.1813 - acc: 0.9374 - auc: 0.9449\n",
            "Epoch 23/30\n",
            "12724/12724 [==============================] - 6s 492us/step - loss: 0.1762 - acc: 0.9376 - auc: 0.9463\n",
            "Epoch 24/30\n",
            "12724/12724 [==============================] - 6s 493us/step - loss: 0.1675 - acc: 0.9422 - auc: 0.9478\n",
            "Epoch 25/30\n",
            "12724/12724 [==============================] - 6s 494us/step - loss: 0.1641 - acc: 0.9410 - auc: 0.9492\n",
            "Epoch 26/30\n",
            "12724/12724 [==============================] - 6s 492us/step - loss: 0.1742 - acc: 0.9378 - auc: 0.9504\n",
            "Epoch 27/30\n",
            "12724/12724 [==============================] - 6s 494us/step - loss: 0.1527 - acc: 0.9484 - auc: 0.9516\n",
            "Epoch 28/30\n",
            "12724/12724 [==============================] - 6s 491us/step - loss: 0.1419 - acc: 0.9509 - auc: 0.9530\n",
            "Epoch 29/30\n",
            "12724/12724 [==============================] - 6s 491us/step - loss: 0.1433 - acc: 0.9487 - auc: 0.9542\n",
            "Epoch 30/30\n",
            "12724/12724 [==============================] - 6s 496us/step - loss: 0.1355 - acc: 0.9539 - auc: 0.9555\n",
            "3180/3180 [==============================] - 2s 656us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/50\n",
            "12723/12723 [==============================] - 11s 839us/step - loss: 0.6440 - acc: 0.6329 - auc: 0.5367\n",
            "Epoch 2/50\n",
            "12723/12723 [==============================] - 7s 514us/step - loss: 0.4077 - acc: 0.8287 - auc: 0.7333\n",
            "Epoch 3/50\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.3371 - acc: 0.8668 - auc: 0.8174\n",
            "Epoch 4/50\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.3313 - acc: 0.8681 - auc: 0.8520\n",
            "Epoch 5/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.3066 - acc: 0.8826 - auc: 0.8689\n",
            "Epoch 6/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.2983 - acc: 0.8878 - auc: 0.8810\n",
            "Epoch 7/50\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.3102 - acc: 0.8795 - auc: 0.8880\n",
            "Epoch 8/50\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.2908 - acc: 0.8889 - auc: 0.8931\n",
            "Epoch 9/50\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2811 - acc: 0.8922 - auc: 0.8980\n",
            "Epoch 10/50\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.2722 - acc: 0.8963 - auc: 0.9021\n",
            "Epoch 11/50\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2658 - acc: 0.8996 - auc: 0.9059\n",
            "Epoch 12/50\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.2613 - acc: 0.9005 - auc: 0.9093\n",
            "Epoch 13/50\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.2598 - acc: 0.8992 - auc: 0.9121\n",
            "Epoch 14/50\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.2549 - acc: 0.9048 - auc: 0.9147\n",
            "Epoch 15/50\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2412 - acc: 0.9097 - auc: 0.9172\n",
            "Epoch 16/50\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2441 - acc: 0.9089 - auc: 0.9194\n",
            "Epoch 17/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.2319 - acc: 0.9128 - auc: 0.9218\n",
            "Epoch 18/50\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.2263 - acc: 0.9170 - auc: 0.9241\n",
            "Epoch 19/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.2185 - acc: 0.9197 - auc: 0.9262\n",
            "Epoch 20/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.2064 - acc: 0.9242 - auc: 0.9283\n",
            "Epoch 21/50\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2028 - acc: 0.9242 - auc: 0.9304\n",
            "Epoch 22/50\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2125 - acc: 0.9222 - auc: 0.9323\n",
            "Epoch 23/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.1932 - acc: 0.9278 - auc: 0.9339\n",
            "Epoch 24/50\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.1820 - acc: 0.9347 - auc: 0.9359\n",
            "Epoch 25/50\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.1730 - acc: 0.9363 - auc: 0.9378\n",
            "Epoch 26/50\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.1696 - acc: 0.9359 - auc: 0.9397\n",
            "Epoch 27/50\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.1850 - acc: 0.9323 - auc: 0.9413\n",
            "Epoch 28/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.1672 - acc: 0.9361 - auc: 0.9429\n",
            "Epoch 29/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1481 - acc: 0.9453 - auc: 0.9445\n",
            "Epoch 30/50\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.1444 - acc: 0.9469 - auc: 0.9463\n",
            "Epoch 31/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.1403 - acc: 0.9480 - auc: 0.9479\n",
            "Epoch 32/50\n",
            "12723/12723 [==============================] - 6s 483us/step - loss: 0.1310 - acc: 0.9519 - auc: 0.9495\n",
            "Epoch 33/50\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.1074 - acc: 0.9585 - auc: 0.9512\n",
            "Epoch 34/50\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.1081 - acc: 0.9584 - auc: 0.9529\n",
            "Epoch 35/50\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.1029 - acc: 0.9629 - auc: 0.9546\n",
            "Epoch 36/50\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.0989 - acc: 0.9643 - auc: 0.9561\n",
            "Epoch 37/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.0989 - acc: 0.9638 - auc: 0.9575\n",
            "Epoch 38/50\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.0855 - acc: 0.9675 - auc: 0.9590\n",
            "Epoch 39/50\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.0782 - acc: 0.9705 - auc: 0.9604\n",
            "Epoch 40/50\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.0940 - acc: 0.9664 - auc: 0.9617\n",
            "Epoch 41/50\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.0773 - acc: 0.9720 - auc: 0.9629\n",
            "Epoch 42/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.0834 - acc: 0.9716 - auc: 0.9642\n",
            "Epoch 43/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.0654 - acc: 0.9756 - auc: 0.9653\n",
            "Epoch 44/50\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.0651 - acc: 0.9767 - auc: 0.9664\n",
            "Epoch 45/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.0616 - acc: 0.9781 - auc: 0.9675\n",
            "Epoch 46/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.0698 - acc: 0.9761 - auc: 0.9685\n",
            "Epoch 47/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.0770 - acc: 0.9743 - auc: 0.9695\n",
            "Epoch 48/50\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.0485 - acc: 0.9837 - auc: 0.9703\n",
            "Epoch 49/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.0474 - acc: 0.9841 - auc: 0.9713\n",
            "Epoch 50/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.0885 - acc: 0.9703 - auc: 0.9721\n",
            "3181/3181 [==============================] - 2s 710us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/50\n",
            "12723/12723 [==============================] - 11s 848us/step - loss: 0.7009 - acc: 0.5378 - auc: 0.4957\n",
            "Epoch 2/50\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.6877 - acc: 0.5536 - auc: 0.5042\n",
            "Epoch 3/50\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.6888 - acc: 0.5544 - auc: 0.5043\n",
            "Epoch 4/50\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.6882 - acc: 0.5536 - auc: 0.5029\n",
            "Epoch 5/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6878 - acc: 0.5545 - auc: 0.5019\n",
            "Epoch 6/50\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5010\n",
            "Epoch 7/50\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.6868 - acc: 0.5546 - auc: 0.5015\n",
            "Epoch 8/50\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.6955 - acc: 0.5546 - auc: 0.5017\n",
            "Epoch 9/50\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.6920 - acc: 0.5538 - auc: 0.5017\n",
            "Epoch 10/50\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.6867 - acc: 0.5546 - auc: 0.5016\n",
            "Epoch 11/50\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5012\n",
            "Epoch 12/50\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.5007\n",
            "Epoch 13/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.5004\n",
            "Epoch 14/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5003\n",
            "Epoch 15/50\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5003\n",
            "Epoch 16/50\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5003\n",
            "Epoch 17/50\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5001\n",
            "Epoch 18/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5000\n",
            "Epoch 19/50\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4999\n",
            "Epoch 20/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4999\n",
            "Epoch 21/50\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4998\n",
            "Epoch 22/50\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4998\n",
            "Epoch 23/50\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4997\n",
            "Epoch 24/50\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.6874 - acc: 0.5544 - auc: 0.4996\n",
            "Epoch 25/50\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.6898 - acc: 0.5536 - auc: 0.4996\n",
            "Epoch 26/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.6869 - acc: 0.5546 - auc: 0.4995\n",
            "Epoch 27/50\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.6879 - acc: 0.5546 - auc: 0.4995\n",
            "Epoch 28/50\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4994\n",
            "Epoch 29/50\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.6869 - acc: 0.5546 - auc: 0.4994\n",
            "Epoch 30/50\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.6891 - acc: 0.5547 - auc: 0.4996\n",
            "Epoch 31/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6868 - acc: 0.5570 - auc: 0.4998\n",
            "Epoch 32/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6873 - acc: 0.5545 - auc: 0.4999\n",
            "Epoch 33/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.6915 - acc: 0.5546 - auc: 0.4999\n",
            "Epoch 34/50\n",
            "12723/12723 [==============================] - 6s 502us/step - loss: 0.6871 - acc: 0.5546 - auc: 0.4998\n",
            "Epoch 35/50\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.6872 - acc: 0.5545 - auc: 0.4997\n",
            "Epoch 36/50\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.4996\n",
            "Epoch 37/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6869 - acc: 0.5546 - auc: 0.4996\n",
            "Epoch 38/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6871 - acc: 0.5546 - auc: 0.4996\n",
            "Epoch 39/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4994\n",
            "Epoch 40/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4994\n",
            "Epoch 41/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4994\n",
            "Epoch 42/50\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4995\n",
            "Epoch 43/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6874 - acc: 0.5545 - auc: 0.4993\n",
            "Epoch 44/50\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4993\n",
            "Epoch 45/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4992\n",
            "Epoch 46/50\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4992\n",
            "Epoch 47/50\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4992\n",
            "Epoch 48/50\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4991\n",
            "Epoch 49/50\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4991\n",
            "Epoch 50/50\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4991\n",
            "3181/3181 [==============================] - 2s 716us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/50\n",
            "12723/12723 [==============================] - 11s 872us/step - loss: 0.7675 - acc: 0.5536 - auc: 0.5049\n",
            "Epoch 2/50\n",
            "12723/12723 [==============================] - 7s 532us/step - loss: 0.5850 - acc: 0.6923 - auc: 0.5778\n",
            "Epoch 3/50\n",
            "12723/12723 [==============================] - 7s 525us/step - loss: 0.4516 - acc: 0.7995 - auc: 0.6920\n",
            "Epoch 4/50\n",
            "12723/12723 [==============================] - 7s 516us/step - loss: 0.3733 - acc: 0.8497 - auc: 0.7582\n",
            "Epoch 5/50\n",
            "12723/12723 [==============================] - 7s 512us/step - loss: 0.3163 - acc: 0.8782 - auc: 0.8028\n",
            "Epoch 6/50\n",
            "12723/12723 [==============================] - 7s 517us/step - loss: 0.2922 - acc: 0.8905 - auc: 0.8337\n",
            "Epoch 7/50\n",
            "12723/12723 [==============================] - 7s 512us/step - loss: 0.2828 - acc: 0.8951 - auc: 0.8543\n",
            "Epoch 8/50\n",
            "12723/12723 [==============================] - 7s 512us/step - loss: 0.2812 - acc: 0.8945 - auc: 0.8683\n",
            "Epoch 9/50\n",
            "12723/12723 [==============================] - 6s 511us/step - loss: 0.2650 - acc: 0.9035 - auc: 0.8786\n",
            "Epoch 10/50\n",
            "12723/12723 [==============================] - 7s 514us/step - loss: 0.2665 - acc: 0.9018 - auc: 0.8867\n",
            "Epoch 11/50\n",
            "12723/12723 [==============================] - 6s 511us/step - loss: 0.2474 - acc: 0.9123 - auc: 0.8936\n",
            "Epoch 12/50\n",
            "12723/12723 [==============================] - 7s 517us/step - loss: 0.2412 - acc: 0.9139 - auc: 0.8995\n",
            "Epoch 13/50\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.2361 - acc: 0.9145 - auc: 0.9045\n",
            "Epoch 14/50\n",
            "12723/12723 [==============================] - 6s 511us/step - loss: 0.2316 - acc: 0.9159 - auc: 0.9087\n",
            "Epoch 15/50\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.2247 - acc: 0.9194 - auc: 0.9127\n",
            "Epoch 16/50\n",
            "12723/12723 [==============================] - 7s 515us/step - loss: 0.2223 - acc: 0.9229 - auc: 0.9164\n",
            "Epoch 17/50\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.2127 - acc: 0.9229 - auc: 0.9194\n",
            "Epoch 18/50\n",
            "12723/12723 [==============================] - 7s 518us/step - loss: 0.2167 - acc: 0.9227 - auc: 0.9223\n",
            "Epoch 19/50\n",
            "12723/12723 [==============================] - 7s 514us/step - loss: 0.2021 - acc: 0.9297 - auc: 0.9248\n",
            "Epoch 20/50\n",
            "12723/12723 [==============================] - 6s 505us/step - loss: 0.2007 - acc: 0.9281 - auc: 0.9271\n",
            "Epoch 21/50\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.1923 - acc: 0.9332 - auc: 0.9295\n",
            "Epoch 22/50\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.1856 - acc: 0.9350 - auc: 0.9317\n",
            "Epoch 23/50\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.1790 - acc: 0.9369 - auc: 0.9338\n",
            "Epoch 24/50\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.1699 - acc: 0.9396 - auc: 0.9359\n",
            "Epoch 25/50\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.1722 - acc: 0.9415 - auc: 0.9379\n",
            "Epoch 26/50\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.1741 - acc: 0.9395 - auc: 0.9396\n",
            "Epoch 27/50\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.1661 - acc: 0.9426 - auc: 0.9412\n",
            "Epoch 28/50\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.1588 - acc: 0.9457 - auc: 0.9428\n",
            "Epoch 29/50\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.1426 - acc: 0.9497 - auc: 0.9445\n",
            "Epoch 30/50\n",
            "12723/12723 [==============================] - 7s 520us/step - loss: 0.1384 - acc: 0.9547 - auc: 0.9461\n",
            "Epoch 31/50\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.1372 - acc: 0.9532 - auc: 0.9477\n",
            "Epoch 32/50\n",
            "12723/12723 [==============================] - 6s 511us/step - loss: 0.1408 - acc: 0.9499 - auc: 0.9493\n",
            "Epoch 33/50\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.1340 - acc: 0.9520 - auc: 0.9507\n",
            "Epoch 34/50\n",
            "12723/12723 [==============================] - 6s 502us/step - loss: 0.1297 - acc: 0.9528 - auc: 0.9520\n",
            "Epoch 35/50\n",
            "12723/12723 [==============================] - 7s 513us/step - loss: 0.1226 - acc: 0.9567 - auc: 0.9533\n",
            "Epoch 36/50\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.1140 - acc: 0.9593 - auc: 0.9546\n",
            "Epoch 37/50\n",
            "12723/12723 [==============================] - 6s 505us/step - loss: 0.1097 - acc: 0.9624 - auc: 0.9560\n",
            "Epoch 38/50\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.0935 - acc: 0.9689 - auc: 0.9573\n",
            "Epoch 39/50\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.0937 - acc: 0.9672 - auc: 0.9586\n",
            "Epoch 40/50\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.1078 - acc: 0.9624 - auc: 0.9598\n",
            "Epoch 41/50\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.0973 - acc: 0.9659 - auc: 0.9609\n",
            "Epoch 42/50\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.0792 - acc: 0.9714 - auc: 0.9620\n",
            "Epoch 43/50\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.0907 - acc: 0.9682 - auc: 0.9632\n",
            "Epoch 44/50\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.0845 - acc: 0.9720 - auc: 0.9642\n",
            "Epoch 45/50\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.0825 - acc: 0.9737 - auc: 0.9652\n",
            "Epoch 46/50\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.0729 - acc: 0.9752 - auc: 0.9662\n",
            "Epoch 47/50\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.0615 - acc: 0.9786 - auc: 0.9672\n",
            "Epoch 48/50\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.1529 - acc: 0.9482 - auc: 0.9680\n",
            "Epoch 49/50\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.0931 - acc: 0.9679 - auc: 0.9683\n",
            "Epoch 50/50\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.0887 - acc: 0.9695 - auc: 0.9691\n",
            "3181/3181 [==============================] - 2s 743us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/50\n",
            "12723/12723 [==============================] - 11s 881us/step - loss: 0.7337 - acc: 0.5508 - auc: 0.4983\n",
            "Epoch 2/50\n",
            "12723/12723 [==============================] - 7s 525us/step - loss: 0.6871 - acc: 0.5544 - auc: 0.5082\n",
            "Epoch 3/50\n",
            "12723/12723 [==============================] - 7s 523us/step - loss: 0.6897 - acc: 0.5540 - auc: 0.5061\n",
            "Epoch 4/50\n",
            "12723/12723 [==============================] - 7s 521us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5056\n",
            "Epoch 5/50\n",
            "12723/12723 [==============================] - 7s 523us/step - loss: 0.6872 - acc: 0.5547 - auc: 0.5046\n",
            "Epoch 6/50\n",
            "12723/12723 [==============================] - 7s 514us/step - loss: 0.6874 - acc: 0.5547 - auc: 0.5043\n",
            "Epoch 7/50\n",
            "12723/12723 [==============================] - 7s 519us/step - loss: 0.6873 - acc: 0.5547 - auc: 0.5031\n",
            "Epoch 8/50\n",
            "12723/12723 [==============================] - 7s 520us/step - loss: 0.6875 - acc: 0.5547 - auc: 0.5023\n",
            "Epoch 9/50\n",
            "12723/12723 [==============================] - 7s 517us/step - loss: 0.6873 - acc: 0.5547 - auc: 0.5015\n",
            "Epoch 10/50\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.6873 - acc: 0.5547 - auc: 0.5010\n",
            "Epoch 11/50\n",
            "12723/12723 [==============================] - 7s 521us/step - loss: 0.6872 - acc: 0.5547 - auc: 0.5008\n",
            "Epoch 12/50\n",
            "12723/12723 [==============================] - 7s 533us/step - loss: 0.6871 - acc: 0.5547 - auc: 0.5008\n",
            "Epoch 13/50\n",
            "12723/12723 [==============================] - 7s 521us/step - loss: 0.6877 - acc: 0.5539 - auc: 0.5006\n",
            "Epoch 14/50\n",
            "12723/12723 [==============================] - 7s 532us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.5002\n",
            "Epoch 15/50\n",
            "12723/12723 [==============================] - 7s 537us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5003\n",
            "Epoch 16/50\n",
            "12723/12723 [==============================] - 7s 545us/step - loss: 0.6875 - acc: 0.5546 - auc: 0.5001\n",
            "Epoch 17/50\n",
            "12723/12723 [==============================] - 7s 526us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4997\n",
            "Epoch 18/50\n",
            "12723/12723 [==============================] - 7s 527us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.4995\n",
            "Epoch 19/50\n",
            "12723/12723 [==============================] - 7s 531us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4994\n",
            "Epoch 20/50\n",
            "12723/12723 [==============================] - 7s 536us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4993\n",
            "Epoch 21/50\n",
            "12723/12723 [==============================] - 7s 535us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4991\n",
            "Epoch 22/50\n",
            "12723/12723 [==============================] - 7s 544us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4989\n",
            "Epoch 23/50\n",
            "12723/12723 [==============================] - 7s 536us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4989\n",
            "Epoch 24/50\n",
            "12723/12723 [==============================] - 7s 538us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4987\n",
            "Epoch 25/50\n",
            "12723/12723 [==============================] - 7s 534us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4985\n",
            "Epoch 26/50\n",
            "12723/12723 [==============================] - 7s 525us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4983\n",
            "Epoch 27/50\n",
            "12723/12723 [==============================] - 7s 526us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4983\n",
            "Epoch 28/50\n",
            "12723/12723 [==============================] - 7s 525us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 29/50\n",
            "12723/12723 [==============================] - 7s 539us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 30/50\n",
            "12723/12723 [==============================] - 7s 534us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 31/50\n",
            "12723/12723 [==============================] - 7s 537us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 32/50\n",
            "12723/12723 [==============================] - 7s 541us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 33/50\n",
            "12723/12723 [==============================] - 7s 538us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 34/50\n",
            "12723/12723 [==============================] - 7s 536us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4981\n",
            "Epoch 35/50\n",
            "12723/12723 [==============================] - 7s 531us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4980\n",
            "Epoch 36/50\n",
            "12723/12723 [==============================] - 7s 529us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4980\n",
            "Epoch 37/50\n",
            "12723/12723 [==============================] - 7s 543us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4979\n",
            "Epoch 38/50\n",
            "12723/12723 [==============================] - 7s 535us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4979\n",
            "Epoch 39/50\n",
            "12723/12723 [==============================] - 7s 532us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4979\n",
            "Epoch 40/50\n",
            "12723/12723 [==============================] - 7s 546us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4978\n",
            "Epoch 41/50\n",
            "12723/12723 [==============================] - 7s 528us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4978\n",
            "Epoch 42/50\n",
            "12723/12723 [==============================] - 7s 530us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4978\n",
            "Epoch 43/50\n",
            "12723/12723 [==============================] - 7s 527us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4978\n",
            "Epoch 44/50\n",
            "12723/12723 [==============================] - 7s 526us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4977\n",
            "Epoch 45/50\n",
            "12723/12723 [==============================] - 7s 526us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4977\n",
            "Epoch 46/50\n",
            "12723/12723 [==============================] - 7s 531us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4977\n",
            "Epoch 47/50\n",
            "12723/12723 [==============================] - 7s 532us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4977\n",
            "Epoch 48/50\n",
            "12723/12723 [==============================] - 7s 518us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4977\n",
            "Epoch 49/50\n",
            "12723/12723 [==============================] - 7s 535us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4977\n",
            "Epoch 50/50\n",
            "12723/12723 [==============================] - 7s 536us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4977\n",
            "3181/3181 [==============================] - 3s 845us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/50\n",
            "12724/12724 [==============================] - 12s 951us/step - loss: 0.7002 - acc: 0.5469 - auc: 0.5058\n",
            "Epoch 2/50\n",
            "12724/12724 [==============================] - 7s 558us/step - loss: 0.6545 - acc: 0.6106 - auc: 0.5251\n",
            "Epoch 3/50\n",
            "12724/12724 [==============================] - 7s 557us/step - loss: 0.4428 - acc: 0.8185 - auc: 0.6439\n",
            "Epoch 4/50\n",
            "12724/12724 [==============================] - 7s 552us/step - loss: 0.3660 - acc: 0.8561 - auc: 0.7378\n",
            "Epoch 5/50\n",
            "12724/12724 [==============================] - 7s 559us/step - loss: 0.3098 - acc: 0.8850 - auc: 0.7952\n",
            "Epoch 6/50\n",
            "12724/12724 [==============================] - 7s 569us/step - loss: 0.2939 - acc: 0.8915 - auc: 0.8286\n",
            "Epoch 7/50\n",
            "12724/12724 [==============================] - 7s 583us/step - loss: 0.2719 - acc: 0.9011 - auc: 0.8510\n",
            "Epoch 8/50\n",
            "12724/12724 [==============================] - 7s 583us/step - loss: 0.2660 - acc: 0.9051 - auc: 0.8672\n",
            "Epoch 9/50\n",
            "12724/12724 [==============================] - 7s 567us/step - loss: 0.2510 - acc: 0.9105 - auc: 0.8792\n",
            "Epoch 10/50\n",
            "12724/12724 [==============================] - 7s 558us/step - loss: 0.2411 - acc: 0.9140 - auc: 0.8892\n",
            "Epoch 11/50\n",
            "12724/12724 [==============================] - 7s 544us/step - loss: 0.2477 - acc: 0.9091 - auc: 0.8966\n",
            "Epoch 12/50\n",
            "12724/12724 [==============================] - 7s 550us/step - loss: 0.2337 - acc: 0.9175 - auc: 0.9028\n",
            "Epoch 13/50\n",
            "12724/12724 [==============================] - 7s 552us/step - loss: 0.2264 - acc: 0.9196 - auc: 0.9082\n",
            "Epoch 14/50\n",
            "12724/12724 [==============================] - 7s 551us/step - loss: 0.2187 - acc: 0.9237 - auc: 0.9127\n",
            "Epoch 15/50\n",
            "12724/12724 [==============================] - 7s 547us/step - loss: 0.2106 - acc: 0.9248 - auc: 0.9170\n",
            "Epoch 16/50\n",
            "12724/12724 [==============================] - 7s 546us/step - loss: 0.2035 - acc: 0.9297 - auc: 0.9206\n",
            "Epoch 17/50\n",
            "12724/12724 [==============================] - 7s 554us/step - loss: 0.2117 - acc: 0.9260 - auc: 0.9238\n",
            "Epoch 18/50\n",
            "12724/12724 [==============================] - 7s 545us/step - loss: 0.2006 - acc: 0.9312 - auc: 0.9265\n",
            "Epoch 19/50\n",
            "12724/12724 [==============================] - 7s 538us/step - loss: 0.1902 - acc: 0.9339 - auc: 0.9292\n",
            "Epoch 20/50\n",
            "12724/12724 [==============================] - 7s 545us/step - loss: 0.1843 - acc: 0.9345 - auc: 0.9318\n",
            "Epoch 21/50\n",
            "12724/12724 [==============================] - 7s 539us/step - loss: 0.1812 - acc: 0.9350 - auc: 0.9341\n",
            "Epoch 22/50\n",
            "12724/12724 [==============================] - 7s 534us/step - loss: 0.1790 - acc: 0.9374 - auc: 0.9364\n",
            "Epoch 23/50\n",
            "12724/12724 [==============================] - 7s 532us/step - loss: 0.1740 - acc: 0.9404 - auc: 0.9383\n",
            "Epoch 24/50\n",
            "12724/12724 [==============================] - 7s 540us/step - loss: 0.1707 - acc: 0.9394 - auc: 0.9402\n",
            "Epoch 25/50\n",
            "12724/12724 [==============================] - 7s 533us/step - loss: 0.1572 - acc: 0.9474 - auc: 0.9419\n",
            "Epoch 26/50\n",
            "12724/12724 [==============================] - 7s 535us/step - loss: 0.1583 - acc: 0.9455 - auc: 0.9438\n",
            "Epoch 27/50\n",
            "12724/12724 [==============================] - 7s 532us/step - loss: 0.1494 - acc: 0.9480 - auc: 0.9455\n",
            "Epoch 28/50\n",
            "12724/12724 [==============================] - 7s 528us/step - loss: 0.1507 - acc: 0.9499 - auc: 0.9471\n",
            "Epoch 29/50\n",
            "12724/12724 [==============================] - 7s 535us/step - loss: 0.1482 - acc: 0.9488 - auc: 0.9486\n",
            "Epoch 30/50\n",
            "12724/12724 [==============================] - 7s 529us/step - loss: 0.1436 - acc: 0.9506 - auc: 0.9500\n",
            "Epoch 31/50\n",
            "12724/12724 [==============================] - 7s 532us/step - loss: 0.1305 - acc: 0.9543 - auc: 0.9514\n",
            "Epoch 32/50\n",
            "12724/12724 [==============================] - 7s 539us/step - loss: 0.1311 - acc: 0.9533 - auc: 0.9528\n",
            "Epoch 33/50\n",
            "12724/12724 [==============================] - 7s 539us/step - loss: 0.1117 - acc: 0.9624 - auc: 0.9542\n",
            "Epoch 34/50\n",
            "12724/12724 [==============================] - 7s 529us/step - loss: 0.1175 - acc: 0.9586 - auc: 0.9557\n",
            "Epoch 35/50\n",
            "12724/12724 [==============================] - 7s 537us/step - loss: 0.1212 - acc: 0.9595 - auc: 0.9570\n",
            "Epoch 36/50\n",
            "12724/12724 [==============================] - 7s 528us/step - loss: 0.1171 - acc: 0.9601 - auc: 0.9581\n",
            "Epoch 37/50\n",
            "12724/12724 [==============================] - 7s 526us/step - loss: 0.1001 - acc: 0.9664 - auc: 0.9594\n",
            "Epoch 38/50\n",
            "12724/12724 [==============================] - 7s 517us/step - loss: 0.0925 - acc: 0.9690 - auc: 0.9606\n",
            "Epoch 39/50\n",
            "12724/12724 [==============================] - 7s 520us/step - loss: 0.0984 - acc: 0.9659 - auc: 0.9617\n",
            "Epoch 40/50\n",
            "12724/12724 [==============================] - 7s 518us/step - loss: 0.0795 - acc: 0.9704 - auc: 0.9629\n",
            "Epoch 41/50\n",
            "12724/12724 [==============================] - 7s 515us/step - loss: 0.0882 - acc: 0.9693 - auc: 0.9641\n",
            "Epoch 42/50\n",
            "12724/12724 [==============================] - 7s 520us/step - loss: 0.0715 - acc: 0.9747 - auc: 0.9652\n",
            "Epoch 43/50\n",
            "12724/12724 [==============================] - 7s 516us/step - loss: 0.0958 - acc: 0.9684 - auc: 0.9662\n",
            "Epoch 44/50\n",
            "12724/12724 [==============================] - 7s 521us/step - loss: 0.0720 - acc: 0.9750 - auc: 0.9671\n",
            "Epoch 45/50\n",
            "12724/12724 [==============================] - 7s 525us/step - loss: 0.0724 - acc: 0.9762 - auc: 0.9681\n",
            "Epoch 46/50\n",
            "12724/12724 [==============================] - 7s 522us/step - loss: 0.0716 - acc: 0.9740 - auc: 0.9690\n",
            "Epoch 47/50\n",
            "12724/12724 [==============================] - 7s 523us/step - loss: 0.0709 - acc: 0.9739 - auc: 0.9699\n",
            "Epoch 48/50\n",
            "12724/12724 [==============================] - 7s 521us/step - loss: 0.0617 - acc: 0.9794 - auc: 0.9708\n",
            "Epoch 49/50\n",
            "12724/12724 [==============================] - 7s 532us/step - loss: 0.0547 - acc: 0.9818 - auc: 0.9716\n",
            "Epoch 50/50\n",
            "12724/12724 [==============================] - 7s 527us/step - loss: 0.0553 - acc: 0.9822 - auc: 0.9725\n",
            "3180/3180 [==============================] - 3s 847us/step\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "15904/15904 [==============================] - 14s 903us/step - loss: 0.6834 - acc: 0.5880 - auc: 0.5209\n",
            "Epoch 2/30\n",
            "15904/15904 [==============================] - 9s 576us/step - loss: 0.4191 - acc: 0.8275 - auc: 0.6897\n",
            "Epoch 3/30\n",
            "15904/15904 [==============================] - 9s 567us/step - loss: 0.3412 - acc: 0.8669 - auc: 0.8003\n",
            "Epoch 4/30\n",
            "15904/15904 [==============================] - 9s 564us/step - loss: 0.3125 - acc: 0.8814 - auc: 0.8435\n",
            "Epoch 5/30\n",
            "15904/15904 [==============================] - 9s 563us/step - loss: 0.2970 - acc: 0.8886 - auc: 0.8669\n",
            "Epoch 6/30\n",
            "15904/15904 [==============================] - 9s 550us/step - loss: 0.2751 - acc: 0.8947 - auc: 0.8818\n",
            "Epoch 7/30\n",
            "15904/15904 [==============================] - 9s 549us/step - loss: 0.2671 - acc: 0.9005 - auc: 0.8927\n",
            "Epoch 8/30\n",
            "15904/15904 [==============================] - 9s 554us/step - loss: 0.2644 - acc: 0.9020 - auc: 0.9008\n",
            "Epoch 9/30\n",
            "15904/15904 [==============================] - 9s 546us/step - loss: 0.2517 - acc: 0.9091 - auc: 0.9069\n",
            "Epoch 10/30\n",
            "15904/15904 [==============================] - 9s 543us/step - loss: 0.2422 - acc: 0.9123 - auc: 0.9119\n",
            "Epoch 11/30\n",
            "15904/15904 [==============================] - 8s 530us/step - loss: 0.2371 - acc: 0.9130 - auc: 0.9164\n",
            "Epoch 12/30\n",
            "15904/15904 [==============================] - 8s 530us/step - loss: 0.2344 - acc: 0.9171 - auc: 0.9201\n",
            "Epoch 13/30\n",
            "15904/15904 [==============================] - 8s 530us/step - loss: 0.2224 - acc: 0.9191 - auc: 0.9235\n",
            "Epoch 14/30\n",
            "15904/15904 [==============================] - 8s 534us/step - loss: 0.2177 - acc: 0.9209 - auc: 0.9264\n",
            "Epoch 15/30\n",
            "15904/15904 [==============================] - 9s 535us/step - loss: 0.2152 - acc: 0.9227 - auc: 0.9292\n",
            "Epoch 16/30\n",
            "15904/15904 [==============================] - 8s 531us/step - loss: 0.2127 - acc: 0.9244 - auc: 0.9315\n",
            "Epoch 17/30\n",
            "15904/15904 [==============================] - 8s 523us/step - loss: 0.1984 - acc: 0.9298 - auc: 0.9338\n",
            "Epoch 18/30\n",
            "15904/15904 [==============================] - 8s 531us/step - loss: 0.1984 - acc: 0.9300 - auc: 0.9358\n",
            "Epoch 19/30\n",
            "15904/15904 [==============================] - 8s 522us/step - loss: 0.1983 - acc: 0.9291 - auc: 0.9378\n",
            "Epoch 20/30\n",
            "15904/15904 [==============================] - 9s 538us/step - loss: 0.1933 - acc: 0.9314 - auc: 0.9395\n",
            "Epoch 21/30\n",
            "15904/15904 [==============================] - 8s 530us/step - loss: 0.1813 - acc: 0.9356 - auc: 0.9413\n",
            "Epoch 22/30\n",
            "15904/15904 [==============================] - 8s 528us/step - loss: 0.1785 - acc: 0.9381 - auc: 0.9431\n",
            "Epoch 23/30\n",
            "15904/15904 [==============================] - 9s 538us/step - loss: 0.1756 - acc: 0.9376 - auc: 0.9446\n",
            "Epoch 24/30\n",
            "15904/15904 [==============================] - 8s 526us/step - loss: 0.1712 - acc: 0.9386 - auc: 0.9461\n",
            "Epoch 25/30\n",
            "15904/15904 [==============================] - 8s 527us/step - loss: 0.1676 - acc: 0.9406 - auc: 0.9476\n",
            "Epoch 26/30\n",
            "15904/15904 [==============================] - 8s 532us/step - loss: 0.1570 - acc: 0.9453 - auc: 0.9490\n",
            "Epoch 27/30\n",
            "15904/15904 [==============================] - 8s 531us/step - loss: 0.1500 - acc: 0.9462 - auc: 0.9505\n",
            "Epoch 28/30\n",
            "15904/15904 [==============================] - 8s 529us/step - loss: 0.1491 - acc: 0.9462 - auc: 0.9519\n",
            "Epoch 29/30\n",
            "15904/15904 [==============================] - 8s 532us/step - loss: 0.1534 - acc: 0.9471 - auc: 0.9531\n",
            "Epoch 30/30\n",
            "15904/15904 [==============================] - 8s 528us/step - loss: 0.1379 - acc: 0.9511 - auc: 0.9544\n",
            "The parameters of the best model are: \n",
            "{'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 30, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.765971 (0.172621) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.898076 (0.005580) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 30, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.755596 (0.164217) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 50, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.752892 (0.162003) with: {'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.900277 (0.004775) with: {'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 30, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.754842 (0.164148) with: {'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 50, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGXggAs2UY2m",
        "colab_type": "code",
        "outputId": "dc80f1f0-98d9-48c6-bcb4-0cdb6010524c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "print('The parameters of the best model are: ')\n",
        "print(validator66.best_params_)\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters of the best model are: \n",
            "{'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.869907 (0.014068) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.671089 (0.122335) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.752012 (0.161275) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.710827 (0.093366) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.758426 (0.166309) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.674233 (0.127986) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.904049 (0.006086) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.687814 (0.123917) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.871227 (0.006594) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.671152 (0.116912) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.888141 (0.004962) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.745410 (0.021864) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.820485 (0.132884) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.635123 (0.157508) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.695108 (0.172173) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.725541 (0.060771) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXE3mC5lnkaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  grid_result.cv_results_['split1_test_score']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZoiIznKfCrX",
        "colab_type": "code",
        "outputId": "30b715f7-8500-4386-f15e-235c228ffabb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print('The parameters of the best model are: ')\n",
        "print(validator66.best_params_)\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters of the best model are: \n",
            "{'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 1, 'filters': 10, 'kernel_size': 3, 'pool_size': 2}\n",
            "-0.696088 (0.005621) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 1, 'filters': 10, 'kernel_size': 3, 'pool_size': 2}\n",
            "-4.680589 (5.632574) with: {'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 1, 'filters': 10, 'kernel_size': 3, 'pool_size': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsMOoTCLm7im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "#del model\n",
        "#dense_size_candidates = [[32], [64], [32, 32], [64, 64]]\n",
        "#model = make_model(5, 20, 3, 2)\n",
        "#model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "my_classifier = KerasClassifier(search_model)\n",
        "validator = GridSearchCV(my_classifier,\n",
        "                         param_grid={'dense_layer_sizes': [128, 256],\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [10, 30, 50],\n",
        "                                     'filters': [10, 20],\n",
        "                                     'kernel_size': [(3,3)],\n",
        "                                     'pool_size': [(2,2)],\n",
        "                                     'hidden_layers': [1,2,3],\n",
        "                                     'loss_function': ['binary_crossentropy', custom_loss2],\n",
        "                                     'batch_size': [[32], [64]]}, \n",
        "                         scoring='neg_log_loss')\n",
        "\n",
        "my_classifier2 = KerasClassifier(make_model_modified)\n",
        "validator2 = GridSearchCV(my_classifier2,\n",
        "                         param_grid={'dense_layer_sizes': [128,256],\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [1,2],\n",
        "                                     'filters': [20],\n",
        "                                     'kernel_size': [(3,3)],\n",
        "                                     'pool_size': [(2,2)],\n",
        "                                     'batch_size': [32]}, \n",
        "                         scoring='neg_log_loss', n_jobs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unF8fvbDwTF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_frame = pd.DataFrame(y_train)\n",
        "y=y_train.astype('int')\n",
        "grid_result = validator.fit(X_train, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW98cIwoRGpM",
        "colab_type": "code",
        "outputId": "4202e3fe-7754-4cb6-93d3-2d006285872d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "'''Example of how to use sklearn wrapper\n",
        "Builds simple CNN models on MNIST and uses sklearn's GridSearchCV to find best model\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# load training data and do basic data normalization\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(numpy.shape(x_train))\n",
        "print(numpy.shape(y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9TJIdqjTikR",
        "colab_type": "code",
        "outputId": "44625f03-27b2-411d-b8e5-8e3347e7827d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "a = y_train>1\n",
        "y_train_binary = a*1\n",
        "print(y_train_binary)\n",
        "print(numpy.shape(x_train))\n",
        "print(numpy.shape(y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 0 0]\n",
            "(60000, 28, 28, 1)\n",
            "(15904,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPxN3JQb51Bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# convert class vectors to binary class matrices\n",
        "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def make_model(dense_layer_sizes, filters, kernel_size, pool_size):\n",
        "    '''Creates model comprised of 2 convolutional layers followed by dense layers\n",
        "    dense_layer_sizes: List of layer sizes.\n",
        "        This list has one number for each layer\n",
        "    filters: Number of convolutional filters in each convolutional layer\n",
        "    kernel_size: Convolutional kernel size\n",
        "    pool_size: Size of pooling area for max pooling\n",
        "    '''\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,\n",
        "                     padding='valid',\n",
        "                     input_shape=input_shape, data_format='channels_first'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters, kernel_size))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    for layer_size in dense_layer_sizes:\n",
        "        model.add(Dense(layer_size))\n",
        "        model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))#(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', #'categorical_crossentropy',\n",
        "                  optimizer='adadelta',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "dense_size_candidates = [[32], [64]] #, [32, 32], [64, 64]]\n",
        "my_classifier = KerasClassifier(make_model, batch_size=32)\n",
        "validator = GridSearchCV(my_classifier,\n",
        "                         param_grid={'dense_layer_sizes': dense_size_candidates,\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [1],\n",
        "                                     'filters': [8],\n",
        "                                     'kernel_size': [3],\n",
        "                                     'pool_size': [2]},\n",
        "                         scoring='neg_log_loss',\n",
        "                         n_jobs=1)\n",
        "y=y_train.astype('int')\n",
        "validator.fit(X_train, y) #_binary)\n",
        "\n",
        "print('The parameters of the best model are: ')\n",
        "print(validator.best_params_)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kswr3l8Z17v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validator.best_estimator_ returns sklearn-wrapped version of best model.\n",
        "# validator.best_estimator_.model returns the (unwrapped) keras model\n",
        "best_model = validator.best_estimator_.model\n",
        "metric_names = best_model.metrics_names\n",
        "metric_values = best_model.evaluate(x_test, y_test)\n",
        "for metric, value in zip(metric_names, metric_values):\n",
        "    print(metric, ': ', value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPTbon76RXfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}