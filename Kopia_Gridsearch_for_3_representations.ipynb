{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopia Gridsearch for 3 representations. ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannape/IBAC-Biodiv/blob/master/Kopia_Gridsearch_for_3_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mq81jmxhMYV",
        "colab_type": "text"
      },
      "source": [
        "Gridsearch dla każdej reprezentacji osobno. Bo na IBACu wszystkie 3 reprezentacje były puszczone na architekturę z mel-spectrogramu (chyba?)\n",
        "\n",
        "Na podstawie hania.dldisc: cnn_gridsearch.ipynb , 03 CNN fit and predict (I2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65s8txmrhL7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CddWh85JjLPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "import numpy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import numpy\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "#K.set_image_dim_ordering('th')\n",
        "K.tensorflow_backend.set_image_dim_ordering('th')\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from keras.datasets import mnist\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from functools import partial, update_wrapper\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAjy6F71jTVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(667)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(667)\n",
        "import random\n",
        "random.seed()\n",
        "\n",
        "'''\n",
        "#################### Rep 1 - spektro ####################\n",
        "# rep 1  ------- 63 x 148 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep1/X_train_rep1.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep1/y_train.npy', allow_pickle=True)\n",
        "'''\n",
        "# rep 1V2 ------- 63 x 148 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep1V2/X_train_rep1.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep1V2/y_train_rep1.npy', allow_pickle=True)\n",
        "'''\n",
        "# rep 1b ------- 63 x 63 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep1b/X_train_rep1b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep1b/y_train_rep1b.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 3 - mel-spektro ####################\n",
        "\n",
        "# rep 3 ------- 60 x 111 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3/X_train_rep3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3/y_train.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V2 ------- 60 x 111 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V2/X_train_rep3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V2/y_train_rep3.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3b ------- 60 x 63 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3b/X_train_rep3b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3b/y_train_rep3b.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V3 ------- 60 x 148 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V3/X_train_rep3V3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V3/y_train_rep3V3.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 5 - mel-spektro ####################\n",
        "\n",
        "# rep 5 ------- 64 x 61 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep5/X_train_rep5.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep5/y_train.npy', allow_pickle=True)\n",
        "\n",
        "# rep 5V2 ------- 64 x 61 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep5V2/X_train_rep5.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep5V2/y_train_rep5.npy', allow_pickle=True)\n",
        "\n",
        "# rep 5b ------- 64 x 149 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep5b/X_train_rep5b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep5b/y_train_rep5b.npy', allow_pickle=True)\n",
        "'''\n",
        "\n",
        "print('Training set size:')\n",
        "print(np.shape(X_train1))\n",
        "print(np.shape(y_train1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST0WDnQSxeLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.shape(X_train1)[1]\n",
        "s = np.shape(X_train1)[2]\n",
        "y_train = y_train1\n",
        "X_train = X_train1.reshape(X_train1.shape[0], 1, r, s).astype('float32')\n",
        "print('Training set size:')\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(y_train))\n",
        "input_shape = (1, r, s)\n",
        "print(input_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j27RDs6YmWqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/keras-team/keras/issues/2115\n",
        "\n",
        "### definiowanie wag \n",
        "for_zeros = 0.1\n",
        "for_ones = 0.9\n",
        "###\n",
        "\n",
        "### SCORERS\n",
        "\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "from functools import partial, update_wrapper\n",
        "\n",
        "def my_score(y_true, y_pred, sample_weight): \n",
        "  return log_loss(y_true.values, y_pred, sample_weight=sample_weight.loc[y_true.index.values].values.reshape(-1), normalize=True)\n",
        "\n",
        "my_scorer = make_scorer(my_score,greater_is_better=False, needs_threshold=False,**score_params)  ## scoring for gridsearchCV\n",
        "\n",
        "#def binary_crossentropy_weigted(y_true, y_pred, class_weights):\n",
        "#\ty_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "#  \n",
        "#\tloss = K.mean(class_weights*(-y_true * K.log(y_pred) - (1.0 - y_true) * K.log(1.0 - y_pred)),axis=-1)\n",
        "#\treturn loss\n",
        "\n",
        "def weighted_binary_crossentropy( y_true, y_pred, weights_10) :\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "    logloss = -(y_true * K.log(y_pred) * weights_10[0] + (1 - y_true) * K.log(1 - y_pred) * weights_10[1])\n",
        "    return K.mean(logloss, axis=-1)\n",
        "\n",
        "#custom_loss1 = partial(binary_crossentropy_weigted, class_weights=np.array([for_zeros,for_ones])) ## scoring for model.compile\n",
        "#custom_loss1.__name__ ='binary_crossentropy_weigted'\n",
        "\n",
        "custom_loss2 = partial(weighted_binary_crossentropy, weights_10=np.array([for_ones,for_zeros])) ## scoring for model.compile\n",
        "custom_loss2.__name__ ='weighted_binary_crossentropy'\n",
        "\n",
        "## AUC METRIC\n",
        "def as_keras_metric(method):\n",
        "    import functools\n",
        "    from keras import backend as K\n",
        "    \n",
        "    @functools.wraps(method)\n",
        "    def wrapper(self, args, **kwargs):\n",
        "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
        "        value, update_op = method(self, args, **kwargs)\n",
        "        K.get_session().run(tf.local_variables_initializer())\n",
        "        with tf.control_dependencies([update_op]):\n",
        "            value = tf.identity(value)\n",
        "        return value\n",
        "    return wrapper\n",
        "  \n",
        "auc_roc = as_keras_metric(tf.metrics.auc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Oq0VwYupNmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_model(dense_layer_sizes, filters, kernel_size, pool_size, hidden_layers, loss_function):\n",
        "   \n",
        "    #hidden_layers = 1\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,input_shape=input_shape, activation='relu', data_format='channels_first'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    #model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    #model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    #model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    #model.add(MaxPooling2D(pool_size=pool_size))\n",
        "   \n",
        "    for i in range(0,hidden_layers):\n",
        "      # Add one hidden layer\n",
        "      print('Warstwa '+ str(i+1))\n",
        "      model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "      model.add(MaxPooling2D(pool_size=pool_size))\n",
        "  \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss=loss_function,\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy',auc_roc])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NUxERPzqK4I",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "hidden_layers= 2\n",
        "for i in range(hidden_layers):\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5svLHOfqjgDp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "  \n",
        "def make_model_modified(dense_layer_sizes, filters, kernel_size, pool_size):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,input_shape=input_shape, activation='relu', data_format='channels_first')) # \"channels_first\" corresponds to inputs with shape (batch, channels, height, width)\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "   \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "  \n",
        "def probny_model(dense_layer_sizes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(10, (3,3),input_shape=input_shape, activation='relu', data_format='channels_first')) # \"channels_first\" corresponds to inputs with shape (batch, channels, height, width)\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "   \n",
        "    model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model  \n",
        "  \n",
        "def make_model(dense_layer_sizes, filters, kernel_size, pool_size):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,\n",
        "                     padding='valid',\n",
        "                     input_shape=input_shape, data_format='channels_first'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters, kernel_size))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    for layer_size in dense_layer_sizes:\n",
        "        model.add(Dense(layer_size))\n",
        "        model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))#(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', #'categorical_crossentropy',\n",
        "                  optimizer='adadelta',\n",
        "                  metrics=['accuracy'])\n",
        "    return model  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYsSf-Oucqxl",
        "colab_type": "code",
        "outputId": "d70d30a2-6694-46a3-ed36-ea6e2f49de87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "y=y_train.astype('int')\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(y_train))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15904, 1, 63, 148)\n",
            "(15904,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRdAp3bGb7RJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "my_classifier3 = KerasClassifier(make_model)\n",
        "validator3 = GridSearchCV(my_classifier3,\n",
        "                         param_grid={'dense_layer_sizes': [128]}, \n",
        "                         scoring='neg_log_loss')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76411ZmN85od",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "dense_size_candidates = [[32], [64]] #, [32, 32], [64, 64]]\n",
        "my_classifier = KerasClassifier(make_model, batch_size=32)\n",
        "\n",
        "validator = GridSearchCV(my_classifier,\n",
        "                         param_grid={'dense_layer_sizes': [[32], [64]],\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [1],\n",
        "                                     'filters': [8],\n",
        "                                     'kernel_size': [3],\n",
        "                                     'pool_size': [2]},\n",
        "                         scoring='neg_log_loss',\n",
        "                         n_jobs=1)\n",
        "#y2=y_train.astype('int')\n",
        "validator.fit(X_train, y) #_binary)\n",
        "\n",
        "print('The parameters of the best model are: ')\n",
        "print(validator.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MadBqOqYbAo4",
        "colab_type": "code",
        "outputId": "e3200f80-f627-40a9-8469-10a700d50f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "my_classifier66 = KerasClassifier(search_model)\n",
        "validator66 = GridSearchCV(my_classifier66,\n",
        "                         param_grid={'dense_layer_sizes': [256],\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [30, 20], #[15, 30, 50],\n",
        "                                     'filters': [20], #[10, 20],\n",
        "                                     'kernel_size': [(3,3)], #[(3,3)],\n",
        "                                     'pool_size': [(2,2)],#[(2,2)],\n",
        "                                     'hidden_layers': [3],\n",
        "                                     'loss_function': ['binary_crossentropy'],\n",
        "                                     'batch_size': [32], #[32, 64]\n",
        "                                     }, \n",
        "                         scoring='roc_auc', #''neg_log_loss', # or not specified\n",
        "                         cv = StratifiedKFold(n_splits = 5, random_state=667, shuffle = True))\n",
        "#y=y_train.astype('int')\n",
        "'''\n",
        "validator66 = GridSearchCV(my_classifier66,\n",
        "                         param_grid={'dense_layer_sizes': [128, 256],\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [10,30], #[10, 30, 50],\n",
        "                                     'filters': [10,20], #[10, 20],\n",
        "                                     'kernel_size': [(3,3)], #[(3,3)],\n",
        "                                     'pool_size': [(2,2)],#[(2,2)],\n",
        "                                     'hidden_layers': [2,3],\n",
        "                                     'loss_function': ['binary_crossentropy', custom_loss2],\n",
        "                                     'batch_size': [32,64], #[32, 64]\n",
        "                                     }, \n",
        "                         scoring='neg_log_loss', cv = StratifiedKFold(n_splits =4, random_state=667, shuffle = True))\n",
        "'''\n",
        "grid_result = validator66.fit(X_train, y)\n",
        "print('The parameters of the best model are: ')\n",
        "print(validator66.best_params_)\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 9s 672us/step - loss: 0.7289 - acc: 0.5450 - auc: 0.4940\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.6876 - acc: 0.5544 - auc: 0.5064\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.4890 - acc: 0.7779 - auc: 0.5764\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.3721 - acc: 0.8478 - auc: 0.6967\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.3377 - acc: 0.8674 - auc: 0.7626\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.3154 - acc: 0.8786 - auc: 0.8023\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.3038 - acc: 0.8822 - auc: 0.8278\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.2905 - acc: 0.8892 - auc: 0.8462\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.2854 - acc: 0.8909 - auc: 0.8599\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.2759 - acc: 0.8947 - auc: 0.8700\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.2645 - acc: 0.8996 - auc: 0.8788\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2610 - acc: 0.9033 - auc: 0.8857\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 505us/step - loss: 0.2561 - acc: 0.9061 - auc: 0.8918\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.2493 - acc: 0.9084 - auc: 0.8968\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.2455 - acc: 0.9095 - auc: 0.9013\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.2376 - acc: 0.9109 - auc: 0.9052\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.2316 - acc: 0.9141 - auc: 0.9088\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2211 - acc: 0.9187 - auc: 0.9122\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.2220 - acc: 0.9189 - auc: 0.9153\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2145 - acc: 0.9223 - auc: 0.9181\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.2058 - acc: 0.9253 - auc: 0.9208\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.1999 - acc: 0.9279 - auc: 0.9233\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1897 - acc: 0.9324 - auc: 0.9258\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.1809 - acc: 0.9321 - auc: 0.9281\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.1777 - acc: 0.9343 - auc: 0.9305\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.1680 - acc: 0.9380 - auc: 0.9327\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 484us/step - loss: 0.1659 - acc: 0.9415 - auc: 0.9348\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.1656 - acc: 0.9407 - auc: 0.9369\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1430 - acc: 0.9474 - auc: 0.9388\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.1467 - acc: 0.9484 - auc: 0.9409\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 9s 691us/step - loss: 0.6967 - acc: 0.5469 - auc: 0.4899\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.5653 - acc: 0.7026 - auc: 0.5538\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.3756 - acc: 0.8511 - auc: 0.7091\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.3345 - acc: 0.8709 - auc: 0.7873\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.3082 - acc: 0.8827 - auc: 0.8275\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.2953 - acc: 0.8885 - auc: 0.8522\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.2814 - acc: 0.8929 - auc: 0.8687\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.2725 - acc: 0.9007 - auc: 0.8801\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.2597 - acc: 0.9044 - auc: 0.8890\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.2442 - acc: 0.9110 - auc: 0.8966\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.2374 - acc: 0.9152 - auc: 0.9034\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.2332 - acc: 0.9158 - auc: 0.9086\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.2266 - acc: 0.9187 - auc: 0.9128\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.2145 - acc: 0.9242 - auc: 0.9170\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.2044 - acc: 0.9273 - auc: 0.9206\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.2079 - acc: 0.9251 - auc: 0.9241\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 485us/step - loss: 0.2014 - acc: 0.9282 - auc: 0.9270\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.1914 - acc: 0.9330 - auc: 0.9297\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.1804 - acc: 0.9352 - auc: 0.9324\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.1733 - acc: 0.9399 - auc: 0.9349\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.1655 - acc: 0.9410 - auc: 0.9374\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.1600 - acc: 0.9431 - auc: 0.9398\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.1574 - acc: 0.9460 - auc: 0.9419\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.1453 - acc: 0.9487 - auc: 0.9440\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.1378 - acc: 0.9513 - auc: 0.9461\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1360 - acc: 0.9510 - auc: 0.9481\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.1287 - acc: 0.9547 - auc: 0.9499\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.1262 - acc: 0.9565 - auc: 0.9517\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 486us/step - loss: 0.1189 - acc: 0.9587 - auc: 0.9534\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.1099 - acc: 0.9617 - auc: 0.9550\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 9s 725us/step - loss: 0.6463 - acc: 0.6371 - auc: 0.5704\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.4323 - acc: 0.8149 - auc: 0.7290\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.3586 - acc: 0.8572 - auc: 0.8074\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.3233 - acc: 0.8761 - auc: 0.8432\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.3138 - acc: 0.8819 - auc: 0.8636\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2904 - acc: 0.8924 - auc: 0.8769\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.2827 - acc: 0.8943 - auc: 0.8865\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.2786 - acc: 0.8962 - auc: 0.8936\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.2706 - acc: 0.9011 - auc: 0.8994\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.2649 - acc: 0.9040 - auc: 0.9039\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.2497 - acc: 0.9097 - auc: 0.9081\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 491us/step - loss: 0.2444 - acc: 0.9110 - auc: 0.9118\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.2387 - acc: 0.9152 - auc: 0.9153\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.2361 - acc: 0.9125 - auc: 0.9183\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.2304 - acc: 0.9179 - auc: 0.9209\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.2287 - acc: 0.9202 - auc: 0.9232\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2219 - acc: 0.9224 - auc: 0.9255\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.2084 - acc: 0.9279 - auc: 0.9276\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.2030 - acc: 0.9286 - auc: 0.9298\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 490us/step - loss: 0.2097 - acc: 0.9257 - auc: 0.9317\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.2066 - acc: 0.9274 - auc: 0.9333\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 487us/step - loss: 0.2010 - acc: 0.9299 - auc: 0.9348\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 489us/step - loss: 0.1911 - acc: 0.9329 - auc: 0.9364\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 7s 511us/step - loss: 0.1934 - acc: 0.9326 - auc: 0.9379\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.1841 - acc: 0.9363 - auc: 0.9394\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.1748 - acc: 0.9400 - auc: 0.9408\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.1724 - acc: 0.9404 - auc: 0.9423\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.1661 - acc: 0.9438 - auc: 0.9436\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.1737 - acc: 0.9402 - auc: 0.9449\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.1618 - acc: 0.9421 - auc: 0.9461\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12723/12723 [==============================] - 9s 733us/step - loss: 0.5895 - acc: 0.6849 - auc: 0.5878\n",
            "Epoch 2/30\n",
            "12723/12723 [==============================] - 6s 502us/step - loss: 0.3705 - acc: 0.8507 - auc: 0.7882\n",
            "Epoch 3/30\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.3360 - acc: 0.8682 - auc: 0.8453\n",
            "Epoch 4/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.3155 - acc: 0.8792 - auc: 0.8686\n",
            "Epoch 5/30\n",
            "12723/12723 [==============================] - 6s 488us/step - loss: 0.3042 - acc: 0.8816 - auc: 0.8817\n",
            "Epoch 6/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.2936 - acc: 0.8901 - auc: 0.8909\n",
            "Epoch 7/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.2883 - acc: 0.8890 - auc: 0.8968\n",
            "Epoch 8/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.2721 - acc: 0.8985 - auc: 0.9019\n",
            "Epoch 9/30\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.2655 - acc: 0.9012 - auc: 0.9071\n",
            "Epoch 10/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.2668 - acc: 0.9013 - auc: 0.9108\n",
            "Epoch 11/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.2553 - acc: 0.9057 - auc: 0.9141\n",
            "Epoch 12/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.2474 - acc: 0.9066 - auc: 0.9172\n",
            "Epoch 13/30\n",
            "12723/12723 [==============================] - 6s 496us/step - loss: 0.2393 - acc: 0.9127 - auc: 0.9199\n",
            "Epoch 14/30\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.2325 - acc: 0.9149 - auc: 0.9228\n",
            "Epoch 15/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.2268 - acc: 0.9203 - auc: 0.9252\n",
            "Epoch 16/30\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.2225 - acc: 0.9168 - auc: 0.9275\n",
            "Epoch 17/30\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.2164 - acc: 0.9224 - auc: 0.9297\n",
            "Epoch 18/30\n",
            "12723/12723 [==============================] - 7s 511us/step - loss: 0.2176 - acc: 0.9193 - auc: 0.9318\n",
            "Epoch 19/30\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.2003 - acc: 0.9271 - auc: 0.9337\n",
            "Epoch 20/30\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.1947 - acc: 0.9289 - auc: 0.9358\n",
            "Epoch 21/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1846 - acc: 0.9333 - auc: 0.9378\n",
            "Epoch 22/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1904 - acc: 0.9310 - auc: 0.9396\n",
            "Epoch 23/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1919 - acc: 0.9302 - auc: 0.9410\n",
            "Epoch 24/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1776 - acc: 0.9347 - auc: 0.9426\n",
            "Epoch 25/30\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.1637 - acc: 0.9400 - auc: 0.9443\n",
            "Epoch 26/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1545 - acc: 0.9423 - auc: 0.9459\n",
            "Epoch 27/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.1619 - acc: 0.9420 - auc: 0.9476\n",
            "Epoch 28/30\n",
            "12723/12723 [==============================] - 6s 492us/step - loss: 0.1461 - acc: 0.9473 - auc: 0.9491\n",
            "Epoch 29/30\n",
            "12723/12723 [==============================] - 6s 493us/step - loss: 0.1414 - acc: 0.9481 - auc: 0.9508\n",
            "Epoch 30/30\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.1330 - acc: 0.9524 - auc: 0.9523\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "12724/12724 [==============================] - 10s 769us/step - loss: 0.6968 - acc: 0.5490 - auc: 0.4942\n",
            "Epoch 2/30\n",
            "12724/12724 [==============================] - 7s 520us/step - loss: 0.6879 - acc: 0.5513 - auc: 0.5023\n",
            "Epoch 3/30\n",
            "12724/12724 [==============================] - 7s 513us/step - loss: 0.6866 - acc: 0.5546 - auc: 0.5057\n",
            "Epoch 4/30\n",
            "12724/12724 [==============================] - 6s 498us/step - loss: 0.6879 - acc: 0.5547 - auc: 0.5053\n",
            "Epoch 5/30\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.6864 - acc: 0.5546 - auc: 0.5048\n",
            "Epoch 6/30\n",
            "12724/12724 [==============================] - 6s 506us/step - loss: 0.6874 - acc: 0.5540 - auc: 0.5047\n",
            "Epoch 7/30\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.6878 - acc: 0.5546 - auc: 0.5044\n",
            "Epoch 8/30\n",
            "12724/12724 [==============================] - 6s 503us/step - loss: 0.6871 - acc: 0.5545 - auc: 0.5036\n",
            "Epoch 9/30\n",
            "12724/12724 [==============================] - 6s 503us/step - loss: 0.6870 - acc: 0.5546 - auc: 0.5027\n",
            "Epoch 10/30\n",
            "12724/12724 [==============================] - 6s 502us/step - loss: 0.6888 - acc: 0.5546 - auc: 0.5020\n",
            "Epoch 11/30\n",
            "12724/12724 [==============================] - 6s 499us/step - loss: 0.6866 - acc: 0.5556 - auc: 0.5018\n",
            "Epoch 12/30\n",
            "12724/12724 [==============================] - 6s 507us/step - loss: 0.6869 - acc: 0.5545 - auc: 0.5019\n",
            "Epoch 13/30\n",
            "12724/12724 [==============================] - 6s 505us/step - loss: 0.6945 - acc: 0.5545 - auc: 0.5018\n",
            "Epoch 14/30\n",
            "12724/12724 [==============================] - 6s 502us/step - loss: 0.6877 - acc: 0.5545 - auc: 0.5017\n",
            "Epoch 15/30\n",
            "12724/12724 [==============================] - 6s 502us/step - loss: 0.6865 - acc: 0.5545 - auc: 0.5020\n",
            "Epoch 16/30\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.6878 - acc: 0.5545 - auc: 0.5016\n",
            "Epoch 17/30\n",
            "12724/12724 [==============================] - 6s 502us/step - loss: 0.6871 - acc: 0.5545 - auc: 0.5013\n",
            "Epoch 18/30\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.6861 - acc: 0.5545 - auc: 0.5014\n",
            "Epoch 19/30\n",
            "12724/12724 [==============================] - 6s 502us/step - loss: 0.6874 - acc: 0.5545 - auc: 0.5012\n",
            "Epoch 20/30\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.6878 - acc: 0.5542 - auc: 0.5011\n",
            "Epoch 21/30\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.6872 - acc: 0.5545 - auc: 0.5013\n",
            "Epoch 22/30\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.6873 - acc: 0.5545 - auc: 0.5012\n",
            "Epoch 23/30\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.6872 - acc: 0.5545 - auc: 0.5010\n",
            "Epoch 24/30\n",
            "12724/12724 [==============================] - 6s 506us/step - loss: 0.6872 - acc: 0.5545 - auc: 0.5010\n",
            "Epoch 25/30\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.6874 - acc: 0.5545 - auc: 0.5007\n",
            "Epoch 26/30\n",
            "12724/12724 [==============================] - 6s 505us/step - loss: 0.6873 - acc: 0.5545 - auc: 0.5005\n",
            "Epoch 27/30\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.6872 - acc: 0.5545 - auc: 0.5004\n",
            "Epoch 28/30\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.6872 - acc: 0.5545 - auc: 0.5003\n",
            "Epoch 29/30\n",
            "12724/12724 [==============================] - 6s 501us/step - loss: 0.6872 - acc: 0.5545 - auc: 0.5004\n",
            "Epoch 30/30\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.6872 - acc: 0.5545 - auc: 0.5003\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/20\n",
            "12723/12723 [==============================] - 10s 771us/step - loss: 0.7009 - acc: 0.5521 - auc: 0.4943\n",
            "Epoch 2/20\n",
            "12723/12723 [==============================] - 7s 512us/step - loss: 0.6873 - acc: 0.5556 - auc: 0.5027\n",
            "Epoch 3/20\n",
            "12723/12723 [==============================] - 7s 518us/step - loss: 0.6859 - acc: 0.5575 - auc: 0.5042\n",
            "Epoch 4/20\n",
            "12723/12723 [==============================] - 7s 517us/step - loss: 0.6871 - acc: 0.5547 - auc: 0.5035\n",
            "Epoch 5/20\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.6864 - acc: 0.5547 - auc: 0.5023\n",
            "Epoch 6/20\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.6868 - acc: 0.5547 - auc: 0.5022\n",
            "Epoch 7/20\n",
            "12723/12723 [==============================] - 6s 502us/step - loss: 0.6949 - acc: 0.5583 - auc: 0.5022\n",
            "Epoch 8/20\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.6862 - acc: 0.5546 - auc: 0.5026\n",
            "Epoch 9/20\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.6861 - acc: 0.5547 - auc: 0.5023\n",
            "Epoch 10/20\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.6863 - acc: 0.5547 - auc: 0.5023\n",
            "Epoch 11/20\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.6856 - acc: 0.5547 - auc: 0.5019\n",
            "Epoch 12/20\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.6855 - acc: 0.5582 - auc: 0.5020\n",
            "Epoch 13/20\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.6873 - acc: 0.5549 - auc: 0.5023\n",
            "Epoch 14/20\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6860 - acc: 0.5547 - auc: 0.5019\n",
            "Epoch 15/20\n",
            "12723/12723 [==============================] - 6s 495us/step - loss: 0.6863 - acc: 0.5547 - auc: 0.5017\n",
            "Epoch 16/20\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.5220 - acc: 0.7435 - auc: 0.5117\n",
            "Epoch 17/20\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.3591 - acc: 0.8576 - auc: 0.5458\n",
            "Epoch 18/20\n",
            "12723/12723 [==============================] - 6s 494us/step - loss: 0.3235 - acc: 0.8740 - auc: 0.5806\n",
            "Epoch 19/20\n",
            "12723/12723 [==============================] - 6s 498us/step - loss: 0.2997 - acc: 0.8844 - auc: 0.6115\n",
            "Epoch 20/20\n",
            "12723/12723 [==============================] - 6s 499us/step - loss: 0.2927 - acc: 0.8891 - auc: 0.6390\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/20\n",
            "12723/12723 [==============================] - 10s 785us/step - loss: 0.6536 - acc: 0.6256 - auc: 0.5409\n",
            "Epoch 2/20\n",
            "12723/12723 [==============================] - 7s 512us/step - loss: 0.4008 - acc: 0.8371 - auc: 0.7360\n",
            "Epoch 3/20\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.3449 - acc: 0.8680 - auc: 0.8182\n",
            "Epoch 4/20\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.3180 - acc: 0.8786 - auc: 0.8511\n",
            "Epoch 5/20\n",
            "12723/12723 [==============================] - 6s 505us/step - loss: 0.3069 - acc: 0.8858 - auc: 0.8691\n",
            "Epoch 6/20\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.2919 - acc: 0.8922 - auc: 0.8810\n",
            "Epoch 7/20\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.2831 - acc: 0.8954 - auc: 0.8900\n",
            "Epoch 8/20\n",
            "12723/12723 [==============================] - 6s 509us/step - loss: 0.2711 - acc: 0.9010 - auc: 0.8962\n",
            "Epoch 9/20\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.2677 - acc: 0.9018 - auc: 0.9021\n",
            "Epoch 10/20\n",
            "12723/12723 [==============================] - 6s 497us/step - loss: 0.2625 - acc: 0.9030 - auc: 0.9062\n",
            "Epoch 11/20\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.2526 - acc: 0.9071 - auc: 0.9103\n",
            "Epoch 12/20\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.2496 - acc: 0.9102 - auc: 0.9135\n",
            "Epoch 13/20\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.2416 - acc: 0.9114 - auc: 0.9166\n",
            "Epoch 14/20\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.2383 - acc: 0.9145 - auc: 0.9193\n",
            "Epoch 15/20\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.2320 - acc: 0.9176 - auc: 0.9218\n",
            "Epoch 16/20\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.2254 - acc: 0.9210 - auc: 0.9242\n",
            "Epoch 17/20\n",
            "12723/12723 [==============================] - 6s 500us/step - loss: 0.2228 - acc: 0.9203 - auc: 0.9263\n",
            "Epoch 18/20\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.2113 - acc: 0.9238 - auc: 0.9285\n",
            "Epoch 19/20\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.2111 - acc: 0.9230 - auc: 0.9305\n",
            "Epoch 20/20\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.2031 - acc: 0.9291 - auc: 0.9324\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/20\n",
            "12723/12723 [==============================] - 10s 800us/step - loss: 0.7017 - acc: 0.5467 - auc: 0.5106\n",
            "Epoch 2/20\n",
            "12723/12723 [==============================] - 7s 530us/step - loss: 0.6674 - acc: 0.5894 - auc: 0.5081\n",
            "Epoch 3/20\n",
            "12723/12723 [==============================] - 7s 517us/step - loss: 0.5410 - acc: 0.7335 - auc: 0.5967\n",
            "Epoch 4/20\n",
            "12723/12723 [==============================] - 7s 517us/step - loss: 0.4216 - acc: 0.8210 - auc: 0.6802\n",
            "Epoch 5/20\n",
            "12723/12723 [==============================] - 7s 519us/step - loss: 0.3339 - acc: 0.8690 - auc: 0.7475\n",
            "Epoch 6/20\n",
            "12723/12723 [==============================] - 7s 513us/step - loss: 0.3085 - acc: 0.8820 - auc: 0.7928\n",
            "Epoch 7/20\n",
            "12723/12723 [==============================] - 6s 502us/step - loss: 0.2914 - acc: 0.8912 - auc: 0.8224\n",
            "Epoch 8/20\n",
            "12723/12723 [==============================] - 7s 512us/step - loss: 0.2796 - acc: 0.8950 - auc: 0.8435\n",
            "Epoch 9/20\n",
            "12723/12723 [==============================] - 7s 523us/step - loss: 0.2819 - acc: 0.8930 - auc: 0.8582\n",
            "Epoch 10/20\n",
            "12723/12723 [==============================] - 7s 520us/step - loss: 0.2657 - acc: 0.9005 - auc: 0.8697\n",
            "Epoch 11/20\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.2528 - acc: 0.9072 - auc: 0.8792\n",
            "Epoch 12/20\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.2497 - acc: 0.9114 - auc: 0.8872\n",
            "Epoch 13/20\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.2432 - acc: 0.9127 - auc: 0.8937\n",
            "Epoch 14/20\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.2493 - acc: 0.9087 - auc: 0.8989\n",
            "Epoch 15/20\n",
            "12723/12723 [==============================] - 6s 503us/step - loss: 0.2359 - acc: 0.9147 - auc: 0.9032\n",
            "Epoch 16/20\n",
            "12723/12723 [==============================] - 6s 510us/step - loss: 0.2320 - acc: 0.9180 - auc: 0.9074\n",
            "Epoch 17/20\n",
            "12723/12723 [==============================] - 6s 508us/step - loss: 0.2281 - acc: 0.9190 - auc: 0.9112\n",
            "Epoch 18/20\n",
            "12723/12723 [==============================] - 6s 501us/step - loss: 0.2281 - acc: 0.9179 - auc: 0.9144\n",
            "Epoch 19/20\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.2146 - acc: 0.9231 - auc: 0.9173\n",
            "Epoch 20/20\n",
            "12723/12723 [==============================] - 7s 511us/step - loss: 0.2103 - acc: 0.9254 - auc: 0.9200\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/20\n",
            "12723/12723 [==============================] - 10s 816us/step - loss: 0.6942 - acc: 0.5491 - auc: 0.4906\n",
            "Epoch 2/20\n",
            "12723/12723 [==============================] - 7s 522us/step - loss: 0.4643 - acc: 0.7847 - auc: 0.6142\n",
            "Epoch 3/20\n",
            "12723/12723 [==============================] - 7s 513us/step - loss: 0.3537 - acc: 0.8621 - auc: 0.7656\n",
            "Epoch 4/20\n",
            "12723/12723 [==============================] - 7s 521us/step - loss: 0.3257 - acc: 0.8767 - auc: 0.8224\n",
            "Epoch 5/20\n",
            "12723/12723 [==============================] - 7s 518us/step - loss: 0.3060 - acc: 0.8831 - auc: 0.8504\n",
            "Epoch 6/20\n",
            "12723/12723 [==============================] - 7s 513us/step - loss: 0.2906 - acc: 0.8907 - auc: 0.8689\n",
            "Epoch 7/20\n",
            "12723/12723 [==============================] - 7s 514us/step - loss: 0.2866 - acc: 0.8918 - auc: 0.8802\n",
            "Epoch 8/20\n",
            "12723/12723 [==============================] - 6s 510us/step - loss: 0.2704 - acc: 0.9027 - auc: 0.8894\n",
            "Epoch 9/20\n",
            "12723/12723 [==============================] - 7s 512us/step - loss: 0.2603 - acc: 0.9055 - auc: 0.8967\n",
            "Epoch 10/20\n",
            "12723/12723 [==============================] - 7s 515us/step - loss: 0.2468 - acc: 0.9105 - auc: 0.9032\n",
            "Epoch 11/20\n",
            "12723/12723 [==============================] - 7s 520us/step - loss: 0.2398 - acc: 0.9147 - auc: 0.9086\n",
            "Epoch 12/20\n",
            "12723/12723 [==============================] - 7s 518us/step - loss: 0.2283 - acc: 0.9194 - auc: 0.9133\n",
            "Epoch 13/20\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.2284 - acc: 0.9216 - auc: 0.9173\n",
            "Epoch 14/20\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.2141 - acc: 0.9244 - auc: 0.9207\n",
            "Epoch 15/20\n",
            "12723/12723 [==============================] - 6s 507us/step - loss: 0.2139 - acc: 0.9231 - auc: 0.9241\n",
            "Epoch 16/20\n",
            "12723/12723 [==============================] - 7s 515us/step - loss: 0.2085 - acc: 0.9255 - auc: 0.9269\n",
            "Epoch 17/20\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.2025 - acc: 0.9314 - auc: 0.9295\n",
            "Epoch 18/20\n",
            "12723/12723 [==============================] - 6s 504us/step - loss: 0.1950 - acc: 0.9314 - auc: 0.9319\n",
            "Epoch 19/20\n",
            "12723/12723 [==============================] - 6s 505us/step - loss: 0.1924 - acc: 0.9329 - auc: 0.9342\n",
            "Epoch 20/20\n",
            "12723/12723 [==============================] - 6s 506us/step - loss: 0.1891 - acc: 0.9365 - auc: 0.9361\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/20\n",
            "12724/12724 [==============================] - 11s 828us/step - loss: 0.6848 - acc: 0.5728 - auc: 0.5215\n",
            "Epoch 2/20\n",
            "12724/12724 [==============================] - 6s 507us/step - loss: 0.4352 - acc: 0.8115 - auc: 0.6647\n",
            "Epoch 3/20\n",
            "12724/12724 [==============================] - 7s 511us/step - loss: 0.3466 - acc: 0.8607 - auc: 0.7873\n",
            "Epoch 4/20\n",
            "12724/12724 [==============================] - 7s 511us/step - loss: 0.3192 - acc: 0.8770 - auc: 0.8343\n",
            "Epoch 5/20\n",
            "12724/12724 [==============================] - 6s 506us/step - loss: 0.3081 - acc: 0.8820 - auc: 0.8589\n",
            "Epoch 6/20\n",
            "12724/12724 [==============================] - 6s 506us/step - loss: 0.2946 - acc: 0.8878 - auc: 0.8745\n",
            "Epoch 7/20\n",
            "12724/12724 [==============================] - 6s 508us/step - loss: 0.2911 - acc: 0.8892 - auc: 0.8848\n",
            "Epoch 8/20\n",
            "12724/12724 [==============================] - 6s 506us/step - loss: 0.2901 - acc: 0.8908 - auc: 0.8917\n",
            "Epoch 9/20\n",
            "12724/12724 [==============================] - 6s 505us/step - loss: 0.2745 - acc: 0.8946 - auc: 0.8978\n",
            "Epoch 10/20\n",
            "12724/12724 [==============================] - 6s 508us/step - loss: 0.2762 - acc: 0.8948 - auc: 0.9025\n",
            "Epoch 11/20\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.2640 - acc: 0.9019 - auc: 0.9064\n",
            "Epoch 12/20\n",
            "12724/12724 [==============================] - 6s 502us/step - loss: 0.2546 - acc: 0.9038 - auc: 0.9102\n",
            "Epoch 13/20\n",
            "12724/12724 [==============================] - 7s 517us/step - loss: 0.2538 - acc: 0.9063 - auc: 0.9137\n",
            "Epoch 14/20\n",
            "12724/12724 [==============================] - 7s 515us/step - loss: 0.2433 - acc: 0.9074 - auc: 0.9165\n",
            "Epoch 15/20\n",
            "12724/12724 [==============================] - 6s 504us/step - loss: 0.2467 - acc: 0.9106 - auc: 0.9191\n",
            "Epoch 16/20\n",
            "12724/12724 [==============================] - 6s 501us/step - loss: 0.2356 - acc: 0.9109 - auc: 0.9215\n",
            "Epoch 17/20\n",
            "12724/12724 [==============================] - 6s 502us/step - loss: 0.2257 - acc: 0.9162 - auc: 0.9240\n",
            "Epoch 18/20\n",
            "12724/12724 [==============================] - 6s 502us/step - loss: 0.2193 - acc: 0.9181 - auc: 0.9263\n",
            "Epoch 19/20\n",
            "12724/12724 [==============================] - 6s 500us/step - loss: 0.2102 - acc: 0.9220 - auc: 0.9286\n",
            "Epoch 20/20\n",
            "12724/12724 [==============================] - 6s 509us/step - loss: 0.2094 - acc: 0.9220 - auc: 0.9307\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/20\n",
            "15904/15904 [==============================] - 12s 773us/step - loss: 0.7033 - acc: 0.5516 - auc: 0.5060\n",
            "Epoch 2/20\n",
            "15904/15904 [==============================] - 8s 518us/step - loss: 0.6881 - acc: 0.5546 - auc: 0.5065\n",
            "Epoch 3/20\n",
            "15904/15904 [==============================] - 8s 500us/step - loss: 0.6873 - acc: 0.5538 - auc: 0.5016\n",
            "Epoch 4/20\n",
            "15904/15904 [==============================] - 8s 500us/step - loss: 0.6894 - acc: 0.5548 - auc: 0.5004\n",
            "Epoch 5/20\n",
            "15904/15904 [==============================] - 8s 492us/step - loss: 0.6900 - acc: 0.5559 - auc: 0.5024\n",
            "Epoch 6/20\n",
            "15904/15904 [==============================] - 8s 497us/step - loss: 0.6895 - acc: 0.5541 - auc: 0.5031\n",
            "Epoch 7/20\n",
            "15904/15904 [==============================] - 8s 493us/step - loss: 0.6883 - acc: 0.5545 - auc: 0.5036\n",
            "Epoch 8/20\n",
            "15904/15904 [==============================] - 8s 495us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5035\n",
            "Epoch 9/20\n",
            "15904/15904 [==============================] - 8s 496us/step - loss: 0.6875 - acc: 0.5546 - auc: 0.5029\n",
            "Epoch 10/20\n",
            "15904/15904 [==============================] - 8s 499us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.5027\n",
            "Epoch 11/20\n",
            "15904/15904 [==============================] - 8s 498us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5022\n",
            "Epoch 12/20\n",
            "15904/15904 [==============================] - 8s 509us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5016\n",
            "Epoch 13/20\n",
            "15904/15904 [==============================] - 8s 504us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.5015\n",
            "Epoch 14/20\n",
            "15904/15904 [==============================] - 8s 493us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5009\n",
            "Epoch 15/20\n",
            "15904/15904 [==============================] - 8s 494us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.5006\n",
            "Epoch 16/20\n",
            "15904/15904 [==============================] - 8s 500us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.5002\n",
            "Epoch 17/20\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.6874 - acc: 0.5546 - auc: 0.4999\n",
            "Epoch 18/20\n",
            "15904/15904 [==============================] - 8s 495us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4998\n",
            "Epoch 19/20\n",
            "15904/15904 [==============================] - 8s 494us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4996\n",
            "Epoch 20/20\n",
            "15904/15904 [==============================] - 8s 492us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4995\n",
            "The parameters of the best model are: \n",
            "{'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 20, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.838905 (0.169477) with: {'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 30, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.929378 (0.008435) with: {'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 20, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d7Y-vG3ZRnY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6373ded8-2c3f-4cf4-ab62-9fbea85ca717"
      },
      "source": [
        "'''\n",
        "#################### Rep 1 - spektro ####################\n",
        "# rep 1  ------- 63 x 148 ------ \n",
        "'''\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep1/X_test2_rep1.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep1/y_test2_rep1.npy', allow_pickle=True)\n",
        "'''\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep1/X_test_rep1.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep1/y_test_rep1.npy', allow_pickle=True)  ### ----- 322 positive! -----\n",
        "'''\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep1/X_test1_rep1_nowa676.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep1/y_test1_rep1_nowa676.npy', allow_pickle=True)  ### ----- 676 positive! -----\n",
        "'''\n",
        "# rep 1b ------- 63 x 63 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep1b/X_test2_rep1b.npy', allow_pickle=True) \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep1b/y_test2_rep1b.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep1b/X_test1_rep1b.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep1b/y_test1_rep1b.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 3 - mel-spektro ####################\n",
        "\n",
        "# rep 3 ------- 60 x 111 ------ \n",
        "X_test1_14 = numpy.load('drive/My Drive/rep3/X_test2_rep3.npy', allow_pickle=True)  \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep3/y_test2_popr.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep3/X_test_rep3.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep3/y_test.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3b ------- 60 x 63 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep3b/X_test2_rep3b.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep3b/y_test2_rep3b.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep3b/X_test1_rep3b.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep3b/y_test1_rep3b.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V3 ------- 60 x 148 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep3V3/X_test2_rep3V3.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep3V3/y_test2_rep3V3.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep3V3/X_test1_rep3V3.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep3V3/y_test1_rep3V3.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 5 - mel-spektro ####################\n",
        "\n",
        "# rep 5 ------- 64 x 61 ------ \n",
        "X_test1_14 = numpy.load('drive/My Drive/rep5/X_test2_rep5.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep5/y_test2_rep5.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep5/X_test_rep5.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep5/y_test_rep5.npy', allow_pickle=True) ### ----- 322 positive! -----\n",
        "\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep5/X_test1_rep5_nowa676.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep5/y_test1_rep5_nowa676.npy', allow_pickle=True)  ### ----- 676 positive! -----\n",
        "\n",
        "# rep 5b ------- 64 x 149 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep5b/X_test2_rep5b.npy', allow_pickle=True)  \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep5b/y_test2_rep5b.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep5b/X_test1_rep5b.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep5b/y_test1_rep5b.npy', allow_pickle=True)\n",
        "'''\n",
        "\n",
        "r = np.shape(X_test1_6)[1]\n",
        "s = np.shape(X_test1_6)[2]\n",
        "\n",
        "X_test1_14 = X_test1_14.reshape(X_test1_14.shape[0], 1, r, s).astype('float32')\n",
        "y_test1_14 = np.squeeze(y_test1_14)\n",
        "X_test1_6 = X_test1_6.reshape(X_test1_6.shape[0], 1, r, s).astype('float32')\n",
        "y_test1_6 = np.squeeze(y_test1_6)\n",
        "\n",
        "best1 = validator66.best_params_ #{'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 30, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
        "means_best = grid_result.cv_results_['mean_test_score']\n",
        "a = means_best.argsort()[-3:][::-1]\n",
        "best2 = grid_result.cv_results_['params'][a[1]]\n",
        "#best3 = grid_result.cv_results_['params'][a[2]]\n",
        "#means = np.array([0.765971 ,0.898076 ,0.755596,0.752892,0.900277 ,0.754842])\n",
        "#a = means.argsort()[-3:][::-1]\n",
        "dict_set = [best1, best2]\n",
        "\n",
        "print(dict_set[0])\n",
        "for k in range(0,np.shape(dict_set)[0]):  \n",
        "  best = dict_set[k]\n",
        "  dense_layer_sizes= best['dense_layer_sizes']\n",
        "  filters= best['filters']\n",
        "  kernel_size= best['kernel_size']\n",
        "  pool_size= best['pool_size']\n",
        "  hidden_layers= best['hidden_layers']\n",
        "  loss_function= best['loss_function']\n",
        "  batch_size= best['batch_size']\n",
        "  epochs= best['epochs']\n",
        "  print(epochs)\n",
        "  \n",
        "  chosen_model = search_model(dense_layer_sizes, filters, kernel_size, pool_size, hidden_layers, loss_function)\n",
        "  model_result = chosen_model.fit(X_train, y, batch_size = batch_size, epochs = epochs)\n",
        "  \n",
        "  probs1 = chosen_model.predict_proba(X_test1_14)\n",
        "  probs2 = chosen_model.predict_proba(X_test1_6)\n",
        "  probs = np.concatenate([probs1, probs2])\n",
        "  y_test1 = np.concatenate([y_test1_14, y_test1_6])\n",
        "  print(np.shape(probs))\n",
        "  print(np.shape(y_test1))\n",
        "  auc = roc_auc_score(y_test1, probs)\n",
        "  print('AUC: %.6f' % auc)\n",
        "  \n",
        "  fpr, tpr, thresholds = roc_curve(y_test1, probs)\n",
        "  plt.plot(fpr, tpr)\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 20, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "20\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/20\n",
            "15904/15904 [==============================] - 13s 808us/step - loss: 0.6960 - acc: 0.5475 - auc: 0.5000\n",
            "Epoch 2/20\n",
            "15904/15904 [==============================] - 9s 541us/step - loss: 0.6199 - acc: 0.6395 - auc: 0.5261\n",
            "Epoch 3/20\n",
            "15904/15904 [==============================] - 8s 529us/step - loss: 0.3967 - acc: 0.8362 - auc: 0.6658\n",
            "Epoch 4/20\n",
            "15904/15904 [==============================] - 8s 531us/step - loss: 0.3305 - acc: 0.8714 - auc: 0.7632\n",
            "Epoch 5/20\n",
            "15904/15904 [==============================] - 8s 524us/step - loss: 0.3086 - acc: 0.8791 - auc: 0.8121\n",
            "Epoch 6/20\n",
            "15904/15904 [==============================] - 8s 530us/step - loss: 0.2938 - acc: 0.8885 - auc: 0.8406\n",
            "Epoch 7/20\n",
            "15904/15904 [==============================] - 8s 527us/step - loss: 0.2816 - acc: 0.8952 - auc: 0.8593\n",
            "Epoch 8/20\n",
            "15904/15904 [==============================] - 8s 526us/step - loss: 0.2672 - acc: 0.9024 - auc: 0.8736\n",
            "Epoch 9/20\n",
            "15904/15904 [==============================] - 8s 526us/step - loss: 0.2591 - acc: 0.9042 - auc: 0.8843\n",
            "Epoch 10/20\n",
            "15904/15904 [==============================] - 8s 531us/step - loss: 0.2504 - acc: 0.9080 - auc: 0.8927\n",
            "Epoch 11/20\n",
            "15904/15904 [==============================] - 8s 529us/step - loss: 0.2510 - acc: 0.9091 - auc: 0.8995\n",
            "Epoch 12/20\n",
            "15904/15904 [==============================] - 8s 525us/step - loss: 0.2442 - acc: 0.9116 - auc: 0.9046\n",
            "Epoch 13/20\n",
            "15904/15904 [==============================] - 8s 519us/step - loss: 0.2392 - acc: 0.9127 - auc: 0.9091\n",
            "Epoch 14/20\n",
            "15904/15904 [==============================] - 8s 519us/step - loss: 0.2381 - acc: 0.9143 - auc: 0.9131\n",
            "Epoch 15/20\n",
            "15904/15904 [==============================] - 8s 518us/step - loss: 0.2235 - acc: 0.9191 - auc: 0.9166\n",
            "Epoch 16/20\n",
            "15904/15904 [==============================] - 8s 519us/step - loss: 0.2228 - acc: 0.9218 - auc: 0.9198\n",
            "Epoch 17/20\n",
            "15904/15904 [==============================] - 8s 521us/step - loss: 0.2161 - acc: 0.9214 - auc: 0.9228\n",
            "Epoch 18/20\n",
            "15904/15904 [==============================] - 8s 514us/step - loss: 0.2093 - acc: 0.9264 - auc: 0.9254\n",
            "Epoch 19/20\n",
            "15904/15904 [==============================] - 8s 508us/step - loss: 0.2027 - acc: 0.9276 - auc: 0.9280\n",
            "Epoch 20/20\n",
            "15904/15904 [==============================] - 8s 515us/step - loss: 0.1990 - acc: 0.9252 - auc: 0.9302\n",
            "(102695, 1)\n",
            "(102695,)\n",
            "AUC: 0.901215\n",
            "30\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/30\n",
            "15904/15904 [==============================] - 13s 808us/step - loss: 0.7195 - acc: 0.5492 - auc: 0.5101\n",
            "Epoch 2/30\n",
            "15904/15904 [==============================] - 8s 521us/step - loss: 0.5585 - acc: 0.7101 - auc: 0.5824\n",
            "Epoch 3/30\n",
            "15904/15904 [==============================] - 8s 514us/step - loss: 0.3869 - acc: 0.8402 - auc: 0.7168\n",
            "Epoch 4/30\n",
            "15904/15904 [==============================] - 8s 511us/step - loss: 0.3449 - acc: 0.8624 - auc: 0.7876\n",
            "Epoch 5/30\n",
            "15904/15904 [==============================] - 8s 512us/step - loss: 0.3132 - acc: 0.8804 - auc: 0.8253\n",
            "Epoch 6/30\n",
            "15904/15904 [==============================] - 8s 509us/step - loss: 0.2993 - acc: 0.8877 - auc: 0.8482\n",
            "Epoch 7/30\n",
            "15904/15904 [==============================] - 8s 510us/step - loss: 0.2893 - acc: 0.8902 - auc: 0.8645\n",
            "Epoch 8/30\n",
            "15904/15904 [==============================] - 8s 509us/step - loss: 0.2801 - acc: 0.8978 - auc: 0.8761\n",
            "Epoch 9/30\n",
            "15904/15904 [==============================] - 8s 509us/step - loss: 0.2696 - acc: 0.9014 - auc: 0.8848\n",
            "Epoch 10/30\n",
            "15904/15904 [==============================] - 8s 510us/step - loss: 0.2629 - acc: 0.9035 - auc: 0.8921\n",
            "Epoch 11/30\n",
            "15904/15904 [==============================] - 8s 516us/step - loss: 0.2610 - acc: 0.9039 - auc: 0.8977\n",
            "Epoch 12/30\n",
            "15904/15904 [==============================] - 8s 515us/step - loss: 0.2541 - acc: 0.9066 - auc: 0.9024\n",
            "Epoch 13/30\n",
            "15904/15904 [==============================] - 8s 507us/step - loss: 0.2476 - acc: 0.9098 - auc: 0.9064\n",
            "Epoch 14/30\n",
            "15904/15904 [==============================] - 8s 508us/step - loss: 0.2429 - acc: 0.9140 - auc: 0.9101\n",
            "Epoch 15/30\n",
            "15904/15904 [==============================] - 8s 511us/step - loss: 0.2370 - acc: 0.9151 - auc: 0.9133\n",
            "Epoch 16/30\n",
            "15904/15904 [==============================] - 8s 511us/step - loss: 0.2292 - acc: 0.9179 - auc: 0.9164\n",
            "Epoch 17/30\n",
            "15904/15904 [==============================] - 8s 508us/step - loss: 0.2269 - acc: 0.9200 - auc: 0.9192\n",
            "Epoch 18/30\n",
            "15904/15904 [==============================] - 8s 507us/step - loss: 0.2180 - acc: 0.9228 - auc: 0.9217\n",
            "Epoch 19/30\n",
            "15904/15904 [==============================] - 8s 510us/step - loss: 0.2224 - acc: 0.9209 - auc: 0.9240\n",
            "Epoch 20/30\n",
            "15904/15904 [==============================] - 8s 512us/step - loss: 0.2084 - acc: 0.9264 - auc: 0.9261\n",
            "Epoch 21/30\n",
            "15904/15904 [==============================] - 8s 508us/step - loss: 0.2053 - acc: 0.9286 - auc: 0.9281\n",
            "Epoch 22/30\n",
            "15904/15904 [==============================] - 8s 511us/step - loss: 0.1982 - acc: 0.9309 - auc: 0.9302\n",
            "Epoch 23/30\n",
            "15904/15904 [==============================] - 8s 509us/step - loss: 0.1960 - acc: 0.9300 - auc: 0.9321\n",
            "Epoch 24/30\n",
            "15904/15904 [==============================] - 8s 502us/step - loss: 0.1931 - acc: 0.9302 - auc: 0.9338\n",
            "Epoch 25/30\n",
            "15904/15904 [==============================] - 8s 503us/step - loss: 0.1871 - acc: 0.9342 - auc: 0.9356\n",
            "Epoch 26/30\n",
            "15904/15904 [==============================] - 8s 503us/step - loss: 0.1821 - acc: 0.9342 - auc: 0.9372\n",
            "Epoch 27/30\n",
            "15904/15904 [==============================] - 8s 498us/step - loss: 0.1706 - acc: 0.9400 - auc: 0.9389\n",
            "Epoch 28/30\n",
            "15904/15904 [==============================] - 8s 505us/step - loss: 0.1721 - acc: 0.9377 - auc: 0.9405\n",
            "Epoch 29/30\n",
            "15904/15904 [==============================] - 8s 498us/step - loss: 0.1611 - acc: 0.9429 - auc: 0.9421\n",
            "Epoch 30/30\n",
            "15904/15904 [==============================] - 8s 503us/step - loss: 0.1571 - acc: 0.9428 - auc: 0.9437\n",
            "(102695, 1)\n",
            "(102695,)\n",
            "AUC: 0.892975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRnbG_YouRol",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "dense_layer_sizes= best['dense_layer_sizes']\n",
        "filters= best['filters']\n",
        "kernel_size= best['kernel_size']\n",
        "pool_size= best['pool_size']\n",
        "hidden_layers= best['hidden_layers']\n",
        "loss_function= best['loss_function']\n",
        "batch_size= best['batch_size']\n",
        "epochs= best['epochs']\n",
        "chosen_model = search_model(dense_layer_sizes, filters, kernel_size, pool_size, hidden_layers, loss_function)\n",
        "model_result = chosen_model.fit(X_train, y, batch_size = batch_size, epochs = epochs)\n",
        "\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep1/X_test1_rep1_nowa676.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep1/y_test1_rep1_nowa676.npy', allow_pickle=True)  ### ----- 676 positive! -----\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep1/X_test2_rep1.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep1/y_test2_rep1.npy', allow_pickle=True)\n",
        "\n",
        "r = np.shape(X_test1_6)[1]\n",
        "s = np.shape(X_test1_6)[2]\n",
        "\n",
        "X_test1_14 = X_test1_14.reshape(X_test1_14.shape[0], 1, r, s).astype('float32')\n",
        "y_test1_14 = np.squeeze(y_test1_14)\n",
        "X_test1_6 = X_test1_6.reshape(X_test1_6.shape[0], 1, r, s).astype('float32')\n",
        "y_test1_6 = np.squeeze(y_test1_6)\n",
        "\n",
        "probs1 = chosen_model.predict_proba(X_test1_14)\n",
        "probs2 = chosen_model.predict_proba(X_test1_6)\n",
        "probs = np.concatenate([probs1, probs2])\n",
        "y_test1 = np.concatenate([y_test1_14, y_test1_6])\n",
        "\n",
        "print(np.shape(probs))\n",
        "print(np.shape(y_test1))\n",
        "auc = roc_auc_score(y_test1, probs)\n",
        "#print(\"Run: \" + str(k))\n",
        "print('AUC: %.6f' % auc)\n",
        "fpr, tpr, thresholds = roc_curve(y_test1, probs)\n",
        "plt.plot(fpr, tpr)\n",
        "#plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([-0.01, 1.01])\n",
        "plt.ylim([-0.01, 1.01])\n",
        "#plt.title(title_rys)\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGXggAs2UY2m",
        "colab_type": "code",
        "outputId": "dc80f1f0-98d9-48c6-bcb4-0cdb6010524c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "print('The parameters of the best model are: ')\n",
        "print(validator66.best_params_)\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters of the best model are: \n",
            "{'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.869907 (0.014068) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.671089 (0.122335) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.752012 (0.161275) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.710827 (0.093366) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.758426 (0.166309) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.674233 (0.127986) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.904049 (0.006086) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.687814 (0.123917) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.871227 (0.006594) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.671152 (0.116912) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.888141 (0.004962) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.745410 (0.021864) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.820485 (0.132884) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.635123 (0.157508) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.695108 (0.172173) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.725541 (0.060771) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}