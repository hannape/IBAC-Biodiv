{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopia Gridsearch for 3 representations. ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannape/IBAC-Biodiv/blob/master/Kopia_Gridsearch_for_3_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mq81jmxhMYV",
        "colab_type": "text"
      },
      "source": [
        "Gridsearch dla każdej reprezentacji osobno. Bo na IBACu wszystkie 3 reprezentacje były puszczone na architekturę z mel-spectrogramu (chyba?)\n",
        "\n",
        "Na podstawie hania.dldisc: cnn_gridsearch.ipynb , 03 CNN fit and predict (I2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65s8txmrhL7Y",
        "colab_type": "code",
        "outputId": "42d3879b-91fe-4a38-fe61-f25bd170d94a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from __future__ import print_function"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CddWh85JjLPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "import numpy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import numpy\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "#K.set_image_dim_ordering('th')\n",
        "K.tensorflow_backend.set_image_dim_ordering('th')\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from keras.datasets import mnist\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from functools import partial, update_wrapper\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAjy6F71jTVn",
        "colab_type": "code",
        "outputId": "b4bded5a-ebfe-463b-a6a2-438f9e5ea947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(667)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(667)\n",
        "import random\n",
        "random.seed()\n",
        "\n",
        "'''\n",
        "#################### Rep 1 - spektro ####################\n",
        "# rep 1  ------- 63 x 148 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep1/X_train_rep1.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep1/y_train.npy', allow_pickle=True)\n",
        "\n",
        "# rep 1V2 ------- 63 x 148 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep1V2/X_train_rep1.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep1V2/y_train_rep1.npy', allow_pickle=True)\n",
        "\n",
        "# rep 1b ------- 63 x 63 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep1b/X_train_rep1b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep1b/y_train_rep1b.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 3 - mel-spektro ####################\n",
        "\n",
        "# rep 3 ------- 60 x 111 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3/X_train_rep3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3/y_train.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V2 ------- 60 x 111 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V2/X_train_rep3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V2/y_train_rep3.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3b ------- 60 x 63 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3b/X_train_rep3b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3b/y_train_rep3b.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V3 ------- 60 x 148 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V3/X_train_rep3V3.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep3V3/y_train_rep3V3.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 5 - multitaper ####################\n",
        "\n",
        "# rep 5 ------- 64 x 61 ------ TRAIN 17.3k\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep5/X_train_rep5.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep5/y_train.npy', allow_pickle=True)\n",
        "\n",
        "# rep 5V2 ------- 64 x 61 ------ TRAIN 15.9\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep5V2/X_train_rep5.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep5V2/y_train_rep5.npy', allow_pickle=True)\n",
        "'''\n",
        "# rep 5b ------- 64 x 149 ------\n",
        "X_train1 = numpy.load('drive/My Drive/rep IBAC/rep5b/X_train_rep5b.npy', allow_pickle=True) # 1 and 0 in order\n",
        "y_train1 = numpy.load('drive/My Drive/rep IBAC/rep5b/y_train_rep5b.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "print('Training set size:')\n",
        "print(np.shape(X_train1))\n",
        "print(np.shape(y_train1))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size:\n",
            "(15904, 64, 149)\n",
            "(15904,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST0WDnQSxeLD",
        "colab_type": "code",
        "outputId": "da73a2b6-f611-4445-c75b-b4aab24dcd0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "r = np.shape(X_train1)[1]\n",
        "s = np.shape(X_train1)[2]\n",
        "y_train = y_train1\n",
        "X_train = X_train1.reshape(X_train1.shape[0], 1, r, s).astype('float32')\n",
        "\n",
        "input_shape = (1, r, s)\n",
        "print(input_shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 64, 149)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j27RDs6YmWqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/keras-team/keras/issues/2115\n",
        "\n",
        "### definiowanie wag \n",
        "for_zeros = 0.1\n",
        "for_ones = 0.9\n",
        "###\n",
        "\n",
        "### SCORERS\n",
        "\n",
        "#import tensorflow as tf\n",
        "#import functools\n",
        "#from functools import partial, update_wrapper\n",
        "\n",
        "#def my_score(y_true, y_pred, sample_weight): \n",
        "#  return log_loss(y_true.values, y_pred, sample_weight=sample_weight.loc[y_true.index.values].values.reshape(-1), normalize=True)\n",
        "\n",
        "#my_scorer = make_scorer(my_score,greater_is_better=False, needs_threshold=False,**score_params)  ## scoring for gridsearchCV\n",
        "\n",
        "#def binary_crossentropy_weigted(y_true, y_pred, class_weights):\n",
        "#\ty_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "#  \n",
        "#\tloss = K.mean(class_weights*(-y_true * K.log(y_pred) - (1.0 - y_true) * K.log(1.0 - y_pred)),axis=-1)\n",
        "#\treturn loss\n",
        "\n",
        "def weighted_binary_crossentropy( y_true, y_pred, weights_10) :\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "    logloss = -(y_true * K.log(y_pred) * weights_10[0] + (1 - y_true) * K.log(1 - y_pred) * weights_10[1])\n",
        "    return K.mean(logloss, axis=-1)\n",
        "\n",
        "#custom_loss1 = partial(binary_crossentropy_weigted, class_weights=np.array([for_zeros,for_ones])) ## scoring for model.compile\n",
        "#custom_loss1.__name__ ='binary_crossentropy_weigted'\n",
        "\n",
        "custom_loss2 = partial(weighted_binary_crossentropy, weights_10=np.array([for_ones,for_zeros])) ## scoring for model.compile\n",
        "custom_loss2.__name__ ='weighted_binary_crossentropy'\n",
        "\n",
        "## AUC METRIC\n",
        "def as_keras_metric(method):\n",
        "    import functools\n",
        "    from keras import backend as K\n",
        "    \n",
        "    @functools.wraps(method)\n",
        "    def wrapper(self, args, **kwargs):\n",
        "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
        "        value, update_op = method(self, args, **kwargs)\n",
        "        K.get_session().run(tf.local_variables_initializer())\n",
        "        with tf.control_dependencies([update_op]):\n",
        "            value = tf.identity(value)\n",
        "        return value\n",
        "    return wrapper\n",
        "  \n",
        "auc_roc = as_keras_metric(tf.metrics.auc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Oq0VwYupNmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_model(dense_layer_sizes, filters, kernel_size, pool_size, hidden_layers, loss_function,drop_rate):\n",
        "   \n",
        "    #hidden_layers = 1\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,input_shape=input_shape, activation='relu', data_format='channels_first'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    #model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    #model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    #model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    #model.add(MaxPooling2D(pool_size=pool_size))\n",
        "   \n",
        "    for i in range(0,hidden_layers):\n",
        "      # Add one hidden layer\n",
        "      print('Warstwa '+ str(i+1))\n",
        "      model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "      model.add(MaxPooling2D(pool_size=pool_size))\n",
        "  \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "    model.add(Dropout(drop_rate))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(drop_rate))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss=loss_function,\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy',auc_roc])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NUxERPzqK4I",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "hidden_layers= 2\n",
        "for i in range(hidden_layers):\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5svLHOfqjgDp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "  \n",
        "def make_model_modified(dense_layer_sizes, filters, kernel_size, pool_size):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,input_shape=input_shape, activation='relu', data_format='channels_first')) # \"channels_first\" corresponds to inputs with shape (batch, channels, height, width)\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "   \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "  \n",
        "def probny_model(dense_layer_sizes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(10, (3,3),input_shape=input_shape, activation='relu', data_format='channels_first')) # \"channels_first\" corresponds to inputs with shape (batch, channels, height, width)\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "   \n",
        "    model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model  \n",
        "  \n",
        "def make_model(dense_layer_sizes, filters, kernel_size, pool_size):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,\n",
        "                     padding='valid',\n",
        "                     input_shape=input_shape, data_format='channels_first'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters, kernel_size))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    for layer_size in dense_layer_sizes:\n",
        "        model.add(Dense(layer_size))\n",
        "        model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))#(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', #'categorical_crossentropy',\n",
        "                  optimizer='adadelta',\n",
        "                  metrics=['accuracy'])\n",
        "    return model  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYsSf-Oucqxl",
        "colab_type": "code",
        "outputId": "9d2d648a-6dab-4bb6-9a44-0bf9c2cb3f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "y=y_train.astype('int')\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(y_train))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15904, 1, 64, 149)\n",
            "(15904,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRdAp3bGb7RJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "my_classifier3 = KerasClassifier(make_model)\n",
        "validator3 = GridSearchCV(my_classifier3,\n",
        "                         param_grid={'dense_layer_sizes': [128]}, \n",
        "                         scoring='neg_log_loss')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76411ZmN85od",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "dense_size_candidates = [[32], [64]] #, [32, 32], [64, 64]]\n",
        "my_classifier = KerasClassifier(make_model, batch_size=32)\n",
        "\n",
        "validator = GridSearchCV(my_classifier,\n",
        "                         param_grid={'dense_layer_sizes': [[32], [64]],\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [1],\n",
        "                                     'filters': [8],\n",
        "                                     'kernel_size': [3],\n",
        "                                     'pool_size': [2]},\n",
        "                         scoring='neg_log_loss',\n",
        "                         n_jobs=1)\n",
        "#y2=y_train.astype('int')\n",
        "validator.fit(X_train, y) #_binary)\n",
        "\n",
        "print('The parameters of the best model are: ')\n",
        "print(validator.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MadBqOqYbAo4",
        "colab_type": "code",
        "outputId": "96cff297-0c12-4ac0-faff-cc92ce75ac2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "my_classifier66 = KerasClassifier(search_model)\n",
        "validator66 = GridSearchCV(my_classifier66,\n",
        "                         param_grid={'dense_layer_sizes': [128], #[256],\n",
        "                                     'epochs': [100], #[15, 30, 50], [20, 30]\n",
        "                                     'filters': [10], #[10, 20],\n",
        "                                     'kernel_size': [(3,3)], #[(3,3)],\n",
        "                                     'pool_size': [(2,2)],#[(2,2)],\n",
        "                                     'hidden_layers': [3],\n",
        "                                     'loss_function': ['binary_crossentropy',custom_loss2],\n",
        "                                     'drop_rate': [0.5],\n",
        "                                     'batch_size': [32], #[32, 64]\n",
        "                                     }, \n",
        "                         scoring='roc_auc', #''neg_log_loss', # or not specified\n",
        "                         cv = StratifiedKFold(n_splits = 3, random_state=667, shuffle = True))\n",
        "#y=y_train.astype('int')\n",
        "\n",
        "grid_result = validator66.fit(X_train, y)\n",
        "print('The parameters of the best model are: ')\n",
        "print(validator66.best_params_)\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/100\n",
            "10602/10602 [==============================] - 10s 926us/step - loss: 0.6888 - acc: 0.5546 - auc: 0.4832\n",
            "Epoch 2/100\n",
            "10602/10602 [==============================] - 6s 528us/step - loss: 0.6875 - acc: 0.5547 - auc: 0.4916\n",
            "Epoch 3/100\n",
            "10602/10602 [==============================] - 6s 534us/step - loss: 0.6870 - acc: 0.5547 - auc: 0.4928\n",
            "Epoch 4/100\n",
            "10602/10602 [==============================] - 6s 527us/step - loss: 0.6873 - acc: 0.5547 - auc: 0.4931\n",
            "Epoch 5/100\n",
            "10602/10602 [==============================] - 5s 516us/step - loss: 0.6866 - acc: 0.5548 - auc: 0.4937\n",
            "Epoch 6/100\n",
            "10602/10602 [==============================] - 6s 522us/step - loss: 0.6863 - acc: 0.5547 - auc: 0.4946\n",
            "Epoch 7/100\n",
            "10602/10602 [==============================] - 5s 517us/step - loss: 0.6862 - acc: 0.5549 - auc: 0.4964\n",
            "Epoch 8/100\n",
            "10602/10602 [==============================] - 6s 529us/step - loss: 0.6869 - acc: 0.5548 - auc: 0.4972\n",
            "Epoch 9/100\n",
            "10602/10602 [==============================] - 5s 515us/step - loss: 0.6872 - acc: 0.5549 - auc: 0.4969\n",
            "Epoch 10/100\n",
            "10602/10602 [==============================] - 5s 517us/step - loss: 0.6871 - acc: 0.5549 - auc: 0.4976\n",
            "Epoch 11/100\n",
            "10602/10602 [==============================] - 5s 515us/step - loss: 0.6864 - acc: 0.5548 - auc: 0.4981\n",
            "Epoch 12/100\n",
            "10602/10602 [==============================] - 5s 518us/step - loss: 0.6860 - acc: 0.5549 - auc: 0.4990\n",
            "Epoch 13/100\n",
            "10602/10602 [==============================] - 6s 523us/step - loss: 0.6867 - acc: 0.5548 - auc: 0.4992\n",
            "Epoch 14/100\n",
            "10602/10602 [==============================] - 6s 523us/step - loss: 0.6862 - acc: 0.5549 - auc: 0.4991\n",
            "Epoch 15/100\n",
            "10602/10602 [==============================] - 6s 521us/step - loss: 0.6863 - acc: 0.5549 - auc: 0.4994\n",
            "Epoch 16/100\n",
            "10602/10602 [==============================] - 6s 531us/step - loss: 0.6864 - acc: 0.5548 - auc: 0.4999\n",
            "Epoch 17/100\n",
            "10602/10602 [==============================] - 6s 544us/step - loss: 0.6859 - acc: 0.5549 - auc: 0.5000\n",
            "Epoch 18/100\n",
            "10602/10602 [==============================] - 5s 516us/step - loss: 0.6864 - acc: 0.5530 - auc: 0.5007\n",
            "Epoch 19/100\n",
            "10602/10602 [==============================] - 6s 528us/step - loss: 0.6862 - acc: 0.5549 - auc: 0.5008\n",
            "Epoch 20/100\n",
            "10602/10602 [==============================] - 6s 522us/step - loss: 0.6860 - acc: 0.5549 - auc: 0.5007\n",
            "Epoch 21/100\n",
            "10602/10602 [==============================] - 5s 512us/step - loss: 0.6865 - acc: 0.5549 - auc: 0.5007\n",
            "Epoch 22/100\n",
            "10602/10602 [==============================] - 5s 509us/step - loss: 0.6865 - acc: 0.5548 - auc: 0.5007\n",
            "Epoch 23/100\n",
            "10602/10602 [==============================] - 5s 516us/step - loss: 0.6859 - acc: 0.5550 - auc: 0.5006\n",
            "Epoch 24/100\n",
            "10602/10602 [==============================] - 5s 516us/step - loss: 0.6861 - acc: 0.5548 - auc: 0.5006\n",
            "Epoch 25/100\n",
            "10602/10602 [==============================] - 5s 518us/step - loss: 0.6859 - acc: 0.5538 - auc: 0.5008\n",
            "Epoch 26/100\n",
            "10602/10602 [==============================] - 6s 537us/step - loss: 0.6854 - acc: 0.5551 - auc: 0.5009\n",
            "Epoch 27/100\n",
            "10602/10602 [==============================] - 6s 520us/step - loss: 0.6859 - acc: 0.5549 - auc: 0.5008\n",
            "Epoch 28/100\n",
            "10602/10602 [==============================] - 6s 537us/step - loss: 0.6853 - acc: 0.5552 - auc: 0.5007\n",
            "Epoch 29/100\n",
            "10602/10602 [==============================] - 6s 542us/step - loss: 0.6870 - acc: 0.5549 - auc: 0.5007\n",
            "Epoch 30/100\n",
            "10602/10602 [==============================] - 6s 529us/step - loss: 0.6860 - acc: 0.5548 - auc: 0.5008\n",
            "Epoch 31/100\n",
            "10602/10602 [==============================] - 6s 521us/step - loss: 0.6854 - acc: 0.5552 - auc: 0.5008\n",
            "Epoch 32/100\n",
            "10602/10602 [==============================] - 5s 513us/step - loss: 0.6854 - acc: 0.5551 - auc: 0.5008\n",
            "Epoch 33/100\n",
            "10602/10602 [==============================] - 5s 513us/step - loss: 0.6853 - acc: 0.5552 - auc: 0.5008\n",
            "Epoch 34/100\n",
            "10602/10602 [==============================] - 6s 523us/step - loss: 0.6854 - acc: 0.5554 - auc: 0.5008\n",
            "Epoch 35/100\n",
            "10602/10602 [==============================] - 5s 516us/step - loss: 0.6853 - acc: 0.5553 - auc: 0.5008\n",
            "Epoch 36/100\n",
            "10602/10602 [==============================] - 5s 514us/step - loss: 0.6862 - acc: 0.5556 - auc: 0.5008\n",
            "Epoch 37/100\n",
            "10602/10602 [==============================] - 5s 517us/step - loss: 0.6853 - acc: 0.5553 - auc: 0.5008\n",
            "Epoch 38/100\n",
            "10602/10602 [==============================] - 5s 510us/step - loss: 0.6852 - acc: 0.5565 - auc: 0.5009\n",
            "Epoch 39/100\n",
            "10602/10602 [==============================] - 6s 527us/step - loss: 0.6853 - acc: 0.5556 - auc: 0.5011\n",
            "Epoch 40/100\n",
            "10602/10602 [==============================] - 5s 514us/step - loss: 0.6853 - acc: 0.5549 - auc: 0.5012\n",
            "Epoch 41/100\n",
            "10602/10602 [==============================] - 6s 519us/step - loss: 0.6846 - acc: 0.5558 - auc: 0.5013\n",
            "Epoch 42/100\n",
            "10602/10602 [==============================] - 5s 510us/step - loss: 0.6842 - acc: 0.5561 - auc: 0.5015\n",
            "Epoch 43/100\n",
            "10602/10602 [==============================] - 6s 520us/step - loss: 0.6862 - acc: 0.5563 - auc: 0.5015\n",
            "Epoch 44/100\n",
            "10602/10602 [==============================] - 6s 522us/step - loss: 0.6872 - acc: 0.5563 - auc: 0.5017\n",
            "Epoch 45/100\n",
            "10602/10602 [==============================] - 5s 511us/step - loss: 0.6862 - acc: 0.5573 - auc: 0.5020\n",
            "Epoch 46/100\n",
            "10602/10602 [==============================] - 6s 521us/step - loss: 0.6808 - acc: 0.5732 - auc: 0.5028\n",
            "Epoch 47/100\n",
            "10602/10602 [==============================] - 6s 523us/step - loss: 0.6592 - acc: 0.6136 - auc: 0.5049\n",
            "Epoch 48/100\n",
            "10602/10602 [==============================] - 6s 521us/step - loss: 0.6469 - acc: 0.6309 - auc: 0.5090\n",
            "Epoch 49/100\n",
            "10602/10602 [==============================] - 6s 520us/step - loss: 0.5722 - acc: 0.7218 - auc: 0.5144\n",
            "Epoch 50/100\n",
            "10602/10602 [==============================] - 6s 522us/step - loss: 0.6135 - acc: 0.6578 - auc: 0.5212\n",
            "Epoch 51/100\n",
            "10602/10602 [==============================] - 6s 524us/step - loss: 0.4929 - acc: 0.7889 - auc: 0.5278\n",
            "Epoch 52/100\n",
            "10602/10602 [==============================] - 5s 518us/step - loss: 0.4440 - acc: 0.8222 - auc: 0.5383\n",
            "Epoch 53/100\n",
            "10602/10602 [==============================] - 5s 507us/step - loss: 0.5089 - acc: 0.7624 - auc: 0.5476\n",
            "Epoch 54/100\n",
            "10602/10602 [==============================] - 5s 505us/step - loss: 0.4597 - acc: 0.8102 - auc: 0.5560\n",
            "Epoch 55/100\n",
            "10602/10602 [==============================] - 5s 506us/step - loss: 0.4379 - acc: 0.8221 - auc: 0.5651\n",
            "Epoch 56/100\n",
            "10602/10602 [==============================] - 5s 502us/step - loss: 0.4283 - acc: 0.8277 - auc: 0.5741\n",
            "Epoch 57/100\n",
            "10602/10602 [==============================] - 5s 506us/step - loss: 0.4302 - acc: 0.8278 - auc: 0.5827\n",
            "Epoch 58/100\n",
            "10602/10602 [==============================] - 5s 513us/step - loss: 0.4105 - acc: 0.8399 - auc: 0.5912\n",
            "Epoch 59/100\n",
            "10602/10602 [==============================] - 5s 503us/step - loss: 0.3965 - acc: 0.8447 - auc: 0.5995\n",
            "Epoch 60/100\n",
            "10602/10602 [==============================] - 6s 527us/step - loss: 0.4216 - acc: 0.8270 - auc: 0.6070\n",
            "Epoch 61/100\n",
            "10602/10602 [==============================] - 5s 503us/step - loss: 0.4061 - acc: 0.8426 - auc: 0.6145\n",
            "Epoch 62/100\n",
            "10602/10602 [==============================] - 5s 510us/step - loss: 0.4043 - acc: 0.8389 - auc: 0.6215\n",
            "Epoch 63/100\n",
            "10602/10602 [==============================] - 6s 524us/step - loss: 0.3928 - acc: 0.8535 - auc: 0.6286\n",
            "Epoch 64/100\n",
            "10602/10602 [==============================] - 5s 502us/step - loss: 0.3996 - acc: 0.8432 - auc: 0.6355\n",
            "Epoch 65/100\n",
            "10602/10602 [==============================] - 5s 515us/step - loss: 0.3871 - acc: 0.8513 - auc: 0.6418\n",
            "Epoch 66/100\n",
            "10602/10602 [==============================] - 5s 506us/step - loss: 0.3772 - acc: 0.8555 - auc: 0.6481\n",
            "Epoch 67/100\n",
            "10602/10602 [==============================] - 5s 516us/step - loss: 0.3753 - acc: 0.8583 - auc: 0.6543\n",
            "Epoch 68/100\n",
            "10602/10602 [==============================] - 5s 507us/step - loss: 0.3812 - acc: 0.8543 - auc: 0.6602\n",
            "Epoch 69/100\n",
            "10602/10602 [==============================] - 5s 509us/step - loss: 0.3848 - acc: 0.8533 - auc: 0.6658\n",
            "Epoch 70/100\n",
            "10602/10602 [==============================] - 5s 502us/step - loss: 0.3979 - acc: 0.8427 - auc: 0.6710\n",
            "Epoch 71/100\n",
            "10602/10602 [==============================] - 6s 521us/step - loss: 0.4207 - acc: 0.8325 - auc: 0.6757\n",
            "Epoch 72/100\n",
            "10602/10602 [==============================] - 5s 514us/step - loss: 0.3748 - acc: 0.8607 - auc: 0.6805\n",
            "Epoch 73/100\n",
            "10602/10602 [==============================] - 6s 522us/step - loss: 0.3654 - acc: 0.8619 - auc: 0.6854\n",
            "Epoch 74/100\n",
            "10602/10602 [==============================] - 5s 518us/step - loss: 0.3710 - acc: 0.8592 - auc: 0.6902\n",
            "Epoch 75/100\n",
            "10602/10602 [==============================] - 5s 518us/step - loss: 0.3746 - acc: 0.8581 - auc: 0.6948\n",
            "Epoch 76/100\n",
            "10602/10602 [==============================] - 5s 511us/step - loss: 0.3954 - acc: 0.8484 - auc: 0.6989\n",
            "Epoch 77/100\n",
            "10602/10602 [==============================] - 6s 520us/step - loss: 0.4165 - acc: 0.8152 - auc: 0.7024\n",
            "Epoch 78/100\n",
            "10602/10602 [==============================] - 5s 504us/step - loss: 0.3589 - acc: 0.8638 - auc: 0.7063\n",
            "Epoch 79/100\n",
            "10602/10602 [==============================] - 5s 508us/step - loss: 0.3725 - acc: 0.8564 - auc: 0.7102\n",
            "Epoch 80/100\n",
            "10602/10602 [==============================] - 5s 509us/step - loss: 0.3628 - acc: 0.8605 - auc: 0.7141\n",
            "Epoch 81/100\n",
            "10602/10602 [==============================] - 5s 506us/step - loss: 0.3666 - acc: 0.8589 - auc: 0.7178\n",
            "Epoch 82/100\n",
            "10602/10602 [==============================] - 5s 504us/step - loss: 0.3765 - acc: 0.8503 - auc: 0.7211\n",
            "Epoch 83/100\n",
            "10602/10602 [==============================] - 5s 510us/step - loss: 0.3628 - acc: 0.8623 - auc: 0.7247\n",
            "Epoch 84/100\n",
            "10602/10602 [==============================] - 5s 498us/step - loss: 0.3596 - acc: 0.8640 - auc: 0.7281\n",
            "Epoch 85/100\n",
            "10602/10602 [==============================] - 5s 509us/step - loss: 0.3626 - acc: 0.8631 - auc: 0.7314\n",
            "Epoch 86/100\n",
            "10602/10602 [==============================] - 6s 525us/step - loss: 0.3559 - acc: 0.8613 - auc: 0.7346\n",
            "Epoch 87/100\n",
            "10602/10602 [==============================] - 5s 518us/step - loss: 0.3439 - acc: 0.8710 - auc: 0.7378\n",
            "Epoch 88/100\n",
            "10602/10602 [==============================] - 5s 504us/step - loss: 0.3451 - acc: 0.8711 - auc: 0.7409\n",
            "Epoch 89/100\n",
            "10602/10602 [==============================] - 5s 517us/step - loss: 0.3485 - acc: 0.8690 - auc: 0.7439\n",
            "Epoch 90/100\n",
            "10602/10602 [==============================] - 5s 510us/step - loss: 0.3476 - acc: 0.8724 - auc: 0.7469\n",
            "Epoch 91/100\n",
            "10602/10602 [==============================] - 5s 509us/step - loss: 0.3426 - acc: 0.8725 - auc: 0.7498\n",
            "Epoch 92/100\n",
            "10602/10602 [==============================] - 5s 507us/step - loss: 0.3401 - acc: 0.8716 - auc: 0.7526\n",
            "Epoch 93/100\n",
            "10602/10602 [==============================] - 5s 496us/step - loss: 0.3336 - acc: 0.8713 - auc: 0.7553\n",
            "Epoch 94/100\n",
            "10602/10602 [==============================] - 5s 505us/step - loss: 0.3481 - acc: 0.8687 - auc: 0.7579\n",
            "Epoch 95/100\n",
            "10602/10602 [==============================] - 5s 502us/step - loss: 0.3377 - acc: 0.8743 - auc: 0.7605\n",
            "Epoch 96/100\n",
            "10602/10602 [==============================] - 5s 496us/step - loss: 0.3374 - acc: 0.8747 - auc: 0.7630\n",
            "Epoch 97/100\n",
            "10602/10602 [==============================] - 5s 486us/step - loss: 0.3325 - acc: 0.8753 - auc: 0.7654\n",
            "Epoch 98/100\n",
            "10602/10602 [==============================] - 5s 487us/step - loss: 0.3253 - acc: 0.8790 - auc: 0.7679\n",
            "Epoch 99/100\n",
            "10602/10602 [==============================] - 5s 496us/step - loss: 0.3256 - acc: 0.8790 - auc: 0.7703\n",
            "Epoch 100/100\n",
            "10602/10602 [==============================] - 6s 522us/step - loss: 0.3247 - acc: 0.8791 - auc: 0.7727\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/100\n",
            "10603/10603 [==============================] - 10s 967us/step - loss: 0.6887 - acc: 0.5542 - auc: 0.4843\n",
            "Epoch 2/100\n",
            "10603/10603 [==============================] - 6s 556us/step - loss: 0.6873 - acc: 0.5546 - auc: 0.4944\n",
            "Epoch 3/100\n",
            "10603/10603 [==============================] - 6s 554us/step - loss: 0.6866 - acc: 0.5546 - auc: 0.5002\n",
            "Epoch 4/100\n",
            "10603/10603 [==============================] - 6s 547us/step - loss: 0.6865 - acc: 0.5546 - auc: 0.5026\n",
            "Epoch 5/100\n",
            "10603/10603 [==============================] - 6s 542us/step - loss: 0.6870 - acc: 0.5547 - auc: 0.5019\n",
            "Epoch 6/100\n",
            "10603/10603 [==============================] - 6s 549us/step - loss: 0.6867 - acc: 0.5545 - auc: 0.5017\n",
            "Epoch 7/100\n",
            "10603/10603 [==============================] - 6s 554us/step - loss: 0.6862 - acc: 0.5548 - auc: 0.5019\n",
            "Epoch 8/100\n",
            "10603/10603 [==============================] - 6s 552us/step - loss: 0.6870 - acc: 0.5551 - auc: 0.5023\n",
            "Epoch 9/100\n",
            "10603/10603 [==============================] - 6s 562us/step - loss: 0.6865 - acc: 0.5551 - auc: 0.5021\n",
            "Epoch 10/100\n",
            "10603/10603 [==============================] - 6s 561us/step - loss: 0.6881 - acc: 0.5551 - auc: 0.5021\n",
            "Epoch 11/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.6868 - acc: 0.5549 - auc: 0.5021\n",
            "Epoch 12/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.6867 - acc: 0.5547 - auc: 0.5021\n",
            "Epoch 13/100\n",
            "10603/10603 [==============================] - 6s 552us/step - loss: 0.6871 - acc: 0.5549 - auc: 0.5021\n",
            "Epoch 14/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.6861 - acc: 0.5548 - auc: 0.5023\n",
            "Epoch 15/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.6867 - acc: 0.5548 - auc: 0.5025\n",
            "Epoch 16/100\n",
            "10603/10603 [==============================] - 6s 544us/step - loss: 0.6862 - acc: 0.5551 - auc: 0.5026\n",
            "Epoch 17/100\n",
            "10603/10603 [==============================] - 6s 531us/step - loss: 0.6859 - acc: 0.5551 - auc: 0.5025\n",
            "Epoch 18/100\n",
            "10603/10603 [==============================] - 6s 522us/step - loss: 0.6868 - acc: 0.5550 - auc: 0.5028\n",
            "Epoch 19/100\n",
            "10603/10603 [==============================] - 6s 522us/step - loss: 0.6863 - acc: 0.5545 - auc: 0.5026\n",
            "Epoch 20/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.6858 - acc: 0.5554 - auc: 0.5026\n",
            "Epoch 21/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.6855 - acc: 0.5558 - auc: 0.5025\n",
            "Epoch 22/100\n",
            "10603/10603 [==============================] - 6s 541us/step - loss: 0.6855 - acc: 0.5565 - auc: 0.5026\n",
            "Epoch 23/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.6851 - acc: 0.5571 - auc: 0.5027\n",
            "Epoch 24/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.6841 - acc: 0.5588 - auc: 0.5031\n",
            "Epoch 25/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.6830 - acc: 0.5604 - auc: 0.5036\n",
            "Epoch 26/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.6764 - acc: 0.5784 - auc: 0.5046\n",
            "Epoch 27/100\n",
            "10603/10603 [==============================] - 6s 548us/step - loss: 0.6617 - acc: 0.6068 - auc: 0.5076\n",
            "Epoch 28/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.6550 - acc: 0.6224 - auc: 0.5118\n",
            "Epoch 29/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.5303 - acc: 0.7620 - auc: 0.5226\n",
            "Epoch 30/100\n",
            "10603/10603 [==============================] - 6s 538us/step - loss: 0.4887 - acc: 0.7967 - auc: 0.5392\n",
            "Epoch 31/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.4878 - acc: 0.7946 - auc: 0.5549\n",
            "Epoch 32/100\n",
            "10603/10603 [==============================] - 6s 527us/step - loss: 0.4470 - acc: 0.8172 - auc: 0.5701\n",
            "Epoch 33/100\n",
            "10603/10603 [==============================] - 6s 521us/step - loss: 0.4295 - acc: 0.8309 - auc: 0.5848\n",
            "Epoch 34/100\n",
            "10603/10603 [==============================] - 6s 525us/step - loss: 0.4078 - acc: 0.8414 - auc: 0.5992\n",
            "Epoch 35/100\n",
            "10603/10603 [==============================] - 6s 531us/step - loss: 0.4051 - acc: 0.8388 - auc: 0.6127\n",
            "Epoch 36/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.3964 - acc: 0.8464 - auc: 0.6250\n",
            "Epoch 37/100\n",
            "10603/10603 [==============================] - 6s 538us/step - loss: 0.3807 - acc: 0.8536 - auc: 0.6370\n",
            "Epoch 38/100\n",
            "10603/10603 [==============================] - 6s 536us/step - loss: 0.3767 - acc: 0.8512 - auc: 0.6484\n",
            "Epoch 39/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.3640 - acc: 0.8597 - auc: 0.6587\n",
            "Epoch 40/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.3584 - acc: 0.8687 - auc: 0.6688\n",
            "Epoch 41/100\n",
            "10603/10603 [==============================] - 6s 525us/step - loss: 0.3522 - acc: 0.8673 - auc: 0.6785\n",
            "Epoch 42/100\n",
            "10603/10603 [==============================] - 6s 523us/step - loss: 0.3582 - acc: 0.8632 - auc: 0.6873\n",
            "Epoch 43/100\n",
            "10603/10603 [==============================] - 6s 530us/step - loss: 0.3376 - acc: 0.8724 - auc: 0.6958\n",
            "Epoch 44/100\n",
            "10603/10603 [==============================] - 6s 530us/step - loss: 0.3350 - acc: 0.8745 - auc: 0.7040\n",
            "Epoch 45/100\n",
            "10603/10603 [==============================] - 6s 521us/step - loss: 0.3346 - acc: 0.8759 - auc: 0.7117\n",
            "Epoch 46/100\n",
            "10603/10603 [==============================] - 6s 531us/step - loss: 0.3301 - acc: 0.8751 - auc: 0.7190\n",
            "Epoch 47/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.3227 - acc: 0.8802 - auc: 0.7258\n",
            "Epoch 48/100\n",
            "10603/10603 [==============================] - 6s 525us/step - loss: 0.3391 - acc: 0.8717 - auc: 0.7324\n",
            "Epoch 49/100\n",
            "10603/10603 [==============================] - 6s 529us/step - loss: 0.3250 - acc: 0.8778 - auc: 0.7383\n",
            "Epoch 50/100\n",
            "10603/10603 [==============================] - 6s 529us/step - loss: 0.3315 - acc: 0.8787 - auc: 0.7443\n",
            "Epoch 51/100\n",
            "10603/10603 [==============================] - 6s 551us/step - loss: 0.3190 - acc: 0.8840 - auc: 0.7499\n",
            "Epoch 52/100\n",
            "10603/10603 [==============================] - 6s 527us/step - loss: 0.3256 - acc: 0.8815 - auc: 0.7552\n",
            "Epoch 53/100\n",
            "10603/10603 [==============================] - 6s 528us/step - loss: 0.3122 - acc: 0.8873 - auc: 0.7603\n",
            "Epoch 54/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.3192 - acc: 0.8848 - auc: 0.7652\n",
            "Epoch 55/100\n",
            "10603/10603 [==============================] - 6s 520us/step - loss: 0.3119 - acc: 0.8866 - auc: 0.7699\n",
            "Epoch 56/100\n",
            "10603/10603 [==============================] - 6s 529us/step - loss: 0.3392 - acc: 0.8732 - auc: 0.7740\n",
            "Epoch 57/100\n",
            "10603/10603 [==============================] - 6s 523us/step - loss: 0.3102 - acc: 0.8868 - auc: 0.7782\n",
            "Epoch 58/100\n",
            "10603/10603 [==============================] - 6s 526us/step - loss: 0.3055 - acc: 0.8891 - auc: 0.7823\n",
            "Epoch 59/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.3112 - acc: 0.8876 - auc: 0.7862\n",
            "Epoch 60/100\n",
            "10603/10603 [==============================] - 6s 525us/step - loss: 0.3266 - acc: 0.8693 - auc: 0.7899\n",
            "Epoch 61/100\n",
            "10603/10603 [==============================] - 6s 523us/step - loss: 0.3685 - acc: 0.8373 - auc: 0.7921\n",
            "Epoch 62/100\n",
            "10603/10603 [==============================] - 6s 526us/step - loss: 0.3754 - acc: 0.8481 - auc: 0.7948\n",
            "Epoch 63/100\n",
            "10603/10603 [==============================] - 5s 518us/step - loss: 0.3281 - acc: 0.8794 - auc: 0.7977\n",
            "Epoch 64/100\n",
            "10603/10603 [==============================] - 5s 517us/step - loss: 0.3140 - acc: 0.8830 - auc: 0.8007\n",
            "Epoch 65/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.3080 - acc: 0.8883 - auc: 0.8036\n",
            "Epoch 66/100\n",
            "10603/10603 [==============================] - 5s 516us/step - loss: 0.3082 - acc: 0.8876 - auc: 0.8065\n",
            "Epoch 67/100\n",
            "10603/10603 [==============================] - 6s 524us/step - loss: 0.3166 - acc: 0.8850 - auc: 0.8093\n",
            "Epoch 68/100\n",
            "10603/10603 [==============================] - 5s 513us/step - loss: 0.3073 - acc: 0.8868 - auc: 0.8118\n",
            "Epoch 69/100\n",
            "10603/10603 [==============================] - 5s 511us/step - loss: 0.2988 - acc: 0.8894 - auc: 0.8144\n",
            "Epoch 70/100\n",
            "10603/10603 [==============================] - 6s 521us/step - loss: 0.3148 - acc: 0.8817 - auc: 0.8169\n",
            "Epoch 71/100\n",
            "10603/10603 [==============================] - 6s 521us/step - loss: 0.3018 - acc: 0.8895 - auc: 0.8193\n",
            "Epoch 72/100\n",
            "10603/10603 [==============================] - 6s 522us/step - loss: 0.2950 - acc: 0.8926 - auc: 0.8216\n",
            "Epoch 73/100\n",
            "10603/10603 [==============================] - 6s 524us/step - loss: 0.4069 - acc: 0.8027 - auc: 0.8234\n",
            "Epoch 74/100\n",
            "10603/10603 [==============================] - 5s 516us/step - loss: 0.3150 - acc: 0.8868 - auc: 0.8247\n",
            "Epoch 75/100\n",
            "10603/10603 [==============================] - 6s 525us/step - loss: 0.3091 - acc: 0.8868 - auc: 0.8267\n",
            "Epoch 76/100\n",
            "10603/10603 [==============================] - 6s 523us/step - loss: 0.3124 - acc: 0.8901 - auc: 0.8287\n",
            "Epoch 77/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.2986 - acc: 0.8918 - auc: 0.8307\n",
            "Epoch 78/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.2964 - acc: 0.8904 - auc: 0.8326\n",
            "Epoch 79/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.2979 - acc: 0.8930 - auc: 0.8345\n",
            "Epoch 80/100\n",
            "10603/10603 [==============================] - 6s 538us/step - loss: 0.2965 - acc: 0.8939 - auc: 0.8363\n",
            "Epoch 81/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.2895 - acc: 0.8963 - auc: 0.8381\n",
            "Epoch 82/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.2933 - acc: 0.8963 - auc: 0.8398\n",
            "Epoch 83/100\n",
            "10603/10603 [==============================] - 6s 541us/step - loss: 0.2879 - acc: 0.8989 - auc: 0.8416\n",
            "Epoch 84/100\n",
            "10603/10603 [==============================] - 6s 538us/step - loss: 0.2827 - acc: 0.8964 - auc: 0.8432\n",
            "Epoch 85/100\n",
            "10603/10603 [==============================] - 6s 536us/step - loss: 0.2918 - acc: 0.8947 - auc: 0.8449\n",
            "Epoch 86/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.2964 - acc: 0.8963 - auc: 0.8464\n",
            "Epoch 87/100\n",
            "10603/10603 [==============================] - 6s 529us/step - loss: 0.2880 - acc: 0.8988 - auc: 0.8479\n",
            "Epoch 88/100\n",
            "10603/10603 [==============================] - 6s 526us/step - loss: 0.2917 - acc: 0.8969 - auc: 0.8493\n",
            "Epoch 89/100\n",
            "10603/10603 [==============================] - 6s 526us/step - loss: 0.2859 - acc: 0.9004 - auc: 0.8507\n",
            "Epoch 90/100\n",
            "10603/10603 [==============================] - 6s 531us/step - loss: 0.2908 - acc: 0.8972 - auc: 0.8522\n",
            "Epoch 91/100\n",
            "10603/10603 [==============================] - 6s 527us/step - loss: 0.2846 - acc: 0.8975 - auc: 0.8535\n",
            "Epoch 92/100\n",
            "10603/10603 [==============================] - 6s 527us/step - loss: 0.2857 - acc: 0.9005 - auc: 0.8548\n",
            "Epoch 93/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.2851 - acc: 0.9007 - auc: 0.8561\n",
            "Epoch 94/100\n",
            "10603/10603 [==============================] - 6s 562us/step - loss: 0.2751 - acc: 0.9039 - auc: 0.8574\n",
            "Epoch 95/100\n",
            "10603/10603 [==============================] - 6s 576us/step - loss: 0.2785 - acc: 0.9005 - auc: 0.8587\n",
            "Epoch 96/100\n",
            "10603/10603 [==============================] - 6s 565us/step - loss: 0.2820 - acc: 0.8997 - auc: 0.8599\n",
            "Epoch 97/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.2727 - acc: 0.9045 - auc: 0.8611\n",
            "Epoch 98/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.2779 - acc: 0.9009 - auc: 0.8622\n",
            "Epoch 99/100\n",
            "10603/10603 [==============================] - 6s 551us/step - loss: 0.2761 - acc: 0.9017 - auc: 0.8633\n",
            "Epoch 100/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.2721 - acc: 0.9020 - auc: 0.8644\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/100\n",
            "10603/10603 [==============================] - 10s 968us/step - loss: 0.6888 - acc: 0.5532 - auc: 0.4976\n",
            "Epoch 2/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.6865 - acc: 0.5546 - auc: 0.4998\n",
            "Epoch 3/100\n",
            "10603/10603 [==============================] - 6s 568us/step - loss: 0.6868 - acc: 0.5546 - auc: 0.5027\n",
            "Epoch 4/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.6858 - acc: 0.5547 - auc: 0.5027\n",
            "Epoch 5/100\n",
            "10603/10603 [==============================] - 6s 551us/step - loss: 0.6861 - acc: 0.5547 - auc: 0.5027\n",
            "Epoch 6/100\n",
            "10603/10603 [==============================] - 6s 544us/step - loss: 0.6859 - acc: 0.5547 - auc: 0.5023\n",
            "Epoch 7/100\n",
            "10603/10603 [==============================] - 6s 554us/step - loss: 0.6855 - acc: 0.5548 - auc: 0.5024\n",
            "Epoch 8/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.6858 - acc: 0.5552 - auc: 0.5027\n",
            "Epoch 9/100\n",
            "10603/10603 [==============================] - 6s 556us/step - loss: 0.6856 - acc: 0.5548 - auc: 0.5027\n",
            "Epoch 10/100\n",
            "10603/10603 [==============================] - 6s 555us/step - loss: 0.6854 - acc: 0.5549 - auc: 0.5028\n",
            "Epoch 11/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.6864 - acc: 0.5550 - auc: 0.5030\n",
            "Epoch 12/100\n",
            "10603/10603 [==============================] - 6s 548us/step - loss: 0.6856 - acc: 0.5551 - auc: 0.5027\n",
            "Epoch 13/100\n",
            "10603/10603 [==============================] - 6s 553us/step - loss: 0.6854 - acc: 0.5553 - auc: 0.5022\n",
            "Epoch 14/100\n",
            "10603/10603 [==============================] - 6s 554us/step - loss: 0.6861 - acc: 0.5555 - auc: 0.5023\n",
            "Epoch 15/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.6856 - acc: 0.5549 - auc: 0.5024\n",
            "Epoch 16/100\n",
            "10603/10603 [==============================] - 6s 547us/step - loss: 0.6851 - acc: 0.5551 - auc: 0.5023\n",
            "Epoch 17/100\n",
            "10603/10603 [==============================] - 6s 554us/step - loss: 0.6850 - acc: 0.5554 - auc: 0.5031\n",
            "Epoch 18/100\n",
            "10603/10603 [==============================] - 6s 551us/step - loss: 0.6861 - acc: 0.5547 - auc: 0.5031\n",
            "Epoch 19/100\n",
            "10603/10603 [==============================] - 6s 551us/step - loss: 0.6854 - acc: 0.5538 - auc: 0.5030\n",
            "Epoch 20/100\n",
            "10603/10603 [==============================] - 6s 542us/step - loss: 0.6856 - acc: 0.5552 - auc: 0.5031\n",
            "Epoch 21/100\n",
            "10603/10603 [==============================] - 6s 554us/step - loss: 0.6851 - acc: 0.5549 - auc: 0.5031\n",
            "Epoch 22/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.6849 - acc: 0.5554 - auc: 0.5032\n",
            "Epoch 23/100\n",
            "10603/10603 [==============================] - 6s 536us/step - loss: 0.6853 - acc: 0.5555 - auc: 0.5033\n",
            "Epoch 24/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.6852 - acc: 0.5552 - auc: 0.5033\n",
            "Epoch 25/100\n",
            "10603/10603 [==============================] - 6s 541us/step - loss: 0.6846 - acc: 0.5563 - auc: 0.5033\n",
            "Epoch 26/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.6816 - acc: 0.5614 - auc: 0.5036\n",
            "Epoch 27/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.6575 - acc: 0.6151 - auc: 0.5061\n",
            "Epoch 28/100\n",
            "10603/10603 [==============================] - 6s 542us/step - loss: 0.6225 - acc: 0.6522 - auc: 0.5139\n",
            "Epoch 29/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.5058 - acc: 0.7819 - auc: 0.5259\n",
            "Epoch 30/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.5183 - acc: 0.7510 - auc: 0.5432\n",
            "Epoch 31/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.6628 - acc: 0.5941 - auc: 0.5478\n",
            "Epoch 32/100\n",
            "10603/10603 [==============================] - 6s 541us/step - loss: 0.5540 - acc: 0.7295 - auc: 0.5536\n",
            "Epoch 33/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.4208 - acc: 0.8352 - auc: 0.5674\n",
            "Epoch 34/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.4341 - acc: 0.8234 - auc: 0.5825\n",
            "Epoch 35/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.3969 - acc: 0.8443 - auc: 0.5966\n",
            "Epoch 36/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.3675 - acc: 0.8585 - auc: 0.6103\n",
            "Epoch 37/100\n",
            "10603/10603 [==============================] - 6s 531us/step - loss: 0.3555 - acc: 0.8641 - auc: 0.6237\n",
            "Epoch 38/100\n",
            "10603/10603 [==============================] - 6s 531us/step - loss: 0.3579 - acc: 0.8623 - auc: 0.6361\n",
            "Epoch 39/100\n",
            "10603/10603 [==============================] - 6s 529us/step - loss: 0.3622 - acc: 0.8601 - auc: 0.6476\n",
            "Epoch 40/100\n",
            "10603/10603 [==============================] - 6s 526us/step - loss: 0.3558 - acc: 0.8669 - auc: 0.6584\n",
            "Epoch 41/100\n",
            "10603/10603 [==============================] - 6s 528us/step - loss: 0.3394 - acc: 0.8721 - auc: 0.6687\n",
            "Epoch 42/100\n",
            "10603/10603 [==============================] - 6s 530us/step - loss: 0.3333 - acc: 0.8740 - auc: 0.6785\n",
            "Epoch 43/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.3443 - acc: 0.8736 - auc: 0.6877\n",
            "Epoch 44/100\n",
            "10603/10603 [==============================] - 6s 548us/step - loss: 0.3397 - acc: 0.8709 - auc: 0.6962\n",
            "Epoch 45/100\n",
            "10603/10603 [==============================] - 6s 549us/step - loss: 0.3408 - acc: 0.8747 - auc: 0.7044\n",
            "Epoch 46/100\n",
            "10603/10603 [==============================] - 6s 551us/step - loss: 0.3261 - acc: 0.8796 - auc: 0.7119\n",
            "Epoch 47/100\n",
            "10603/10603 [==============================] - 6s 544us/step - loss: 0.3239 - acc: 0.8811 - auc: 0.7194\n",
            "Epoch 48/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.3256 - acc: 0.8827 - auc: 0.7263\n",
            "Epoch 49/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.3219 - acc: 0.8802 - auc: 0.7329\n",
            "Epoch 50/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.3252 - acc: 0.8813 - auc: 0.7391\n",
            "Epoch 51/100\n",
            "10603/10603 [==============================] - 6s 536us/step - loss: 0.3090 - acc: 0.8875 - auc: 0.7450\n",
            "Epoch 52/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.4693 - acc: 0.7663 - auc: 0.7491\n",
            "Epoch 53/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.3191 - acc: 0.8844 - auc: 0.7526\n",
            "Epoch 54/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.3092 - acc: 0.8867 - auc: 0.7579\n",
            "Epoch 55/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.3181 - acc: 0.8852 - auc: 0.7628\n",
            "Epoch 56/100\n",
            "10603/10603 [==============================] - 6s 550us/step - loss: 0.3270 - acc: 0.8758 - auc: 0.7675\n",
            "Epoch 57/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.3212 - acc: 0.8844 - auc: 0.7717\n",
            "Epoch 58/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.3228 - acc: 0.8787 - auc: 0.7759\n",
            "Epoch 59/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.3151 - acc: 0.8840 - auc: 0.7799\n",
            "Epoch 60/100\n",
            "10603/10603 [==============================] - 6s 530us/step - loss: 0.3148 - acc: 0.8830 - auc: 0.7838\n",
            "Epoch 61/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.3225 - acc: 0.8800 - auc: 0.7873\n",
            "Epoch 62/100\n",
            "10603/10603 [==============================] - 6s 529us/step - loss: 0.3077 - acc: 0.8894 - auc: 0.7909\n",
            "Epoch 63/100\n",
            "10603/10603 [==============================] - 6s 530us/step - loss: 0.3069 - acc: 0.8882 - auc: 0.7944\n",
            "Epoch 64/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.3001 - acc: 0.8917 - auc: 0.7977\n",
            "Epoch 65/100\n",
            "10603/10603 [==============================] - 6s 536us/step - loss: 0.3129 - acc: 0.8852 - auc: 0.8009\n",
            "Epoch 66/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.3040 - acc: 0.8923 - auc: 0.8040\n",
            "Epoch 67/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.3019 - acc: 0.8921 - auc: 0.8069\n",
            "Epoch 68/100\n",
            "10603/10603 [==============================] - 6s 536us/step - loss: 0.3032 - acc: 0.8903 - auc: 0.8098\n",
            "Epoch 69/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.2985 - acc: 0.8927 - auc: 0.8125\n",
            "Epoch 70/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.2941 - acc: 0.8954 - auc: 0.8152\n",
            "Epoch 71/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.2962 - acc: 0.8947 - auc: 0.8178\n",
            "Epoch 72/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.2926 - acc: 0.8949 - auc: 0.8203\n",
            "Epoch 73/100\n",
            "10603/10603 [==============================] - 6s 526us/step - loss: 0.2893 - acc: 0.8957 - auc: 0.8226\n",
            "Epoch 74/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.2894 - acc: 0.8961 - auc: 0.8250\n",
            "Epoch 75/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.2905 - acc: 0.8955 - auc: 0.8272\n",
            "Epoch 76/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.2922 - acc: 0.8971 - auc: 0.8294\n",
            "Epoch 77/100\n",
            "10603/10603 [==============================] - 6s 531us/step - loss: 0.2889 - acc: 0.8967 - auc: 0.8316\n",
            "Epoch 78/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.2898 - acc: 0.8979 - auc: 0.8336\n",
            "Epoch 79/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.3022 - acc: 0.8980 - auc: 0.8356\n",
            "Epoch 80/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.2903 - acc: 0.8990 - auc: 0.8375\n",
            "Epoch 81/100\n",
            "10603/10603 [==============================] - 6s 528us/step - loss: 0.2879 - acc: 0.8979 - auc: 0.8394\n",
            "Epoch 82/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.3011 - acc: 0.8917 - auc: 0.8411\n",
            "Epoch 83/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.2865 - acc: 0.8980 - auc: 0.8428\n",
            "Epoch 84/100\n",
            "10603/10603 [==============================] - 6s 541us/step - loss: 0.2872 - acc: 0.8996 - auc: 0.8445\n",
            "Epoch 85/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.2836 - acc: 0.8992 - auc: 0.8461\n",
            "Epoch 86/100\n",
            "10603/10603 [==============================] - 6s 538us/step - loss: 0.3009 - acc: 0.8902 - auc: 0.8476\n",
            "Epoch 87/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.2885 - acc: 0.8987 - auc: 0.8491\n",
            "Epoch 88/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.2938 - acc: 0.8966 - auc: 0.8505\n",
            "Epoch 89/100\n",
            "10603/10603 [==============================] - 6s 524us/step - loss: 0.2819 - acc: 0.9005 - auc: 0.8520\n",
            "Epoch 90/100\n",
            "10603/10603 [==============================] - 6s 528us/step - loss: 0.2847 - acc: 0.8997 - auc: 0.8534\n",
            "Epoch 91/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.2759 - acc: 0.9030 - auc: 0.8548\n",
            "Epoch 92/100\n",
            "10603/10603 [==============================] - 6s 529us/step - loss: 0.2771 - acc: 0.9013 - auc: 0.8562\n",
            "Epoch 93/100\n",
            "10603/10603 [==============================] - 6s 523us/step - loss: 0.2796 - acc: 0.9016 - auc: 0.8575\n",
            "Epoch 94/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.2782 - acc: 0.9013 - auc: 0.8588\n",
            "Epoch 95/100\n",
            "10603/10603 [==============================] - 6s 541us/step - loss: 0.2768 - acc: 0.9017 - auc: 0.8600\n",
            "Epoch 96/100\n",
            "10603/10603 [==============================] - 6s 526us/step - loss: 0.2793 - acc: 0.9001 - auc: 0.8612\n",
            "Epoch 97/100\n",
            "10603/10603 [==============================] - 6s 538us/step - loss: 0.2767 - acc: 0.9029 - auc: 0.8624\n",
            "Epoch 98/100\n",
            "10603/10603 [==============================] - 6s 530us/step - loss: 0.2735 - acc: 0.9030 - auc: 0.8636\n",
            "Epoch 99/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.2830 - acc: 0.8978 - auc: 0.8647\n",
            "Epoch 100/100\n",
            "10603/10603 [==============================] - 6s 542us/step - loss: 0.2702 - acc: 0.9041 - auc: 0.8658\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/100\n",
            "10602/10602 [==============================] - 11s 1ms/step - loss: 0.1924 - acc: 0.4459 - auc: 0.4914\n",
            "Epoch 2/100\n",
            "10602/10602 [==============================] - 6s 552us/step - loss: 0.1775 - acc: 0.4460 - auc: 0.4924\n",
            "Epoch 3/100\n",
            "10602/10602 [==============================] - 6s 548us/step - loss: 0.1751 - acc: 0.4468 - auc: 0.4947\n",
            "Epoch 4/100\n",
            "10602/10602 [==============================] - 6s 546us/step - loss: 0.1741 - acc: 0.4462 - auc: 0.4983\n",
            "Epoch 5/100\n",
            "10602/10602 [==============================] - 6s 549us/step - loss: 0.1736 - acc: 0.4468 - auc: 0.4996\n",
            "Epoch 6/100\n",
            "10602/10602 [==============================] - 6s 542us/step - loss: 0.1726 - acc: 0.4473 - auc: 0.5005\n",
            "Epoch 7/100\n",
            "10602/10602 [==============================] - 6s 559us/step - loss: 0.1727 - acc: 0.4460 - auc: 0.5011\n",
            "Epoch 8/100\n",
            "10602/10602 [==============================] - 6s 549us/step - loss: 0.1721 - acc: 0.4470 - auc: 0.5011\n",
            "Epoch 9/100\n",
            "10602/10602 [==============================] - 6s 540us/step - loss: 0.1714 - acc: 0.4467 - auc: 0.5023\n",
            "Epoch 10/100\n",
            "10602/10602 [==============================] - 6s 546us/step - loss: 0.1713 - acc: 0.4460 - auc: 0.5027\n",
            "Epoch 11/100\n",
            "10602/10602 [==============================] - 6s 544us/step - loss: 0.1716 - acc: 0.4472 - auc: 0.5030\n",
            "Epoch 12/100\n",
            "10602/10602 [==============================] - 6s 547us/step - loss: 0.1705 - acc: 0.4460 - auc: 0.5030\n",
            "Epoch 13/100\n",
            "10602/10602 [==============================] - 6s 557us/step - loss: 0.1706 - acc: 0.4465 - auc: 0.5029\n",
            "Epoch 14/100\n",
            "10602/10602 [==============================] - 6s 551us/step - loss: 0.1702 - acc: 0.4468 - auc: 0.5029\n",
            "Epoch 15/100\n",
            "10602/10602 [==============================] - 6s 556us/step - loss: 0.1711 - acc: 0.4468 - auc: 0.5025\n",
            "Epoch 16/100\n",
            "10602/10602 [==============================] - 6s 565us/step - loss: 0.1701 - acc: 0.4469 - auc: 0.5023\n",
            "Epoch 17/100\n",
            "10602/10602 [==============================] - 6s 548us/step - loss: 0.1700 - acc: 0.4466 - auc: 0.5020\n",
            "Epoch 18/100\n",
            "10602/10602 [==============================] - 6s 539us/step - loss: 0.1693 - acc: 0.4477 - auc: 0.5020\n",
            "Epoch 19/100\n",
            "10602/10602 [==============================] - 6s 541us/step - loss: 0.1695 - acc: 0.4471 - auc: 0.5020\n",
            "Epoch 20/100\n",
            "10602/10602 [==============================] - 6s 541us/step - loss: 0.1698 - acc: 0.4466 - auc: 0.5017\n",
            "Epoch 21/100\n",
            "10602/10602 [==============================] - 6s 540us/step - loss: 0.1695 - acc: 0.4461 - auc: 0.5015\n",
            "Epoch 22/100\n",
            "10602/10602 [==============================] - 6s 538us/step - loss: 0.1691 - acc: 0.4463 - auc: 0.5016\n",
            "Epoch 23/100\n",
            "10602/10602 [==============================] - 6s 554us/step - loss: 0.1691 - acc: 0.4468 - auc: 0.5016\n",
            "Epoch 24/100\n",
            "10602/10602 [==============================] - 6s 556us/step - loss: 0.1689 - acc: 0.4475 - auc: 0.5018\n",
            "Epoch 25/100\n",
            "10602/10602 [==============================] - 6s 560us/step - loss: 0.1704 - acc: 0.4477 - auc: 0.5018\n",
            "Epoch 26/100\n",
            "10602/10602 [==============================] - 6s 558us/step - loss: 0.1691 - acc: 0.4470 - auc: 0.5018\n",
            "Epoch 27/100\n",
            "10602/10602 [==============================] - 6s 557us/step - loss: 0.1693 - acc: 0.4467 - auc: 0.5015\n",
            "Epoch 28/100\n",
            "10602/10602 [==============================] - 6s 548us/step - loss: 0.1691 - acc: 0.4460 - auc: 0.5015\n",
            "Epoch 29/100\n",
            "10602/10602 [==============================] - 6s 551us/step - loss: 0.1689 - acc: 0.4461 - auc: 0.5015\n",
            "Epoch 30/100\n",
            "10602/10602 [==============================] - 6s 546us/step - loss: 0.1689 - acc: 0.4463 - auc: 0.5015\n",
            "Epoch 31/100\n",
            "10602/10602 [==============================] - 6s 547us/step - loss: 0.1689 - acc: 0.4464 - auc: 0.5015\n",
            "Epoch 32/100\n",
            "10602/10602 [==============================] - 6s 545us/step - loss: 0.1688 - acc: 0.4466 - auc: 0.5014\n",
            "Epoch 33/100\n",
            "10602/10602 [==============================] - 6s 537us/step - loss: 0.1688 - acc: 0.4470 - auc: 0.5013\n",
            "Epoch 34/100\n",
            "10602/10602 [==============================] - 6s 549us/step - loss: 0.1691 - acc: 0.4463 - auc: 0.5013\n",
            "Epoch 35/100\n",
            "10602/10602 [==============================] - 6s 558us/step - loss: 0.1688 - acc: 0.4460 - auc: 0.5013\n",
            "Epoch 36/100\n",
            "10602/10602 [==============================] - 6s 548us/step - loss: 0.1689 - acc: 0.4461 - auc: 0.5013\n",
            "Epoch 37/100\n",
            "10602/10602 [==============================] - 6s 548us/step - loss: 0.1689 - acc: 0.4461 - auc: 0.5013\n",
            "Epoch 38/100\n",
            "10602/10602 [==============================] - 6s 552us/step - loss: 0.1688 - acc: 0.4464 - auc: 0.5012\n",
            "Epoch 39/100\n",
            "10602/10602 [==============================] - 6s 540us/step - loss: 0.1688 - acc: 0.4467 - auc: 0.5011\n",
            "Epoch 40/100\n",
            "10602/10602 [==============================] - 6s 536us/step - loss: 0.1686 - acc: 0.4473 - auc: 0.5011\n",
            "Epoch 41/100\n",
            "10602/10602 [==============================] - 6s 534us/step - loss: 0.1690 - acc: 0.4460 - auc: 0.5010\n",
            "Epoch 42/100\n",
            "10602/10602 [==============================] - 6s 533us/step - loss: 0.1687 - acc: 0.4462 - auc: 0.5010\n",
            "Epoch 43/100\n",
            "10602/10602 [==============================] - 6s 525us/step - loss: 0.1687 - acc: 0.4465 - auc: 0.5009\n",
            "Epoch 44/100\n",
            "10602/10602 [==============================] - 6s 529us/step - loss: 0.1686 - acc: 0.4465 - auc: 0.5009\n",
            "Epoch 45/100\n",
            "10602/10602 [==============================] - 6s 521us/step - loss: 0.1686 - acc: 0.4466 - auc: 0.5010\n",
            "Epoch 46/100\n",
            "10602/10602 [==============================] - 6s 519us/step - loss: 0.1686 - acc: 0.4471 - auc: 0.5010\n",
            "Epoch 47/100\n",
            "10602/10602 [==============================] - 6s 529us/step - loss: 0.1705 - acc: 0.4473 - auc: 0.5009\n",
            "Epoch 48/100\n",
            "10602/10602 [==============================] - 6s 534us/step - loss: 0.1687 - acc: 0.4462 - auc: 0.5009\n",
            "Epoch 49/100\n",
            "10602/10602 [==============================] - 6s 535us/step - loss: 0.1687 - acc: 0.4463 - auc: 0.5008\n",
            "Epoch 50/100\n",
            "10602/10602 [==============================] - 6s 532us/step - loss: 0.1690 - acc: 0.4464 - auc: 0.5007\n",
            "Epoch 51/100\n",
            "10602/10602 [==============================] - 6s 537us/step - loss: 0.1686 - acc: 0.4465 - auc: 0.5007\n",
            "Epoch 52/100\n",
            "10602/10602 [==============================] - 6s 556us/step - loss: 0.1686 - acc: 0.4465 - auc: 0.5007\n",
            "Epoch 53/100\n",
            "10602/10602 [==============================] - 6s 541us/step - loss: 0.1685 - acc: 0.4466 - auc: 0.5008\n",
            "Epoch 54/100\n",
            "10602/10602 [==============================] - 6s 534us/step - loss: 0.1685 - acc: 0.4470 - auc: 0.5008\n",
            "Epoch 55/100\n",
            "10602/10602 [==============================] - 6s 533us/step - loss: 0.1685 - acc: 0.4472 - auc: 0.5009\n",
            "Epoch 56/100\n",
            "10602/10602 [==============================] - 6s 541us/step - loss: 0.1683 - acc: 0.4475 - auc: 0.5009\n",
            "Epoch 57/100\n",
            "10602/10602 [==============================] - 6s 543us/step - loss: 0.1692 - acc: 0.4477 - auc: 0.5010\n",
            "Epoch 58/100\n",
            "10602/10602 [==============================] - 6s 539us/step - loss: 0.1686 - acc: 0.4464 - auc: 0.5010\n",
            "Epoch 59/100\n",
            "10602/10602 [==============================] - 6s 543us/step - loss: 0.1687 - acc: 0.4464 - auc: 0.5009\n",
            "Epoch 60/100\n",
            "10602/10602 [==============================] - 6s 554us/step - loss: 0.1686 - acc: 0.4465 - auc: 0.5009\n",
            "Epoch 61/100\n",
            "10602/10602 [==============================] - 6s 547us/step - loss: 0.1686 - acc: 0.4465 - auc: 0.5009\n",
            "Epoch 62/100\n",
            "10602/10602 [==============================] - 6s 551us/step - loss: 0.1686 - acc: 0.4465 - auc: 0.5009\n",
            "Epoch 63/100\n",
            "10602/10602 [==============================] - 6s 555us/step - loss: 0.1686 - acc: 0.4465 - auc: 0.5009\n",
            "Epoch 64/100\n",
            "10602/10602 [==============================] - 6s 556us/step - loss: 0.1686 - acc: 0.4465 - auc: 0.5008\n",
            "Epoch 65/100\n",
            "10602/10602 [==============================] - 6s 560us/step - loss: 0.1689 - acc: 0.4472 - auc: 0.5008\n",
            "Epoch 66/100\n",
            "10602/10602 [==============================] - 6s 549us/step - loss: 0.1684 - acc: 0.4472 - auc: 0.5008\n",
            "Epoch 67/100\n",
            "10602/10602 [==============================] - 6s 551us/step - loss: 0.1684 - acc: 0.4476 - auc: 0.5008\n",
            "Epoch 68/100\n",
            "10602/10602 [==============================] - 6s 542us/step - loss: 0.1685 - acc: 0.4474 - auc: 0.5008\n",
            "Epoch 69/100\n",
            "10602/10602 [==============================] - 6s 543us/step - loss: 0.1684 - acc: 0.4475 - auc: 0.5007\n",
            "Epoch 70/100\n",
            "10602/10602 [==============================] - 6s 538us/step - loss: 0.1683 - acc: 0.4479 - auc: 0.5007\n",
            "Epoch 71/100\n",
            "10602/10602 [==============================] - 6s 540us/step - loss: 0.1683 - acc: 0.4480 - auc: 0.5007\n",
            "Epoch 72/100\n",
            "10602/10602 [==============================] - 6s 558us/step - loss: 0.1693 - acc: 0.4467 - auc: 0.5007\n",
            "Epoch 73/100\n",
            "10602/10602 [==============================] - 6s 556us/step - loss: 0.1686 - acc: 0.4469 - auc: 0.5008\n",
            "Epoch 74/100\n",
            "10602/10602 [==============================] - 6s 560us/step - loss: 0.1685 - acc: 0.4472 - auc: 0.5008\n",
            "Epoch 75/100\n",
            "10602/10602 [==============================] - 6s 554us/step - loss: 0.1685 - acc: 0.4472 - auc: 0.5007\n",
            "Epoch 76/100\n",
            "10602/10602 [==============================] - 6s 552us/step - loss: 0.1685 - acc: 0.4474 - auc: 0.5007\n",
            "Epoch 77/100\n",
            "10602/10602 [==============================] - 6s 556us/step - loss: 0.1684 - acc: 0.4474 - auc: 0.5007\n",
            "Epoch 78/100\n",
            "10602/10602 [==============================] - 6s 554us/step - loss: 0.1684 - acc: 0.4476 - auc: 0.5007\n",
            "Epoch 79/100\n",
            "10602/10602 [==============================] - 6s 562us/step - loss: 0.1684 - acc: 0.4476 - auc: 0.5007\n",
            "Epoch 80/100\n",
            "10602/10602 [==============================] - 6s 559us/step - loss: 0.1683 - acc: 0.4479 - auc: 0.5007\n",
            "Epoch 81/100\n",
            "10602/10602 [==============================] - 6s 565us/step - loss: 0.1683 - acc: 0.4477 - auc: 0.5007\n",
            "Epoch 82/100\n",
            "10602/10602 [==============================] - 6s 559us/step - loss: 0.1683 - acc: 0.4477 - auc: 0.5008\n",
            "Epoch 83/100\n",
            "10602/10602 [==============================] - 6s 561us/step - loss: 0.1684 - acc: 0.4477 - auc: 0.5008\n",
            "Epoch 84/100\n",
            "10602/10602 [==============================] - 6s 557us/step - loss: 0.1684 - acc: 0.4474 - auc: 0.5008\n",
            "Epoch 85/100\n",
            "10602/10602 [==============================] - 6s 558us/step - loss: 0.1683 - acc: 0.4477 - auc: 0.5008\n",
            "Epoch 86/100\n",
            "10602/10602 [==============================] - 6s 569us/step - loss: 0.1683 - acc: 0.4479 - auc: 0.5008\n",
            "Epoch 87/100\n",
            "10602/10602 [==============================] - 6s 560us/step - loss: 0.1686 - acc: 0.4482 - auc: 0.5008\n",
            "Epoch 88/100\n",
            "10602/10602 [==============================] - 6s 552us/step - loss: 0.1682 - acc: 0.4481 - auc: 0.5008\n",
            "Epoch 89/100\n",
            "10602/10602 [==============================] - 6s 561us/step - loss: 0.1692 - acc: 0.4476 - auc: 0.5008\n",
            "Epoch 90/100\n",
            "10602/10602 [==============================] - 6s 565us/step - loss: 0.1685 - acc: 0.4470 - auc: 0.5008\n",
            "Epoch 91/100\n",
            "10602/10602 [==============================] - 6s 557us/step - loss: 0.1685 - acc: 0.4471 - auc: 0.5008\n",
            "Epoch 92/100\n",
            "10602/10602 [==============================] - 6s 559us/step - loss: 0.1684 - acc: 0.4471 - auc: 0.5008\n",
            "Epoch 93/100\n",
            "10602/10602 [==============================] - 6s 563us/step - loss: 0.1684 - acc: 0.4472 - auc: 0.5008\n",
            "Epoch 94/100\n",
            "10602/10602 [==============================] - 6s 563us/step - loss: 0.1684 - acc: 0.4473 - auc: 0.5008\n",
            "Epoch 95/100\n",
            "10602/10602 [==============================] - 6s 563us/step - loss: 0.1684 - acc: 0.4473 - auc: 0.5008\n",
            "Epoch 96/100\n",
            "10602/10602 [==============================] - 6s 558us/step - loss: 0.1684 - acc: 0.4472 - auc: 0.5008\n",
            "Epoch 97/100\n",
            "10602/10602 [==============================] - 6s 574us/step - loss: 0.1684 - acc: 0.4475 - auc: 0.5008\n",
            "Epoch 98/100\n",
            "10602/10602 [==============================] - 6s 564us/step - loss: 0.1684 - acc: 0.4476 - auc: 0.5008\n",
            "Epoch 99/100\n",
            "10602/10602 [==============================] - 6s 557us/step - loss: 0.1684 - acc: 0.4476 - auc: 0.5008\n",
            "Epoch 100/100\n",
            "10602/10602 [==============================] - 6s 552us/step - loss: 0.1683 - acc: 0.4477 - auc: 0.5008\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/100\n",
            "10603/10603 [==============================] - 11s 1ms/step - loss: 0.1839 - acc: 0.4453 - auc: 0.5078\n",
            "Epoch 2/100\n",
            "10603/10603 [==============================] - 6s 562us/step - loss: 0.1765 - acc: 0.4461 - auc: 0.5057\n",
            "Epoch 3/100\n",
            "10603/10603 [==============================] - 6s 565us/step - loss: 0.1737 - acc: 0.4461 - auc: 0.5032\n",
            "Epoch 4/100\n",
            "10603/10603 [==============================] - 6s 547us/step - loss: 0.1732 - acc: 0.4473 - auc: 0.5041\n",
            "Epoch 5/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.1728 - acc: 0.4465 - auc: 0.5035\n",
            "Epoch 6/100\n",
            "10603/10603 [==============================] - 6s 550us/step - loss: 0.1715 - acc: 0.4467 - auc: 0.5044\n",
            "Epoch 7/100\n",
            "10603/10603 [==============================] - 6s 547us/step - loss: 0.1715 - acc: 0.4466 - auc: 0.5044\n",
            "Epoch 8/100\n",
            "10603/10603 [==============================] - 6s 555us/step - loss: 0.1710 - acc: 0.4463 - auc: 0.5047\n",
            "Epoch 9/100\n",
            "10603/10603 [==============================] - 6s 552us/step - loss: 0.1710 - acc: 0.4471 - auc: 0.5051\n",
            "Epoch 10/100\n",
            "10603/10603 [==============================] - 6s 550us/step - loss: 0.1708 - acc: 0.4469 - auc: 0.5046\n",
            "Epoch 11/100\n",
            "10603/10603 [==============================] - 6s 542us/step - loss: 0.1713 - acc: 0.4478 - auc: 0.5040\n",
            "Epoch 12/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.1705 - acc: 0.4467 - auc: 0.5032\n",
            "Epoch 13/100\n",
            "10603/10603 [==============================] - 6s 536us/step - loss: 0.1700 - acc: 0.4469 - auc: 0.5035\n",
            "Epoch 14/100\n",
            "10603/10603 [==============================] - 6s 544us/step - loss: 0.1704 - acc: 0.4468 - auc: 0.5034\n",
            "Epoch 15/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.1701 - acc: 0.4469 - auc: 0.5032\n",
            "Epoch 16/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.1697 - acc: 0.4477 - auc: 0.5031\n",
            "Epoch 17/100\n",
            "10603/10603 [==============================] - 6s 538us/step - loss: 0.1698 - acc: 0.4462 - auc: 0.5033\n",
            "Epoch 18/100\n",
            "10603/10603 [==============================] - 6s 531us/step - loss: 0.1698 - acc: 0.4465 - auc: 0.5036\n",
            "Epoch 19/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.1698 - acc: 0.4461 - auc: 0.5036\n",
            "Epoch 20/100\n",
            "10603/10603 [==============================] - 6s 544us/step - loss: 0.1697 - acc: 0.4468 - auc: 0.5034\n",
            "Epoch 21/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.1695 - acc: 0.4473 - auc: 0.5031\n",
            "Epoch 22/100\n",
            "10603/10603 [==============================] - 6s 549us/step - loss: 0.1697 - acc: 0.4471 - auc: 0.5030\n",
            "Epoch 23/100\n",
            "10603/10603 [==============================] - 6s 548us/step - loss: 0.1690 - acc: 0.4467 - auc: 0.5028\n",
            "Epoch 24/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.1699 - acc: 0.4456 - auc: 0.5027\n",
            "Epoch 25/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.1694 - acc: 0.4466 - auc: 0.5026\n",
            "Epoch 26/100\n",
            "10603/10603 [==============================] - 6s 547us/step - loss: 0.1688 - acc: 0.4472 - auc: 0.5024\n",
            "Epoch 27/100\n",
            "10603/10603 [==============================] - 6s 549us/step - loss: 0.1692 - acc: 0.4470 - auc: 0.5026\n",
            "Epoch 28/100\n",
            "10603/10603 [==============================] - 6s 561us/step - loss: 0.1692 - acc: 0.4462 - auc: 0.5027\n",
            "Epoch 29/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.1689 - acc: 0.4467 - auc: 0.5027\n",
            "Epoch 30/100\n",
            "10603/10603 [==============================] - 6s 555us/step - loss: 0.1694 - acc: 0.4472 - auc: 0.5026\n",
            "Epoch 31/100\n",
            "10603/10603 [==============================] - 6s 551us/step - loss: 0.1692 - acc: 0.4461 - auc: 0.5026\n",
            "Epoch 32/100\n",
            "10603/10603 [==============================] - 6s 553us/step - loss: 0.1689 - acc: 0.4463 - auc: 0.5024\n",
            "Epoch 33/100\n",
            "10603/10603 [==============================] - 6s 552us/step - loss: 0.1694 - acc: 0.4465 - auc: 0.5024\n",
            "Epoch 34/100\n",
            "10603/10603 [==============================] - 6s 554us/step - loss: 0.1688 - acc: 0.4460 - auc: 0.5024\n",
            "Epoch 35/100\n",
            "10603/10603 [==============================] - 6s 557us/step - loss: 0.1689 - acc: 0.4462 - auc: 0.5024\n",
            "Epoch 36/100\n",
            "10603/10603 [==============================] - 6s 567us/step - loss: 0.1689 - acc: 0.4466 - auc: 0.5024\n",
            "Epoch 37/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.1689 - acc: 0.4469 - auc: 0.5023\n",
            "Epoch 38/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.1688 - acc: 0.4471 - auc: 0.5023\n",
            "Epoch 39/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.1689 - acc: 0.4466 - auc: 0.5022\n",
            "Epoch 40/100\n",
            "10603/10603 [==============================] - 6s 542us/step - loss: 0.1687 - acc: 0.4467 - auc: 0.5022\n",
            "Epoch 41/100\n",
            "10603/10603 [==============================] - 6s 551us/step - loss: 0.1687 - acc: 0.4469 - auc: 0.5022\n",
            "Epoch 42/100\n",
            "10603/10603 [==============================] - 6s 547us/step - loss: 0.1687 - acc: 0.4473 - auc: 0.5021\n",
            "Epoch 43/100\n",
            "10603/10603 [==============================] - 6s 553us/step - loss: 0.1695 - acc: 0.4472 - auc: 0.5020\n",
            "Epoch 44/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.1687 - acc: 0.4465 - auc: 0.5021\n",
            "Epoch 45/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.1688 - acc: 0.4462 - auc: 0.5020\n",
            "Epoch 46/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.1688 - acc: 0.4467 - auc: 0.5020\n",
            "Epoch 47/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.1686 - acc: 0.4466 - auc: 0.5019\n",
            "Epoch 48/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.1687 - acc: 0.4468 - auc: 0.5020\n",
            "Epoch 49/100\n",
            "10603/10603 [==============================] - 6s 530us/step - loss: 0.1686 - acc: 0.4470 - auc: 0.5019\n",
            "Epoch 50/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.1686 - acc: 0.4473 - auc: 0.5019\n",
            "Epoch 51/100\n",
            "10603/10603 [==============================] - 6s 544us/step - loss: 0.1686 - acc: 0.4474 - auc: 0.5019\n",
            "Epoch 52/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.1686 - acc: 0.4469 - auc: 0.5019\n",
            "Epoch 53/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.1684 - acc: 0.4476 - auc: 0.5019\n",
            "Epoch 54/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.1685 - acc: 0.4475 - auc: 0.5019\n",
            "Epoch 55/100\n",
            "10603/10603 [==============================] - 6s 555us/step - loss: 0.1684 - acc: 0.4475 - auc: 0.5019\n",
            "Epoch 56/100\n",
            "10603/10603 [==============================] - 6s 560us/step - loss: 0.1684 - acc: 0.4477 - auc: 0.5020\n",
            "Epoch 57/100\n",
            "10603/10603 [==============================] - 6s 552us/step - loss: 0.1685 - acc: 0.4478 - auc: 0.5020\n",
            "Epoch 58/100\n",
            "10603/10603 [==============================] - 6s 544us/step - loss: 0.1685 - acc: 0.4474 - auc: 0.5019\n",
            "Epoch 59/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.1689 - acc: 0.4475 - auc: 0.5019\n",
            "Epoch 60/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.1685 - acc: 0.4468 - auc: 0.5019\n",
            "Epoch 61/100\n",
            "10603/10603 [==============================] - 6s 556us/step - loss: 0.1685 - acc: 0.4469 - auc: 0.5019\n",
            "Epoch 62/100\n",
            "10603/10603 [==============================] - 6s 549us/step - loss: 0.1685 - acc: 0.4471 - auc: 0.5019\n",
            "Epoch 63/100\n",
            "10603/10603 [==============================] - 6s 561us/step - loss: 0.1684 - acc: 0.4472 - auc: 0.5019\n",
            "Epoch 64/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.1684 - acc: 0.4474 - auc: 0.5020\n",
            "Epoch 65/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.1684 - acc: 0.4476 - auc: 0.5020\n",
            "Epoch 66/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.1684 - acc: 0.4477 - auc: 0.5020\n",
            "Epoch 67/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.1696 - acc: 0.4485 - auc: 0.5020\n",
            "Epoch 68/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.1684 - acc: 0.4478 - auc: 0.5020\n",
            "Epoch 69/100\n",
            "10603/10603 [==============================] - 6s 547us/step - loss: 0.1683 - acc: 0.4480 - auc: 0.5020\n",
            "Epoch 70/100\n",
            "10603/10603 [==============================] - 6s 541us/step - loss: 0.1684 - acc: 0.4479 - auc: 0.5020\n",
            "Epoch 71/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.1684 - acc: 0.4479 - auc: 0.5020\n",
            "Epoch 72/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.1683 - acc: 0.4479 - auc: 0.5020\n",
            "Epoch 73/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.1684 - acc: 0.4477 - auc: 0.5021\n",
            "Epoch 74/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.1683 - acc: 0.4480 - auc: 0.5021\n",
            "Epoch 75/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.1682 - acc: 0.4480 - auc: 0.5021\n",
            "Epoch 76/100\n",
            "10603/10603 [==============================] - 6s 542us/step - loss: 0.1689 - acc: 0.4474 - auc: 0.5021\n",
            "Epoch 77/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.1685 - acc: 0.4473 - auc: 0.5021\n",
            "Epoch 78/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.1684 - acc: 0.4475 - auc: 0.5020\n",
            "Epoch 79/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.1684 - acc: 0.4477 - auc: 0.5020\n",
            "Epoch 80/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.1683 - acc: 0.4477 - auc: 0.5020\n",
            "Epoch 81/100\n",
            "10603/10603 [==============================] - 6s 533us/step - loss: 0.1683 - acc: 0.4477 - auc: 0.5020\n",
            "Epoch 82/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.1683 - acc: 0.4478 - auc: 0.5020\n",
            "Epoch 83/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.1683 - acc: 0.4480 - auc: 0.5020\n",
            "Epoch 84/100\n",
            "10603/10603 [==============================] - 6s 536us/step - loss: 0.1682 - acc: 0.4481 - auc: 0.5020\n",
            "Epoch 85/100\n",
            "10603/10603 [==============================] - 6s 541us/step - loss: 0.1682 - acc: 0.4482 - auc: 0.5020\n",
            "Epoch 86/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.1687 - acc: 0.4478 - auc: 0.5020\n",
            "Epoch 87/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.1682 - acc: 0.4478 - auc: 0.5020\n",
            "Epoch 88/100\n",
            "10603/10603 [==============================] - 6s 549us/step - loss: 0.1682 - acc: 0.4479 - auc: 0.5020\n",
            "Epoch 89/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.1681 - acc: 0.4484 - auc: 0.5020\n",
            "Epoch 90/100\n",
            "10603/10603 [==============================] - 6s 552us/step - loss: 0.1681 - acc: 0.4485 - auc: 0.5020\n",
            "Epoch 91/100\n",
            "10603/10603 [==============================] - 6s 547us/step - loss: 0.1682 - acc: 0.4481 - auc: 0.5020\n",
            "Epoch 92/100\n",
            "10603/10603 [==============================] - 6s 552us/step - loss: 0.1682 - acc: 0.4483 - auc: 0.5021\n",
            "Epoch 93/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.1681 - acc: 0.4483 - auc: 0.5021\n",
            "Epoch 94/100\n",
            "10603/10603 [==============================] - 6s 536us/step - loss: 0.1680 - acc: 0.4487 - auc: 0.5021\n",
            "Epoch 95/100\n",
            "10603/10603 [==============================] - 6s 544us/step - loss: 0.1680 - acc: 0.4486 - auc: 0.5021\n",
            "Epoch 96/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.1685 - acc: 0.4480 - auc: 0.5021\n",
            "Epoch 97/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.1681 - acc: 0.4480 - auc: 0.5021\n",
            "Epoch 98/100\n",
            "10603/10603 [==============================] - 6s 537us/step - loss: 0.1682 - acc: 0.4483 - auc: 0.5021\n",
            "Epoch 99/100\n",
            "10603/10603 [==============================] - 6s 541us/step - loss: 0.1680 - acc: 0.4485 - auc: 0.5021\n",
            "Epoch 100/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.1679 - acc: 0.4487 - auc: 0.5021\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/100\n",
            "10603/10603 [==============================] - 11s 1ms/step - loss: 0.1869 - acc: 0.4459 - auc: 0.4997\n",
            "Epoch 2/100\n",
            "10603/10603 [==============================] - 6s 544us/step - loss: 0.1768 - acc: 0.4476 - auc: 0.5004\n",
            "Epoch 3/100\n",
            "10603/10603 [==============================] - 6s 546us/step - loss: 0.1747 - acc: 0.4477 - auc: 0.4997\n",
            "Epoch 4/100\n",
            "10603/10603 [==============================] - 6s 540us/step - loss: 0.1746 - acc: 0.4477 - auc: 0.5007\n",
            "Epoch 5/100\n",
            "10603/10603 [==============================] - 6s 542us/step - loss: 0.1732 - acc: 0.4482 - auc: 0.4998\n",
            "Epoch 6/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.1718 - acc: 0.4482 - auc: 0.5008\n",
            "Epoch 7/100\n",
            "10603/10603 [==============================] - 6s 559us/step - loss: 0.1721 - acc: 0.4484 - auc: 0.5014\n",
            "Epoch 8/100\n",
            "10603/10603 [==============================] - 6s 567us/step - loss: 0.1715 - acc: 0.4486 - auc: 0.5013\n",
            "Epoch 9/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.1711 - acc: 0.4483 - auc: 0.5011\n",
            "Epoch 10/100\n",
            "10603/10603 [==============================] - 6s 541us/step - loss: 0.1708 - acc: 0.4485 - auc: 0.5016\n",
            "Epoch 11/100\n",
            "10603/10603 [==============================] - 6s 552us/step - loss: 0.1703 - acc: 0.4472 - auc: 0.5013\n",
            "Epoch 12/100\n",
            "10603/10603 [==============================] - 6s 554us/step - loss: 0.1699 - acc: 0.4486 - auc: 0.5020\n",
            "Epoch 13/100\n",
            "10603/10603 [==============================] - 6s 566us/step - loss: 0.1703 - acc: 0.4481 - auc: 0.5021\n",
            "Epoch 14/100\n",
            "10603/10603 [==============================] - 6s 545us/step - loss: 0.1693 - acc: 0.4484 - auc: 0.5022\n",
            "Epoch 15/100\n",
            "10603/10603 [==============================] - 6s 530us/step - loss: 0.1695 - acc: 0.4487 - auc: 0.5025\n",
            "Epoch 16/100\n",
            "10603/10603 [==============================] - 6s 534us/step - loss: 0.1695 - acc: 0.4486 - auc: 0.5024\n",
            "Epoch 17/100\n",
            "10603/10603 [==============================] - 6s 543us/step - loss: 0.1695 - acc: 0.4487 - auc: 0.5024\n",
            "Epoch 18/100\n",
            "10603/10603 [==============================] - 6s 535us/step - loss: 0.1692 - acc: 0.4487 - auc: 0.5022\n",
            "Epoch 19/100\n",
            "10603/10603 [==============================] - 6s 532us/step - loss: 0.1694 - acc: 0.4479 - auc: 0.5023\n",
            "Epoch 20/100\n",
            "10603/10603 [==============================] - 6s 530us/step - loss: 0.1691 - acc: 0.4477 - auc: 0.5024\n",
            "Epoch 21/100\n",
            "10603/10603 [==============================] - 6s 527us/step - loss: 0.1692 - acc: 0.4486 - auc: 0.5022\n",
            "Epoch 22/100\n",
            "10603/10603 [==============================] - 6s 536us/step - loss: 0.1695 - acc: 0.4489 - auc: 0.5020\n",
            "Epoch 23/100\n",
            "10603/10603 [==============================] - 6s 539us/step - loss: 0.1689 - acc: 0.4479 - auc: 0.5021\n",
            "Epoch 24/100\n",
            "10603/10603 [==============================] - 5s 516us/step - loss: 0.1687 - acc: 0.4486 - auc: 0.5022\n",
            "Epoch 25/100\n",
            "10603/10603 [==============================] - 6s 519us/step - loss: 0.1686 - acc: 0.4487 - auc: 0.5022\n",
            "Epoch 26/100\n",
            "10603/10603 [==============================] - 5s 518us/step - loss: 0.1685 - acc: 0.4487 - auc: 0.5023\n",
            "Epoch 27/100\n",
            "10603/10603 [==============================] - 5s 509us/step - loss: 0.1686 - acc: 0.4488 - auc: 0.5023\n",
            "Epoch 28/100\n",
            "10603/10603 [==============================] - 5s 511us/step - loss: 0.1687 - acc: 0.4487 - auc: 0.5022\n",
            "Epoch 29/100\n",
            "10603/10603 [==============================] - 5s 518us/step - loss: 0.1684 - acc: 0.4488 - auc: 0.5022\n",
            "Epoch 30/100\n",
            "10603/10603 [==============================] - 5s 505us/step - loss: 0.1689 - acc: 0.4486 - auc: 0.5021\n",
            "Epoch 31/100\n",
            "10603/10603 [==============================] - 5s 517us/step - loss: 0.1683 - acc: 0.4488 - auc: 0.5021\n",
            "Epoch 32/100\n",
            "10603/10603 [==============================] - 5s 505us/step - loss: 0.1685 - acc: 0.4486 - auc: 0.5023\n",
            "Epoch 33/100\n",
            "10603/10603 [==============================] - 5s 510us/step - loss: 0.1684 - acc: 0.4483 - auc: 0.5025\n",
            "Epoch 34/100\n",
            "10603/10603 [==============================] - 5s 504us/step - loss: 0.1684 - acc: 0.4485 - auc: 0.5026\n",
            "Epoch 35/100\n",
            "10603/10603 [==============================] - 5s 501us/step - loss: 0.1683 - acc: 0.4488 - auc: 0.5025\n",
            "Epoch 36/100\n",
            "10603/10603 [==============================] - 5s 517us/step - loss: 0.1683 - acc: 0.4488 - auc: 0.5026\n",
            "Epoch 37/100\n",
            "10603/10603 [==============================] - 5s 514us/step - loss: 0.1695 - acc: 0.4485 - auc: 0.5026\n",
            "Epoch 38/100\n",
            "10603/10603 [==============================] - 5s 504us/step - loss: 0.1687 - acc: 0.4482 - auc: 0.5026\n",
            "Epoch 39/100\n",
            "10603/10603 [==============================] - 5s 509us/step - loss: 0.1683 - acc: 0.4482 - auc: 0.5026\n",
            "Epoch 40/100\n",
            "10603/10603 [==============================] - 5s 506us/step - loss: 0.1684 - acc: 0.4486 - auc: 0.5026\n",
            "Epoch 41/100\n",
            "10603/10603 [==============================] - 5s 508us/step - loss: 0.1682 - acc: 0.4488 - auc: 0.5027\n",
            "Epoch 42/100\n",
            "10603/10603 [==============================] - 5s 514us/step - loss: 0.1682 - acc: 0.4488 - auc: 0.5027\n",
            "Epoch 43/100\n",
            "10603/10603 [==============================] - 5s 511us/step - loss: 0.1683 - acc: 0.4486 - auc: 0.5027\n",
            "Epoch 44/100\n",
            "10603/10603 [==============================] - 5s 509us/step - loss: 0.1683 - acc: 0.4488 - auc: 0.5027\n",
            "Epoch 45/100\n",
            "10603/10603 [==============================] - 5s 506us/step - loss: 0.1682 - acc: 0.4488 - auc: 0.5026\n",
            "Epoch 46/100\n",
            "10603/10603 [==============================] - 5s 501us/step - loss: 0.1682 - acc: 0.4488 - auc: 0.5027\n",
            "Epoch 47/100\n",
            "10603/10603 [==============================] - 5s 489us/step - loss: 0.1682 - acc: 0.4488 - auc: 0.5027\n",
            "Epoch 48/100\n",
            "10603/10603 [==============================] - 5s 495us/step - loss: 0.1681 - acc: 0.4489 - auc: 0.5027\n",
            "Epoch 49/100\n",
            "10603/10603 [==============================] - 5s 492us/step - loss: 0.1695 - acc: 0.4489 - auc: 0.5027\n",
            "Epoch 50/100\n",
            "10603/10603 [==============================] - 5s 495us/step - loss: 0.1695 - acc: 0.4487 - auc: 0.5027\n",
            "Epoch 51/100\n",
            "10603/10603 [==============================] - 5s 499us/step - loss: 0.1682 - acc: 0.4486 - auc: 0.5028\n",
            "Epoch 52/100\n",
            "10603/10603 [==============================] - 5s 495us/step - loss: 0.1681 - acc: 0.4488 - auc: 0.5029\n",
            "Epoch 53/100\n",
            "10603/10603 [==============================] - 5s 496us/step - loss: 0.1682 - acc: 0.4488 - auc: 0.5030\n",
            "Epoch 54/100\n",
            "10603/10603 [==============================] - 5s 496us/step - loss: 0.1682 - acc: 0.4488 - auc: 0.5029\n",
            "Epoch 55/100\n",
            "10603/10603 [==============================] - 5s 490us/step - loss: 0.1682 - acc: 0.4488 - auc: 0.5029\n",
            "Epoch 56/100\n",
            "10603/10603 [==============================] - 5s 490us/step - loss: 0.1680 - acc: 0.4489 - auc: 0.5029\n",
            "Epoch 57/100\n",
            "10603/10603 [==============================] - 5s 491us/step - loss: 0.1682 - acc: 0.4486 - auc: 0.5030\n",
            "Epoch 58/100\n",
            "10603/10603 [==============================] - 5s 492us/step - loss: 0.1681 - acc: 0.4488 - auc: 0.5030\n",
            "Epoch 59/100\n",
            "10603/10603 [==============================] - 5s 502us/step - loss: 0.1681 - acc: 0.4489 - auc: 0.5030\n",
            "Epoch 60/100\n",
            "10603/10603 [==============================] - 5s 486us/step - loss: 0.1680 - acc: 0.4489 - auc: 0.5030\n",
            "Epoch 61/100\n",
            "10603/10603 [==============================] - 5s 489us/step - loss: 0.1680 - acc: 0.4489 - auc: 0.5031\n",
            "Epoch 62/100\n",
            "10603/10603 [==============================] - 5s 494us/step - loss: 0.1680 - acc: 0.4489 - auc: 0.5031\n",
            "Epoch 63/100\n",
            "10603/10603 [==============================] - 5s 488us/step - loss: 0.1679 - acc: 0.4490 - auc: 0.5032\n",
            "Epoch 64/100\n",
            "10603/10603 [==============================] - 5s 502us/step - loss: 0.1681 - acc: 0.4490 - auc: 0.5033\n",
            "Epoch 65/100\n",
            "10603/10603 [==============================] - 5s 504us/step - loss: 0.1680 - acc: 0.4489 - auc: 0.5033\n",
            "Epoch 66/100\n",
            "10603/10603 [==============================] - 5s 494us/step - loss: 0.1682 - acc: 0.4487 - auc: 0.5033\n",
            "Epoch 67/100\n",
            "10603/10603 [==============================] - 5s 502us/step - loss: 0.1681 - acc: 0.4486 - auc: 0.5033\n",
            "Epoch 68/100\n",
            "10603/10603 [==============================] - 5s 487us/step - loss: 0.1681 - acc: 0.4489 - auc: 0.5033\n",
            "Epoch 69/100\n",
            "10603/10603 [==============================] - 5s 501us/step - loss: 0.1680 - acc: 0.4491 - auc: 0.5032\n",
            "Epoch 70/100\n",
            "10603/10603 [==============================] - 5s 511us/step - loss: 0.1680 - acc: 0.4492 - auc: 0.5032\n",
            "Epoch 71/100\n",
            "10603/10603 [==============================] - 5s 511us/step - loss: 0.1678 - acc: 0.4495 - auc: 0.5033\n",
            "Epoch 72/100\n",
            "10603/10603 [==============================] - 5s 503us/step - loss: 0.1676 - acc: 0.4498 - auc: 0.5034\n",
            "Epoch 73/100\n",
            "10603/10603 [==============================] - 5s 503us/step - loss: 0.1681 - acc: 0.4500 - auc: 0.5036\n",
            "Epoch 74/100\n",
            "10603/10603 [==============================] - 5s 502us/step - loss: 0.1668 - acc: 0.4500 - auc: 0.5042\n",
            "Epoch 75/100\n",
            "10603/10603 [==============================] - 5s 503us/step - loss: 0.1651 - acc: 0.4497 - auc: 0.5049\n",
            "Epoch 76/100\n",
            "10603/10603 [==============================] - 5s 502us/step - loss: 0.1615 - acc: 0.4517 - auc: 0.5067\n",
            "Epoch 77/100\n",
            "10603/10603 [==============================] - 5s 501us/step - loss: 0.1558 - acc: 0.4525 - auc: 0.5098\n",
            "Epoch 78/100\n",
            "10603/10603 [==============================] - 5s 508us/step - loss: 0.1505 - acc: 0.4547 - auc: 0.5142\n",
            "Epoch 79/100\n",
            "10603/10603 [==============================] - 5s 499us/step - loss: 0.1742 - acc: 0.4513 - auc: 0.5173\n",
            "Epoch 80/100\n",
            "10603/10603 [==============================] - 5s 504us/step - loss: 0.1755 - acc: 0.4508 - auc: 0.5173\n",
            "Epoch 81/100\n",
            "10603/10603 [==============================] - 5s 516us/step - loss: 0.1595 - acc: 0.4531 - auc: 0.5189\n",
            "Epoch 82/100\n",
            "10603/10603 [==============================] - 5s 502us/step - loss: 0.1418 - acc: 0.4554 - auc: 0.5236\n",
            "Epoch 83/100\n",
            "10603/10603 [==============================] - 5s 500us/step - loss: 0.1355 - acc: 0.4556 - auc: 0.5294\n",
            "Epoch 84/100\n",
            "10603/10603 [==============================] - 5s 502us/step - loss: 0.1431 - acc: 0.4545 - auc: 0.5350\n",
            "Epoch 85/100\n",
            "10603/10603 [==============================] - 5s 503us/step - loss: 0.1307 - acc: 0.4556 - auc: 0.5399\n",
            "Epoch 86/100\n",
            "10603/10603 [==============================] - 5s 500us/step - loss: 0.1251 - acc: 0.4569 - auc: 0.5462\n",
            "Epoch 87/100\n",
            "10603/10603 [==============================] - 5s 513us/step - loss: 0.1280 - acc: 0.4555 - auc: 0.5522\n",
            "Epoch 88/100\n",
            "10603/10603 [==============================] - 5s 500us/step - loss: 0.1233 - acc: 0.4558 - auc: 0.5581\n",
            "Epoch 89/100\n",
            "10603/10603 [==============================] - 5s 501us/step - loss: 0.1218 - acc: 0.4554 - auc: 0.5641\n",
            "Epoch 90/100\n",
            "10603/10603 [==============================] - 5s 504us/step - loss: 0.1314 - acc: 0.4549 - auc: 0.5695\n",
            "Epoch 91/100\n",
            "10603/10603 [==============================] - 5s 502us/step - loss: 0.1252 - acc: 0.4552 - auc: 0.5747\n",
            "Epoch 92/100\n",
            "10603/10603 [==============================] - 5s 494us/step - loss: 0.1216 - acc: 0.4536 - auc: 0.5800\n",
            "Epoch 93/100\n",
            "10603/10603 [==============================] - 5s 494us/step - loss: 0.1205 - acc: 0.4554 - auc: 0.5851\n",
            "Epoch 94/100\n",
            "10603/10603 [==============================] - 5s 492us/step - loss: 0.1172 - acc: 0.4566 - auc: 0.5903\n",
            "Epoch 95/100\n",
            "10603/10603 [==============================] - 5s 494us/step - loss: 0.1173 - acc: 0.4545 - auc: 0.5953\n",
            "Epoch 96/100\n",
            "10603/10603 [==============================] - 5s 508us/step - loss: 0.1150 - acc: 0.4553 - auc: 0.6004\n",
            "Epoch 97/100\n",
            "10603/10603 [==============================] - 5s 510us/step - loss: 0.1290 - acc: 0.4558 - auc: 0.6048\n",
            "Epoch 98/100\n",
            "10603/10603 [==============================] - 5s 505us/step - loss: 0.1131 - acc: 0.4562 - auc: 0.6091\n",
            "Epoch 99/100\n",
            "10603/10603 [==============================] - 5s 512us/step - loss: 0.1109 - acc: 0.4569 - auc: 0.6140\n",
            "Epoch 100/100\n",
            "10603/10603 [==============================] - 5s 511us/step - loss: 0.1119 - acc: 0.4563 - auc: 0.6186\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "Epoch 1/100\n",
            "15904/15904 [==============================] - 14s 866us/step - loss: 0.6887 - acc: 0.5533 - auc: 0.4894\n",
            "Epoch 2/100\n",
            "15904/15904 [==============================] - 9s 535us/step - loss: 0.6869 - acc: 0.5545 - auc: 0.4948\n",
            "Epoch 3/100\n",
            "15904/15904 [==============================] - 8s 526us/step - loss: 0.6870 - acc: 0.5546 - auc: 0.4976\n",
            "Epoch 4/100\n",
            "15904/15904 [==============================] - 8s 529us/step - loss: 0.6865 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 5/100\n",
            "15904/15904 [==============================] - 8s 524us/step - loss: 0.6872 - acc: 0.5546 - auc: 0.4982\n",
            "Epoch 6/100\n",
            "15904/15904 [==============================] - 9s 535us/step - loss: 0.6862 - acc: 0.5546 - auc: 0.4986\n",
            "Epoch 7/100\n",
            "15904/15904 [==============================] - 8s 529us/step - loss: 0.6869 - acc: 0.5546 - auc: 0.4996\n",
            "Epoch 8/100\n",
            "15904/15904 [==============================] - 9s 537us/step - loss: 0.6866 - acc: 0.5548 - auc: 0.4999\n",
            "Epoch 9/100\n",
            "15904/15904 [==============================] - 9s 535us/step - loss: 0.6866 - acc: 0.5548 - auc: 0.5000\n",
            "Epoch 10/100\n",
            "15904/15904 [==============================] - 9s 537us/step - loss: 0.6860 - acc: 0.5548 - auc: 0.4999\n",
            "Epoch 11/100\n",
            "15904/15904 [==============================] - 9s 535us/step - loss: 0.6860 - acc: 0.5540 - auc: 0.5002\n",
            "Epoch 12/100\n",
            "15904/15904 [==============================] - 9s 546us/step - loss: 0.6860 - acc: 0.5548 - auc: 0.5002\n",
            "Epoch 13/100\n",
            "15904/15904 [==============================] - 9s 547us/step - loss: 0.6867 - acc: 0.5546 - auc: 0.5006\n",
            "Epoch 14/100\n",
            "15904/15904 [==============================] - 8s 532us/step - loss: 0.6862 - acc: 0.5546 - auc: 0.5009\n",
            "Epoch 15/100\n",
            "15904/15904 [==============================] - 8s 534us/step - loss: 0.6856 - acc: 0.5548 - auc: 0.5010\n",
            "Epoch 16/100\n",
            "15904/15904 [==============================] - 9s 549us/step - loss: 0.6856 - acc: 0.5551 - auc: 0.5012\n",
            "Epoch 17/100\n",
            "15904/15904 [==============================] - 8s 529us/step - loss: 0.6848 - acc: 0.5556 - auc: 0.5012\n",
            "Epoch 18/100\n",
            "15904/15904 [==============================] - 8s 525us/step - loss: 0.6859 - acc: 0.5572 - auc: 0.5014\n",
            "Epoch 19/100\n",
            "15904/15904 [==============================] - 8s 534us/step - loss: 0.6848 - acc: 0.5587 - auc: 0.5019\n",
            "Epoch 20/100\n",
            "15904/15904 [==============================] - 8s 533us/step - loss: 0.6724 - acc: 0.5949 - auc: 0.5037\n",
            "Epoch 21/100\n",
            "15904/15904 [==============================] - 9s 545us/step - loss: 0.5976 - acc: 0.7030 - auc: 0.5138\n",
            "Epoch 22/100\n",
            "15904/15904 [==============================] - 9s 535us/step - loss: 0.6612 - acc: 0.6195 - auc: 0.5284\n",
            "Epoch 23/100\n",
            "15904/15904 [==============================] - 9s 541us/step - loss: 0.6937 - acc: 0.5570 - auc: 0.5286\n",
            "Epoch 24/100\n",
            "15904/15904 [==============================] - 8s 523us/step - loss: 0.6893 - acc: 0.5576 - auc: 0.5275\n",
            "Epoch 25/100\n",
            "15904/15904 [==============================] - 8s 530us/step - loss: 0.6879 - acc: 0.5575 - auc: 0.5267\n",
            "Epoch 26/100\n",
            "15904/15904 [==============================] - 8s 526us/step - loss: 0.6895 - acc: 0.5574 - auc: 0.5258\n",
            "Epoch 27/100\n",
            "15904/15904 [==============================] - 8s 517us/step - loss: 0.6859 - acc: 0.5576 - auc: 0.5251\n",
            "Epoch 28/100\n",
            "15904/15904 [==============================] - 8s 515us/step - loss: 0.6847 - acc: 0.5578 - auc: 0.5243\n",
            "Epoch 29/100\n",
            "15904/15904 [==============================] - 8s 517us/step - loss: 0.6856 - acc: 0.5577 - auc: 0.5238\n",
            "Epoch 30/100\n",
            "15904/15904 [==============================] - 8s 520us/step - loss: 0.6846 - acc: 0.5566 - auc: 0.5233\n",
            "Epoch 31/100\n",
            "15904/15904 [==============================] - 8s 521us/step - loss: 0.6850 - acc: 0.5577 - auc: 0.5227\n",
            "Epoch 32/100\n",
            "15904/15904 [==============================] - 8s 526us/step - loss: 0.6836 - acc: 0.5580 - auc: 0.5222\n",
            "Epoch 33/100\n",
            "15904/15904 [==============================] - 8s 525us/step - loss: 0.6830 - acc: 0.5584 - auc: 0.5217\n",
            "Epoch 34/100\n",
            "15904/15904 [==============================] - 8s 517us/step - loss: 0.6828 - acc: 0.5569 - auc: 0.5214\n",
            "Epoch 35/100\n",
            "15904/15904 [==============================] - 8s 534us/step - loss: 0.6830 - acc: 0.5582 - auc: 0.5212\n",
            "Epoch 36/100\n",
            "15904/15904 [==============================] - 9s 536us/step - loss: 0.6451 - acc: 0.6203 - auc: 0.5226\n",
            "Epoch 37/100\n",
            "15904/15904 [==============================] - 9s 556us/step - loss: 0.6849 - acc: 0.5584 - auc: 0.5240\n",
            "Epoch 38/100\n",
            "15904/15904 [==============================] - 9s 551us/step - loss: 0.5727 - acc: 0.7114 - auc: 0.5273\n",
            "Epoch 39/100\n",
            "15904/15904 [==============================] - 9s 550us/step - loss: 0.4847 - acc: 0.7902 - auc: 0.5382\n",
            "Epoch 40/100\n",
            "15904/15904 [==============================] - 9s 548us/step - loss: 0.4370 - acc: 0.8180 - auc: 0.5509\n",
            "Epoch 41/100\n",
            "15904/15904 [==============================] - 9s 547us/step - loss: 0.4422 - acc: 0.8072 - auc: 0.5636\n",
            "Epoch 42/100\n",
            "15904/15904 [==============================] - 9s 549us/step - loss: 0.4107 - acc: 0.8331 - auc: 0.5751\n",
            "Epoch 43/100\n",
            "15904/15904 [==============================] - 9s 538us/step - loss: 0.4024 - acc: 0.8370 - auc: 0.5867\n",
            "Epoch 44/100\n",
            "15904/15904 [==============================] - 9s 543us/step - loss: 0.5263 - acc: 0.7326 - auc: 0.5943\n",
            "Epoch 45/100\n",
            "15904/15904 [==============================] - 9s 536us/step - loss: 0.4204 - acc: 0.8253 - auc: 0.6031\n",
            "Epoch 46/100\n",
            "15904/15904 [==============================] - 9s 537us/step - loss: 0.4121 - acc: 0.8281 - auc: 0.6126\n",
            "Epoch 47/100\n",
            "15904/15904 [==============================] - 9s 536us/step - loss: 0.3989 - acc: 0.8403 - auc: 0.6218\n",
            "Epoch 48/100\n",
            "15904/15904 [==============================] - 8s 523us/step - loss: 0.4002 - acc: 0.8387 - auc: 0.6307\n",
            "Epoch 49/100\n",
            "15904/15904 [==============================] - 9s 554us/step - loss: 0.3961 - acc: 0.8432 - auc: 0.6391\n",
            "Epoch 50/100\n",
            "15904/15904 [==============================] - 9s 535us/step - loss: 0.3817 - acc: 0.8481 - auc: 0.6472\n",
            "Epoch 51/100\n",
            "15904/15904 [==============================] - 8s 534us/step - loss: 0.3767 - acc: 0.8504 - auc: 0.6550\n",
            "Epoch 52/100\n",
            "15904/15904 [==============================] - 9s 544us/step - loss: 0.3829 - acc: 0.8476 - auc: 0.6624\n",
            "Epoch 53/100\n",
            "15904/15904 [==============================] - 8s 528us/step - loss: 0.3753 - acc: 0.8549 - auc: 0.6695\n",
            "Epoch 54/100\n",
            "15904/15904 [==============================] - 9s 539us/step - loss: 0.4134 - acc: 0.8227 - auc: 0.6763\n",
            "Epoch 55/100\n",
            "15904/15904 [==============================] - 9s 544us/step - loss: 0.4284 - acc: 0.8117 - auc: 0.6810\n",
            "Epoch 56/100\n",
            "15904/15904 [==============================] - 8s 528us/step - loss: 0.3673 - acc: 0.8576 - auc: 0.6866\n",
            "Epoch 57/100\n",
            "15904/15904 [==============================] - 8s 527us/step - loss: 0.4494 - acc: 0.8004 - auc: 0.6918\n",
            "Epoch 58/100\n",
            "15904/15904 [==============================] - 8s 526us/step - loss: 0.3701 - acc: 0.8585 - auc: 0.6969\n",
            "Epoch 59/100\n",
            "15904/15904 [==============================] - 8s 527us/step - loss: 0.3609 - acc: 0.8617 - auc: 0.7024\n",
            "Epoch 60/100\n",
            "15904/15904 [==============================] - 8s 533us/step - loss: 0.4311 - acc: 0.8217 - auc: 0.7070\n",
            "Epoch 61/100\n",
            "15904/15904 [==============================] - 9s 537us/step - loss: 0.3723 - acc: 0.8590 - auc: 0.7117\n",
            "Epoch 62/100\n",
            "15904/15904 [==============================] - 8s 524us/step - loss: 0.3594 - acc: 0.8642 - auc: 0.7167\n",
            "Epoch 63/100\n",
            "15904/15904 [==============================] - 8s 531us/step - loss: 0.3530 - acc: 0.8671 - auc: 0.7215\n",
            "Epoch 64/100\n",
            "15904/15904 [==============================] - 8s 521us/step - loss: 0.3448 - acc: 0.8717 - auc: 0.7263\n",
            "Epoch 65/100\n",
            "15904/15904 [==============================] - 8s 529us/step - loss: 0.3470 - acc: 0.8707 - auc: 0.7309\n",
            "Epoch 66/100\n",
            "15904/15904 [==============================] - 8s 521us/step - loss: 0.3425 - acc: 0.8722 - auc: 0.7353\n",
            "Epoch 67/100\n",
            "15904/15904 [==============================] - 8s 525us/step - loss: 0.3308 - acc: 0.8766 - auc: 0.7396\n",
            "Epoch 68/100\n",
            "15904/15904 [==============================] - 8s 528us/step - loss: 0.3343 - acc: 0.8773 - auc: 0.7438\n",
            "Epoch 69/100\n",
            "15904/15904 [==============================] - 8s 523us/step - loss: 0.3426 - acc: 0.8686 - auc: 0.7478\n",
            "Epoch 70/100\n",
            "15904/15904 [==============================] - 9s 535us/step - loss: 0.3371 - acc: 0.8765 - auc: 0.7515\n",
            "Epoch 71/100\n",
            "15904/15904 [==============================] - 9s 535us/step - loss: 0.3289 - acc: 0.8797 - auc: 0.7552\n",
            "Epoch 72/100\n",
            "15904/15904 [==============================] - 8s 534us/step - loss: 0.3262 - acc: 0.8795 - auc: 0.7588\n",
            "Epoch 73/100\n",
            "15904/15904 [==============================] - 8s 532us/step - loss: 0.3237 - acc: 0.8778 - auc: 0.7623\n",
            "Epoch 74/100\n",
            "15904/15904 [==============================] - 8s 524us/step - loss: 0.3287 - acc: 0.8802 - auc: 0.7657\n",
            "Epoch 75/100\n",
            "15904/15904 [==============================] - 8s 521us/step - loss: 0.3291 - acc: 0.8790 - auc: 0.7689\n",
            "Epoch 76/100\n",
            "15904/15904 [==============================] - 8s 516us/step - loss: 0.3249 - acc: 0.8819 - auc: 0.7720\n",
            "Epoch 77/100\n",
            "15904/15904 [==============================] - 8s 515us/step - loss: 0.3186 - acc: 0.8834 - auc: 0.7751\n",
            "Epoch 78/100\n",
            "15904/15904 [==============================] - 8s 515us/step - loss: 0.3142 - acc: 0.8827 - auc: 0.7782\n",
            "Epoch 79/100\n",
            "15904/15904 [==============================] - 8s 530us/step - loss: 0.3251 - acc: 0.8781 - auc: 0.7809\n",
            "Epoch 80/100\n",
            "15904/15904 [==============================] - 8s 527us/step - loss: 0.3145 - acc: 0.8860 - auc: 0.7837\n",
            "Epoch 81/100\n",
            "15904/15904 [==============================] - 9s 536us/step - loss: 0.3169 - acc: 0.8827 - auc: 0.7865\n",
            "Epoch 82/100\n",
            "15904/15904 [==============================] - 8s 528us/step - loss: 0.3345 - acc: 0.8773 - auc: 0.7890\n",
            "Epoch 83/100\n",
            "15904/15904 [==============================] - 8s 533us/step - loss: 0.3150 - acc: 0.8864 - auc: 0.7915\n",
            "Epoch 84/100\n",
            "15904/15904 [==============================] - 8s 530us/step - loss: 0.3119 - acc: 0.8846 - auc: 0.7940\n",
            "Epoch 85/100\n",
            "15904/15904 [==============================] - 9s 536us/step - loss: 0.3080 - acc: 0.8884 - auc: 0.7964\n",
            "Epoch 86/100\n",
            "15904/15904 [==============================] - 9s 553us/step - loss: 0.3035 - acc: 0.8905 - auc: 0.7988\n",
            "Epoch 87/100\n",
            "15904/15904 [==============================] - 9s 536us/step - loss: 0.3098 - acc: 0.8898 - auc: 0.8011\n",
            "Epoch 88/100\n",
            "15904/15904 [==============================] - 9s 542us/step - loss: 0.3136 - acc: 0.8873 - auc: 0.8034\n",
            "Epoch 89/100\n",
            "15904/15904 [==============================] - 8s 532us/step - loss: 0.3108 - acc: 0.8905 - auc: 0.8055\n",
            "Epoch 90/100\n",
            "15904/15904 [==============================] - 8s 532us/step - loss: 0.3057 - acc: 0.8906 - auc: 0.8076\n",
            "Epoch 91/100\n",
            "15904/15904 [==============================] - 8s 533us/step - loss: 0.3004 - acc: 0.8899 - auc: 0.8097\n",
            "Epoch 92/100\n",
            "15904/15904 [==============================] - 8s 526us/step - loss: 0.3023 - acc: 0.8919 - auc: 0.8117\n",
            "Epoch 93/100\n",
            "15904/15904 [==============================] - 8s 524us/step - loss: 0.3024 - acc: 0.8918 - auc: 0.8137\n",
            "Epoch 94/100\n",
            "15904/15904 [==============================] - 8s 529us/step - loss: 0.3026 - acc: 0.8919 - auc: 0.8156\n",
            "Epoch 95/100\n",
            "15904/15904 [==============================] - 8s 533us/step - loss: 0.2957 - acc: 0.8932 - auc: 0.8175\n",
            "Epoch 96/100\n",
            "15904/15904 [==============================] - 8s 533us/step - loss: 0.2970 - acc: 0.8949 - auc: 0.8193\n",
            "Epoch 97/100\n",
            "15904/15904 [==============================] - 8s 526us/step - loss: 0.2988 - acc: 0.8958 - auc: 0.8211\n",
            "Epoch 98/100\n",
            "15904/15904 [==============================] - 8s 518us/step - loss: 0.2961 - acc: 0.8954 - auc: 0.8229\n",
            "Epoch 99/100\n",
            "15904/15904 [==============================] - 8s 520us/step - loss: 0.2903 - acc: 0.8958 - auc: 0.8245\n",
            "Epoch 100/100\n",
            "15904/15904 [==============================] - 8s 518us/step - loss: 0.2959 - acc: 0.8912 - auc: 0.8262\n",
            "The parameters of the best model are: \n",
            "{'batch_size': 32, 'dense_layer_sizes': 128, 'drop_rate': 0.5, 'epochs': 100, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.925736 (0.009949) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'drop_rate': 0.5, 'epochs': 100, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.677490 (0.164497) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'drop_rate': 0.5, 'epochs': 100, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7f913218b510>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_39XP1sWG8r",
        "colab_type": "code",
        "outputId": "97ab577c-bdfe-4fd7-ae01-422617596df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "\n",
        "print('The parameters of the best model are: ')\n",
        "print(validator66.best_params_)\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters of the best model are: \n",
            "{'batch_size': 32, 'dense_layer_sizes': 128, 'drop_rate': 0.5, 'epochs': 100, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.925736 (0.009949) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'drop_rate': 0.5, 'epochs': 100, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.677490 (0.164497) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'drop_rate': 0.5, 'epochs': 100, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7f913218b510>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ7RJLbEl5il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del X_train1, y_train1\n",
        "'''\n",
        "#################### Rep 1 - spektro ####################\n",
        "# rep 1  ------- 63 x 148 ------ \n",
        "\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep1/X_test2_rep1.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep1/y_test2_rep1.npy', allow_pickle=True)\n",
        "\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep1/X_test_rep1.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep1/y_test_rep1.npy', allow_pickle=True)  ### ----- 322 positive! -----\n",
        "\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep1/X_test1_rep1_nowa676.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep1/y_test1_rep1_nowa676.npy', allow_pickle=True)  ### ----- 676 positive! -----\n",
        "\n",
        "# rep 1b ------- 63 x 63 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep1b/X_test2_rep1b.npy', allow_pickle=True) \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep1b/y_test2_rep1b.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep1b/X_test1_rep1b.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep1b/y_test1_rep1b.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 3 - mel-spektro ####################\n",
        "\n",
        "# rep 3 ------- 60 x 111 ------ \n",
        "X_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep3/X_test2_rep3.npy', allow_pickle=True)  \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep3/y_test2_popr.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep3/X_test_rep3.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep3/y_test.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3b ------- 60 x 63 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep3b/X_test2_rep3b.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep3b/y_test2_rep3b.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep3b/X_test1_rep3b.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep3b/y_test1_rep3b.npy', allow_pickle=True)\n",
        "\n",
        "# rep 3V3 ------- 60 x 148 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep3V3/X_test2_rep3V3.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep3V3/y_test2_rep3V3.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep3V3/X_test1_rep3V3.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep3V3/y_test1_rep3V3.npy', allow_pickle=True)\n",
        "\n",
        "#################### Rep 5 - mel-spektro ####################\n",
        "\n",
        "# rep 5 ------- 64 x 61 ------ \n",
        "X_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep5/X_test2_rep5.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep5/y_test2_rep5.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep5/X_test_rep5.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep5/y_test_rep5.npy', allow_pickle=True) ### ----- 322 positive! -----\n",
        "\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep5/X_test1_rep5_nowa676.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep5/y_test1_rep5_nowa676.npy', allow_pickle=True)  ### ----- 676 positive! -----\n",
        "'''\n",
        "# rep 5b ------- 64 x 149 ------\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep5b/X_test2_rep5b.npy', allow_pickle=True)  \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep5b/y_test2_rep5b.npy', allow_pickle=True)\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep5b/X_test1_rep5b.npy', allow_pickle=True)   \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep5b/y_test1_rep5b.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "r = np.shape(X_test1_6)[1]\n",
        "s = np.shape(X_test1_6)[2]\n",
        "\n",
        "X_test1_14 = X_test1_14.reshape(X_test1_14.shape[0], 1, r, s).astype('float32')\n",
        "y_test1_14 = np.squeeze(y_test1_14)\n",
        "X_test1_6 = X_test1_6.reshape(X_test1_6.shape[0], 1, r, s).astype('float32')\n",
        "y_test1_6 = np.squeeze(y_test1_6)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d7Y-vG3ZRnY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9ae17ef-630f-442e-d407-0dd0aaab4de0"
      },
      "source": [
        "best66 = {'batch_size': 32, 'dense_layer_sizes': 128, 'drop_rate': 0.5, 'epochs': 100, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
        "'''\n",
        "best1 = validator66.best_params_ #{'batch_size': 32, 'dense_layer_sizes': 256, 'epochs': 30, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
        "means_best = grid_result.cv_results_['mean_test_score']\n",
        "a = means_best.argsort()[-3:][::-1]\n",
        "best2 = grid_result.cv_results_['params'][a[1]]\n",
        "#best3 = grid_result.cv_results_['params'][a[2]]\n",
        "'''\n",
        "#means = np.array([0.765971 ,0.898076 ,0.755596,0.752892,0.900277 ,0.754842])\n",
        "#a = means.argsort()[-3:][::-1]\n",
        "\n",
        "#dict_set = [best1, best2]#, best3]\n",
        "dict_set = [best66]\n",
        "\n",
        "print(dict_set[0])\n",
        "for k in range(0,np.shape(dict_set)[0]):  \n",
        "  best = dict_set[k]\n",
        "  dense_layer_sizes= best['dense_layer_sizes']\n",
        "  filters= best['filters']\n",
        "  kernel_size= best['kernel_size']\n",
        "  pool_size= best['pool_size']\n",
        "  hidden_layers= best['hidden_layers']\n",
        "  loss_function= best['loss_function']\n",
        "  batch_size= best['batch_size']\n",
        "  epochs= best['epochs']\n",
        "  drop_rate= best['drop_rate']\n",
        "  print(epochs)\n",
        "  \n",
        "  chosen_model = search_model(dense_layer_sizes, filters, kernel_size, pool_size, hidden_layers, loss_function, drop_rate)\n",
        "  model_result = chosen_model.fit(X_train, y, batch_size = batch_size, epochs = epochs)\n",
        "  \n",
        "  probs1 = chosen_model.predict_proba(X_test1_14)\n",
        "  probs2 = chosen_model.predict_proba(X_test1_6)\n",
        "  probs = np.concatenate([probs1, probs2])\n",
        "  y_test1 = np.concatenate([y_test1_14, y_test1_6])\n",
        "  print(np.shape(probs))\n",
        "  print(np.shape(y_test1))\n",
        "  auc = roc_auc_score(y_test1, probs)\n",
        "  print('AUC: %.6f' % auc)\n",
        "  \n",
        "  fpr, tpr, thresholds = roc_curve(y_test1, probs)\n",
        "  plt.plot(fpr, tpr)\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 32, 'dense_layer_sizes': 128, 'drop_rate': 0.5, 'epochs': 100, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Warstwa 1\n",
            "Warstwa 2\n",
            "Warstwa 3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:809: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/100\n",
            "15904/15904 [==============================] - 12s 736us/step - loss: 0.6877 - acc: 0.5527 - auc: 0.4969\n",
            "Epoch 2/100\n",
            "15904/15904 [==============================] - 8s 495us/step - loss: 0.6869 - acc: 0.5546 - auc: 0.5038\n",
            "Epoch 3/100\n",
            "15904/15904 [==============================] - 8s 491us/step - loss: 0.6870 - acc: 0.5546 - auc: 0.5028\n",
            "Epoch 4/100\n",
            "15904/15904 [==============================] - 8s 506us/step - loss: 0.6861 - acc: 0.5546 - auc: 0.5025\n",
            "Epoch 5/100\n",
            "15904/15904 [==============================] - 8s 509us/step - loss: 0.6868 - acc: 0.5541 - auc: 0.5041\n",
            "Epoch 6/100\n",
            "15904/15904 [==============================] - 8s 502us/step - loss: 0.6859 - acc: 0.5548 - auc: 0.5038\n",
            "Epoch 7/100\n",
            "15904/15904 [==============================] - 8s 504us/step - loss: 0.6869 - acc: 0.5548 - auc: 0.5034\n",
            "Epoch 8/100\n",
            "15904/15904 [==============================] - 8s 516us/step - loss: 0.6864 - acc: 0.5553 - auc: 0.5030\n",
            "Epoch 9/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.6856 - acc: 0.5556 - auc: 0.5026\n",
            "Epoch 10/100\n",
            "15904/15904 [==============================] - 8s 503us/step - loss: 0.6860 - acc: 0.5550 - auc: 0.5034\n",
            "Epoch 11/100\n",
            "15904/15904 [==============================] - 8s 500us/step - loss: 0.6850 - acc: 0.5557 - auc: 0.5043\n",
            "Epoch 12/100\n",
            "15904/15904 [==============================] - 8s 508us/step - loss: 0.6840 - acc: 0.5587 - auc: 0.5054\n",
            "Epoch 13/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.6657 - acc: 0.6047 - auc: 0.5087\n",
            "Epoch 14/100\n",
            "15904/15904 [==============================] - 8s 503us/step - loss: 0.6332 - acc: 0.6413 - auc: 0.5212\n",
            "Epoch 15/100\n",
            "15904/15904 [==============================] - 8s 507us/step - loss: 0.5253 - acc: 0.7691 - auc: 0.5414\n",
            "Epoch 16/100\n",
            "15904/15904 [==============================] - 8s 524us/step - loss: 0.5209 - acc: 0.7481 - auc: 0.5653\n",
            "Epoch 17/100\n",
            "15904/15904 [==============================] - 8s 511us/step - loss: 0.4325 - acc: 0.8227 - auc: 0.5919\n",
            "Epoch 18/100\n",
            "15904/15904 [==============================] - 8s 503us/step - loss: 0.4164 - acc: 0.8326 - auc: 0.6173\n",
            "Epoch 19/100\n",
            "15904/15904 [==============================] - 8s 499us/step - loss: 0.3927 - acc: 0.8465 - auc: 0.6402\n",
            "Epoch 20/100\n",
            "15904/15904 [==============================] - 8s 503us/step - loss: 0.4228 - acc: 0.8153 - auc: 0.6607\n",
            "Epoch 21/100\n",
            "15904/15904 [==============================] - 8s 517us/step - loss: 0.4251 - acc: 0.8306 - auc: 0.6745\n",
            "Epoch 22/100\n",
            "15904/15904 [==============================] - 8s 496us/step - loss: 0.4615 - acc: 0.7869 - auc: 0.6866\n",
            "Epoch 23/100\n",
            "15904/15904 [==============================] - 8s 494us/step - loss: 0.4090 - acc: 0.8351 - auc: 0.6986\n",
            "Epoch 24/100\n",
            "15904/15904 [==============================] - 8s 494us/step - loss: 0.3821 - acc: 0.8504 - auc: 0.7108\n",
            "Epoch 25/100\n",
            "15904/15904 [==============================] - 8s 511us/step - loss: 0.3762 - acc: 0.8521 - auc: 0.7222\n",
            "Epoch 26/100\n",
            "15904/15904 [==============================] - 8s 496us/step - loss: 0.3701 - acc: 0.8579 - auc: 0.7327\n",
            "Epoch 27/100\n",
            "15904/15904 [==============================] - 8s 506us/step - loss: 0.3644 - acc: 0.8593 - auc: 0.7423\n",
            "Epoch 28/100\n",
            "15904/15904 [==============================] - 8s 495us/step - loss: 0.3977 - acc: 0.8355 - auc: 0.7508\n",
            "Epoch 29/100\n",
            "15904/15904 [==============================] - 8s 492us/step - loss: 0.3529 - acc: 0.8640 - auc: 0.7580\n",
            "Epoch 30/100\n",
            "15904/15904 [==============================] - 8s 505us/step - loss: 0.3529 - acc: 0.8664 - auc: 0.7656\n",
            "Epoch 31/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.3456 - acc: 0.8681 - auc: 0.7726\n",
            "Epoch 32/100\n",
            "15904/15904 [==============================] - 8s 511us/step - loss: 0.3475 - acc: 0.8698 - auc: 0.7792\n",
            "Epoch 33/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.3498 - acc: 0.8670 - auc: 0.7851\n",
            "Epoch 34/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.3443 - acc: 0.8727 - auc: 0.7907\n",
            "Epoch 35/100\n",
            "15904/15904 [==============================] - 8s 503us/step - loss: 0.5036 - acc: 0.7409 - auc: 0.7942\n",
            "Epoch 36/100\n",
            "15904/15904 [==============================] - 8s 514us/step - loss: 0.3496 - acc: 0.8695 - auc: 0.7961\n",
            "Epoch 37/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.3340 - acc: 0.8752 - auc: 0.8008\n",
            "Epoch 38/100\n",
            "15904/15904 [==============================] - 8s 502us/step - loss: 0.3398 - acc: 0.8738 - auc: 0.8052\n",
            "Epoch 39/100\n",
            "15904/15904 [==============================] - 8s 511us/step - loss: 0.3357 - acc: 0.8778 - auc: 0.8095\n",
            "Epoch 40/100\n",
            "15904/15904 [==============================] - 8s 504us/step - loss: 0.3244 - acc: 0.8792 - auc: 0.8135\n",
            "Epoch 41/100\n",
            "15904/15904 [==============================] - 8s 503us/step - loss: 0.3271 - acc: 0.8832 - auc: 0.8175\n",
            "Epoch 42/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.3240 - acc: 0.8833 - auc: 0.8212\n",
            "Epoch 43/100\n",
            "15904/15904 [==============================] - 8s 511us/step - loss: 0.3267 - acc: 0.8826 - auc: 0.8246\n",
            "Epoch 44/100\n",
            "15904/15904 [==============================] - 8s 508us/step - loss: 0.3206 - acc: 0.8824 - auc: 0.8278\n",
            "Epoch 45/100\n",
            "15904/15904 [==============================] - 8s 509us/step - loss: 0.3108 - acc: 0.8872 - auc: 0.8310\n",
            "Epoch 46/100\n",
            "15904/15904 [==============================] - 8s 527us/step - loss: 0.3156 - acc: 0.8837 - auc: 0.8339\n",
            "Epoch 47/100\n",
            "15904/15904 [==============================] - 8s 508us/step - loss: 0.3124 - acc: 0.8881 - auc: 0.8368\n",
            "Epoch 48/100\n",
            "15904/15904 [==============================] - 8s 499us/step - loss: 0.3058 - acc: 0.8866 - auc: 0.8395\n",
            "Epoch 49/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.3075 - acc: 0.8868 - auc: 0.8421\n",
            "Epoch 50/100\n",
            "15904/15904 [==============================] - 8s 496us/step - loss: 0.3269 - acc: 0.8790 - auc: 0.8444\n",
            "Epoch 51/100\n",
            "15904/15904 [==============================] - 8s 499us/step - loss: 0.3137 - acc: 0.8857 - auc: 0.8466\n",
            "Epoch 52/100\n",
            "15904/15904 [==============================] - 8s 495us/step - loss: 0.3092 - acc: 0.8872 - auc: 0.8487\n",
            "Epoch 53/100\n",
            "15904/15904 [==============================] - 8s 494us/step - loss: 0.3060 - acc: 0.8910 - auc: 0.8508\n",
            "Epoch 54/100\n",
            "15904/15904 [==============================] - 8s 505us/step - loss: 0.3019 - acc: 0.8902 - auc: 0.8528\n",
            "Epoch 55/100\n",
            "15904/15904 [==============================] - 8s 513us/step - loss: 0.3086 - acc: 0.8860 - auc: 0.8547\n",
            "Epoch 56/100\n",
            "15904/15904 [==============================] - 8s 496us/step - loss: 0.3003 - acc: 0.8919 - auc: 0.8565\n",
            "Epoch 57/100\n",
            "15904/15904 [==============================] - 8s 492us/step - loss: 0.3123 - acc: 0.8890 - auc: 0.8583\n",
            "Epoch 58/100\n",
            "15904/15904 [==============================] - 8s 502us/step - loss: 0.3080 - acc: 0.8893 - auc: 0.8599\n",
            "Epoch 59/100\n",
            "15904/15904 [==============================] - 8s 498us/step - loss: 0.3028 - acc: 0.8902 - auc: 0.8615\n",
            "Epoch 60/100\n",
            "15904/15904 [==============================] - 8s 502us/step - loss: 0.2980 - acc: 0.8910 - auc: 0.8631\n",
            "Epoch 61/100\n",
            "15904/15904 [==============================] - 8s 497us/step - loss: 0.2944 - acc: 0.8927 - auc: 0.8646\n",
            "Epoch 62/100\n",
            "15904/15904 [==============================] - 8s 494us/step - loss: 0.2996 - acc: 0.8920 - auc: 0.8661\n",
            "Epoch 63/100\n",
            "15904/15904 [==============================] - 8s 498us/step - loss: 0.3523 - acc: 0.8526 - auc: 0.8673\n",
            "Epoch 64/100\n",
            "15904/15904 [==============================] - 8s 504us/step - loss: 0.2936 - acc: 0.8927 - auc: 0.8682\n",
            "Epoch 65/100\n",
            "15904/15904 [==============================] - 8s 504us/step - loss: 0.2966 - acc: 0.8910 - auc: 0.8695\n",
            "Epoch 66/100\n",
            "15904/15904 [==============================] - 8s 498us/step - loss: 0.2871 - acc: 0.8954 - auc: 0.8708\n",
            "Epoch 67/100\n",
            "15904/15904 [==============================] - 8s 507us/step - loss: 0.2887 - acc: 0.8949 - auc: 0.8721\n",
            "Epoch 68/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.2883 - acc: 0.8926 - auc: 0.8734\n",
            "Epoch 69/100\n",
            "15904/15904 [==============================] - 8s 497us/step - loss: 0.2864 - acc: 0.8952 - auc: 0.8745\n",
            "Epoch 70/100\n",
            "15904/15904 [==============================] - 8s 504us/step - loss: 0.2905 - acc: 0.8955 - auc: 0.8757\n",
            "Epoch 71/100\n",
            "15904/15904 [==============================] - 8s 494us/step - loss: 0.2904 - acc: 0.8957 - auc: 0.8768\n",
            "Epoch 72/100\n",
            "15904/15904 [==============================] - 8s 492us/step - loss: 0.2824 - acc: 0.9003 - auc: 0.8779\n",
            "Epoch 73/100\n",
            "15904/15904 [==============================] - 8s 500us/step - loss: 0.2842 - acc: 0.8994 - auc: 0.8789\n",
            "Epoch 74/100\n",
            "15904/15904 [==============================] - 8s 491us/step - loss: 0.2886 - acc: 0.8980 - auc: 0.8800\n",
            "Epoch 75/100\n",
            "15904/15904 [==============================] - 8s 496us/step - loss: 0.3044 - acc: 0.8937 - auc: 0.8809\n",
            "Epoch 76/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.2807 - acc: 0.9000 - auc: 0.8819\n",
            "Epoch 77/100\n",
            "15904/15904 [==============================] - 8s 500us/step - loss: 0.2823 - acc: 0.8989 - auc: 0.8828\n",
            "Epoch 78/100\n",
            "15904/15904 [==============================] - 8s 493us/step - loss: 0.2776 - acc: 0.9003 - auc: 0.8838\n",
            "Epoch 79/100\n",
            "15904/15904 [==============================] - 8s 493us/step - loss: 0.2807 - acc: 0.8979 - auc: 0.8847\n",
            "Epoch 80/100\n",
            "15904/15904 [==============================] - 8s 502us/step - loss: 0.2788 - acc: 0.8991 - auc: 0.8855\n",
            "Epoch 81/100\n",
            "15904/15904 [==============================] - 8s 492us/step - loss: 0.2871 - acc: 0.8956 - auc: 0.8864\n",
            "Epoch 82/100\n",
            "15904/15904 [==============================] - 8s 501us/step - loss: 0.2856 - acc: 0.8971 - auc: 0.8871\n",
            "Epoch 83/100\n",
            "15904/15904 [==============================] - 8s 497us/step - loss: 0.2825 - acc: 0.8961 - auc: 0.8879\n",
            "Epoch 84/100\n",
            "15904/15904 [==============================] - 8s 498us/step - loss: 0.2827 - acc: 0.9001 - auc: 0.8887\n",
            "Epoch 85/100\n",
            "15904/15904 [==============================] - 8s 519us/step - loss: 0.2771 - acc: 0.8985 - auc: 0.8894\n",
            "Epoch 86/100\n",
            "15904/15904 [==============================] - 8s 525us/step - loss: 0.2752 - acc: 0.9024 - auc: 0.8902\n",
            "Epoch 87/100\n",
            "15904/15904 [==============================] - 8s 503us/step - loss: 0.2727 - acc: 0.9016 - auc: 0.8909\n",
            "Epoch 88/100\n",
            "15904/15904 [==============================] - 8s 490us/step - loss: 0.2760 - acc: 0.9021 - auc: 0.8917\n",
            "Epoch 89/100\n",
            "15904/15904 [==============================] - 8s 502us/step - loss: 0.3214 - acc: 0.8725 - auc: 0.8921\n",
            "Epoch 90/100\n",
            "15904/15904 [==============================] - 8s 498us/step - loss: 0.2832 - acc: 0.8978 - auc: 0.8927\n",
            "Epoch 91/100\n",
            "15904/15904 [==============================] - 8s 494us/step - loss: 0.2862 - acc: 0.8968 - auc: 0.8933\n",
            "Epoch 92/100\n",
            "15904/15904 [==============================] - 8s 497us/step - loss: 0.2837 - acc: 0.9002 - auc: 0.8939\n",
            "Epoch 93/100\n",
            "15904/15904 [==============================] - 8s 509us/step - loss: 0.2838 - acc: 0.8999 - auc: 0.8945\n",
            "Epoch 94/100\n",
            "15904/15904 [==============================] - 8s 516us/step - loss: 0.2784 - acc: 0.9018 - auc: 0.8950\n",
            "Epoch 95/100\n",
            "15904/15904 [==============================] - 8s 496us/step - loss: 0.2736 - acc: 0.9012 - auc: 0.8956\n",
            "Epoch 96/100\n",
            "15904/15904 [==============================] - 8s 504us/step - loss: 0.2748 - acc: 0.9022 - auc: 0.8962\n",
            "Epoch 97/100\n",
            "15904/15904 [==============================] - 8s 504us/step - loss: 0.2677 - acc: 0.9062 - auc: 0.8968\n",
            "Epoch 98/100\n",
            "15904/15904 [==============================] - 8s 498us/step - loss: 0.2737 - acc: 0.9057 - auc: 0.8974\n",
            "Epoch 99/100\n",
            "15904/15904 [==============================] - 8s 493us/step - loss: 0.2777 - acc: 0.9029 - auc: 0.8979\n",
            "Epoch 100/100\n",
            "15904/15904 [==============================] - 8s 499us/step - loss: 0.2729 - acc: 0.9040 - auc: 0.8985\n",
            "(102695, 1)\n",
            "(102695,)\n",
            "AUC: 0.884943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRnbG_YouRol",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "dense_layer_sizes= best['dense_layer_sizes']\n",
        "filters= best['filters']\n",
        "kernel_size= best['kernel_size']\n",
        "pool_size= best['pool_size']\n",
        "hidden_layers= best['hidden_layers']\n",
        "loss_function= best['loss_function']\n",
        "batch_size= best['batch_size']\n",
        "epochs= best['epochs']\n",
        "chosen_model = search_model(dense_layer_sizes, filters, kernel_size, pool_size, hidden_layers, loss_function)\n",
        "model_result = chosen_model.fit(X_train, y, batch_size = batch_size, epochs = epochs)\n",
        "\n",
        "X_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep1/X_test1_rep1_nowa676.npy', allow_pickle=True)  \n",
        "y_test1_6 = numpy.load('drive/My Drive/rep IBAC/rep1/y_test1_rep1_nowa676.npy', allow_pickle=True)  ### ----- 676 positive! -----\n",
        "X_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep1/X_test2_rep1.npy', allow_pickle=True)   \n",
        "y_test1_14 = numpy.load('drive/My Drive/rep IBAC/rep1/y_test2_rep1.npy', allow_pickle=True)\n",
        "\n",
        "r = np.shape(X_test1_6)[1]\n",
        "s = np.shape(X_test1_6)[2]\n",
        "\n",
        "X_test1_14 = X_test1_14.reshape(X_test1_14.shape[0], 1, r, s).astype('float32')\n",
        "y_test1_14 = np.squeeze(y_test1_14)\n",
        "X_test1_6 = X_test1_6.reshape(X_test1_6.shape[0], 1, r, s).astype('float32')\n",
        "y_test1_6 = np.squeeze(y_test1_6)\n",
        "\n",
        "probs1 = chosen_model.predict_proba(X_test1_14)\n",
        "probs2 = chosen_model.predict_proba(X_test1_6)\n",
        "probs = np.concatenate([probs1, probs2])\n",
        "y_test1 = np.concatenate([y_test1_14, y_test1_6])\n",
        "\n",
        "print(np.shape(probs))\n",
        "print(np.shape(y_test1))\n",
        "auc = roc_auc_score(y_test1, probs)\n",
        "#print(\"Run: \" + str(k))\n",
        "print('AUC: %.6f' % auc)\n",
        "fpr, tpr, thresholds = roc_curve(y_test1, probs)\n",
        "plt.plot(fpr, tpr)\n",
        "#plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([-0.01, 1.01])\n",
        "plt.ylim([-0.01, 1.01])\n",
        "#plt.title(title_rys)\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGXggAs2UY2m",
        "colab_type": "code",
        "outputId": "dc80f1f0-98d9-48c6-bcb4-0cdb6010524c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "print('The parameters of the best model are: ')\n",
        "print(validator66.best_params_)\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters of the best model are: \n",
            "{'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.869907 (0.014068) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.671089 (0.122335) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.752012 (0.161275) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.710827 (0.093366) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.758426 (0.166309) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.674233 (0.127986) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.904049 (0.006086) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.687814 (0.123917) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.871227 (0.006594) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.671152 (0.116912) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.888141 (0.004962) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.745410 (0.021864) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 10, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.820485 (0.132884) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.635123 (0.157508) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 2, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n",
            "0.695108 (0.172173) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': 'binary_crossentropy', 'pool_size': (2, 2)}\n",
            "0.725541 (0.060771) with: {'batch_size': 64, 'dense_layer_sizes': 128, 'epochs': 15, 'filters': 20, 'hidden_layers': 3, 'kernel_size': (3, 3), 'loss_function': functools.partial(<function weighted_binary_crossentropy at 0x7fddec2d7268>, weights_10=array([0.9, 0.1])), 'pool_size': (2, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}